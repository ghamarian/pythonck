---
title: "Intro to AMD CDNA Architecture"
format:
  live-html:
    mermaid:
      theme: default
---

# CDNA

The AMD CDNA architecture is a specialized GPU design for high-performance 
computing (HPC) and AI workloads. Unlike the RDNA architecture used in 
gaming GPUs, CDNA is optimized for data center tasks, prioritizing compute 
density, memory bandwidth, and scalability. This is achieved through several 
key architectural features.

## XCD (eXtreme Chiplet Design)

A fundamental element of CDNA is the **eXtreme Chiplet Design (XCD).**
This design breaks the GPU into smaller, modular chiplets. 
Each XCD contains a portion of the GPU's compute resources and a dedicated 
slice of the L2 cache. This modular approach allows for greater manufacturing 
flexibility, higher yields, and improved scalability, as multiple XCDs can be 
connected together to form a single, powerful GPU.

## CUs (Compute Units)
Each XCD is composed of multiple **Compute Units (CUs)**. 
The CU is the fundamental building block of the GPU, responsible for 
executing a group of threads. Each CU contains a set of scalar and vector units,
a local data share (LDS) for shared memory, and its own instruction and 
data caches. The number of CUs in a CDNA GPU directly correlates with its raw
processing power, as it determines how many parallel operations can be performed.

![](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/_images/gcn_compute_unit.png)

A Compute Unit (CU) contains several specialized execution pipelines and functional blocks, 
such as the VALU (Vector ALU), SALU (Scalar ALU), Local Data Share (LDS),
 and a scheduler. Here are some key CU components:

### SIMD
SIMD (Single Instruction, Multiple Data) units allow a single instruction to operate on multiple data points simultaneously.
 In a CDNA Compute Unit, each SIMD typically consists of 16 lanes, and four SIMDs work together to process a wavefront of 64 threads 
 in parallel. Modern architectures, such as Mi300, support multiple concurrent wavefronts per CU and provide 512 vector registers (VGPRs) shared among them.

### VGPR
VGPRs (Vector General Purpose Registers) are one dword wide registers assigned to each thread in a wavefront. 
The number of VGPRs available per wavefront limits how many threads can be active at once. If a kernel requires more than 512 VGPRs per thread,
 the compiler will spill excess data to scratch memory in HBM, which is much slower due to higher latency.
 This is primary motivation behind hughly templated CK design, the API is aiming to perform as many computations 
 at compile time as possible without allocating  GPU registers at runtime.

### SGPR
SGPRs (Scalar General Purpose Registers) are one dword wide registers shared by all threads in a wavefront. 
There are fewer SGPRs than VGPRs; for example, Mi300 provides 128 SGPRs per CU.

LDS (Local Data Share) is a fast, shared memory block accessible by all threads within a CU, enabling efficient 
data exchange and synchronization. More details on LDS will be discussed in a dedicated section.

### vL1D
The Vector L1 Data Cache (vL1D) is a high-speed, local cache within each Compute Unit (CU) of the AMD CDNA architecture. 
It is designed to handle memory requests from the vector units, which are responsible for parallel operations on a wavefront. 
Also known as the Texture Cache per Pipe (TCP), the vL1D cache works with a series of specialized units to quickly access 
data from memory, including an address translation unit and a tag RAM. The performance of this cache is critical for reducing
memory latency and minimizing stalls in compute-intensive workloads.