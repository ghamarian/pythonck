---
title: "Reduce-2D Kernel"
format: live-html
---

## Overview

The `ck_tile::Reduce` kernel implements a 2D reduction operation that efficiently reduces data along specified dimensions of an arbitrary-rank input tensor. The kernel transforms the input into a logical 2D view with one "kept" dimension (M) and one "reduced" dimension (N), then applies a hierarchical reduction.

**Running Example**:
Throughout this document, we use an input tensor with NHWC layout where:

- **Input shape**: `(N, H, W, C)` 
- **Reduce dimensions**: `{1, 2}` (H and W)
- **Kept dimensions**: `{0, 3}` (N and C)
- **Result**: Output tensor with shape `(N, C)` containing reduced values

The Reduce-2D kernel is structured as a pipeline of three intra-block stages:

```{=html}
<div class="mermaid">
flowchart LR
    T1["Stage 1: Thread-Level<br/>Each thread accumulates its assigned elements"]
    T2["Stage 2: Warp-Level<br/>Threads within warps combine via shuffle ops"]
    T3["Stage 3: Block-Level<br/>Warps combine results via shared memory"]

    T1 --> T2 --> T3
</div>
```

1. **Thread-level accumulation** (`BlockReduce2d`): Individual threads process their assigned data tiles
2. **Intra-warp shuffle reduction** (`BlockReduce2dSync`): Threads within each warp combine results using warp shuffle  
3. **Cross-warp shared-memory reduction** (`BlockReduce2dCrossWarpSync`): Warps coordinate through shared memory to produce final block results

A final store writes the reduced tile to global memory.

## Problem Decomposition and Tensor Transformation

### Dimensional Merging

The kernel transforms an arbitrary-rank input tensor into a 2D logical view through merging dimensions:

**Input tensor X**: `(N, H, W, C)` → **2D view**: `(M, N)` where:

- **M dimension** = merged kept dimensions
- **N dimension** = merged reduction dimensions

### Tensor View Pipeline

1. **Dimension extraction:**
   - `kept_lens = (len(N), len(C))`
   - `reduce_lens = (len(H), len(W))`

2. **Merge transformations:**
   - `kept_merge_transform(kept_lens)` → M
   - `reduce_merge_transform(reduce_lens)` → N

3. **Apply** `transform_tensor_view` → produce 2D `[M, N]`

4. **Block-level padding:** Pad to block tile multiples `(Block_M, Block_N) = (128, 128)`

5. **Tile window setup:** Each block processes a `Block_M × Block_N` tile, sliding along the N dimension


## Core Architecture Components

**Block Shape Configuration**

The kernel's performance characteristics are determined by the `Reduce2dShape` configuration:

```cpp
// Example configuration
using BlockWarps = sequence<4, 1>;    // 4 warps along M, 1 warp along N
using BlockTile  = sequence<128, 128>; // 128×128 elements per block
using WarpTile   = sequence<32, 128>;  // 32×128 elements per warp  
using ThreadTile     = sequence<8, 8>;     // 8×8 elements per thread

```

**Key parameters:**

- `Block_M = 128`, `Block_N = 128`: Each block processes a 128×128 tile
- `ThreadTile_M = 8`, `ThreadTile_N = 8`: Each thread handles an 8×8 sub-tile
- `WarpPerBlock_M = 4`, `WarpPerBlock_N = 1`: 4 warps along M, 1 along N
- `BlockSize = 256`: Total threads per block (4 warps × 64 threads/warp)

### Policy Architecture

The `Reduce2dDefaultPolicy` provides the algorithmic framework:

- **Tile distribution encoding**: Defines how data maps to GPU execution hierarchy
- **Stage object factories**: Creates reduction operators for each pipeline stage
- **Memory management**: Computes shared memory requirements for cross-warp coordination

## Input Tensor Distribution Encoding

The tile distribution encoding specifies how the 2D logical tensor maps onto the GPU's execution hierarchy. This encoding is needed for achieving desired pattern of memory access and parallelization.

Distribution Structure:
```cpp
tile_distribution_encoding<
    sequence<>,                                    // No replication dimensions
    tuple<
        sequence<Repeat_M, WarpPerBlock_M, ThreadPerWarp_M, ThreadTile_M>,  // M dimension hierarchy
        sequence<Repeat_N, WarpPerBlock_N, ThreadPerWarp_N, ThreadTile_N>>, // N dimension hierarchy  
    tuple<sequence<1, 2>, sequence<1, 2>>,         // P→H mapping (major)
    tuple<sequence<1, 1>, sequence<2, 2>>,         // P→H mapping (minor)
    sequence<1, 1, 2, 2>,                          // Y→H mapping (major)
    sequence<0, 3, 0, 3>>                          // Y→H mapping (minor)
``` 

Concrete Example Values

For our running example with `BlockTile = (128, 128)`, `WarpTile = (32, 128)`, `ThreadTile = (8, 8)`:

```cpp
tile_distribution_encoding<
                sequence<>, // No replication
                tuple<
                    sequence<1, 4, 4, 8>,  // M hierarchy: [Repeat=1, Warps=4, Threads=4, Elements=8] = 128
                    sequence<1, 1, 16, 8>>, // N hierarchy: [Repeat=1, Warps=1, Threads=16, Elements=8] = 128
                tuple<sequence<1, 2>, sequence<1, 2>>, // P→RH major mapping
                tuple<sequence<1, 1>, sequence<2, 2>>, // P→RH minor mapping
                sequence<1, 1, 2, 2>, // Y→RH major mapping (4 Y dimensions)
                sequence<0, 3, 0, 3>> // Y→RH minor mapping 
``` 

Distribution Encoding Visualization

![](02_reduce2d_input_tile_distribution.png)

`P0[0]→H0[1]`, `P0[1]→H1[1]`, `P1[0]→H0[2]`, `P1[1]→H1[2]`


The distribution creates a 4-level hierarchy:

- **Y1 Y3 dimensions**: `ThreadTile_M = 8`, `ThreadTile_N = 8` - Each thread's register tile. Each thread processes 64 elements.
- **Y0 Y2 dimensions (repeat)**: `Repeat_M = 1`, `Repeat_N = 1` - No additional looping
- **P1 dimensions**: `ThreadPerWarp_M = 4`, `ThreadPerWarp_N = 16` - Thread organization within warps.This implies, 16 threads in the N dimension and 4 threads in the M dimension per warp.
- **P0 dimensions**: `WarpPerBlock_M = 4`, `WarpPerBlock_N = 1` - Warp organization within blocks. This implies, 1 warp in the N dimension and 4 warps in the M dimension per block.

Key Distribution Properties

- **Thread assignment**: Each thread processes a dedicated 8×8 register tile (64 elements)
- **Warp layout**: 4 warps distributed along M dimension

**Mapping Semantics**: The encoding defines relationships between parallel dimensions (P) and hierarchical dimensions (H):

- `P0[0] → H0[1]` warp mapping along M dimension
- `P0[1] → H1[1]` warp mapping along N dimension
- `P1[0] → H0[2]` thread mapping within warps along M dimension 
- `P1[1] → H1[2]` thread mapping within warps along N dimension 


Distribution for `Block_M = 128, Block_N = 128` would look like:
![](02_reduce2d_input_tile_distribution_layout.png)


## Output Tensor Distribution

The output tensor’s distribution is not independent; it is constructed from the input tensor’s distribution. The transformation is performed by `make_reduce_tile_distribution_encoding`, which takes the input’s static tile distribution encoding and a list of reduction dimensions.

This function produces a new distribution encoding for the output tensor, where the specified reduction dimensions are collapsed into the replication dimension.

```cpp
// In BlockReduce2d::MakeYBlockTile()
constexpr auto reduce_dims = sequence<1>{}; // Reduce along N dimension

constexpr auto output_dstr = 
    make_static_tile_distribution(
        detail::make_reduce_tile_distribution_encoding(
            input_tensor.get_tile_distribution().get_static_tile_distribution_encoding(),
            reduce_dims));

auto y_tensor = make_static_distributed_tensor<ComputeDataType>(output_dstr);

```

The resulting distribution looks like:



## Stage 1: Thread-Level Accumulation (BlockReduce2d)

Each thread:
1. Iterates over its assigned fragment of the loaded tile (possibly >1 element along M and/or N depending on `ThreadTile_*` and repeat factors)
2. Applies `reduce_func(acc, x_ij)` locally in registers
3. Produces a per-thread partial buffer shaped as the reduced-version of its input fragment (N dimension collapsed)

**Result**: A register-resident tensor `y_compute` with only M-related indexing left.

```cpp
// Pseudo-code for thread-level accumulation
for (auto repeat_m : repeat_M_range) {
    for (auto repeat_n : repeat_N_range) {
        auto x_fragment = load_thread_tile(repeat_m, repeat_n);
        
        for (auto n_elem : N_elements) {
            for (auto m_elem : M_elements) {
                acc[m_elem] = reduce_func(acc[m_elem], x_fragment[m_elem][n_elem]);
            }
        }
    }
}
```

## Stage 2: Intra-Warp Reduction (BlockReduce2dSync)

**Goal**: Combine per-thread partials inside each warp for dimensions mapped to lane indices.

**Mechanism**: Uses warp shuffle operations (tree-style or butterfly) across threads where the tile distribution encodes that dimension as "thread-parallel". Only dimensions whose fragments reside across lanes (vs. repeats) are reduced here.

```{mermaid}
graph LR
    A["Thread 0<br/>Partial"] --> E["Warp Shuffle<br/>Operations"]
    B["Thread 1<br/>Partial"] --> E
    C["..."] --> E
    D["Thread 31<br/>Partial"] --> E
    E --> F["Warp Result"]
```

After this stage each warp has a consolidated partial result per its M slice.

## Stage 3: Cross-Warp Reduction (BlockReduce2dCrossWarpSync)

Needed only if more than one warp contributes to the same logical M output element.

**Steps**:
1. Lane 0 of each warp writes its reduced per-thread buffer to shared memory (LDS)
2. Block-wide sync
3. All threads (or a subset) cooperatively load the warp partials
4. Perform final accumulation over the warp dimension
5. Overwrite `y_compute` with the cross-warp final

```{mermaid}
graph TD
    A["Warp 0 Lane 0"] --> D["Shared Memory<br/>(LDS)"]
    B["Warp 1 Lane 0"] --> D
    C["Warp N Lane 0"] --> D
    D --> E["Block Sync"]
    E --> F["Cooperative Load"]
    F --> G["Final Accumulation"]
    G --> H["Final Result"]
```

**Shared memory size formula** (simplified):
```
LDS_size = num_warps × thread_buf_size
```
where `thread_buf_size` = per-thread reduced buffer length.

## Final Store

After Stage 3:
- `y_compute` holds the fully reduced tile (vectorizable along M if alignment allows)
- Stored into the transformed Y view (`y_window`), which maps back to the merged kept axis
- Strides for Y are re-derived to allow correct placement in original kept-dimension layout

## Vectorization Rules

Two independent vector size choices:

**Input vector size**:
- Limited by: 16-byte alignment / element size AND `ThreadTile_N`
- Disabled (→1) if innermost reduction axis isn't the last physical axis (non-unit stride)

**Output vector size**:
- Min of 16-byte alignment factor and `ThreadTile_M`

This ensures:
- Coalesced loads on contiguous reduction axis
- Safe packs per thread along non-reduced dimension

```cpp
// Simplified logic
auto input_vector_size = min(
    16 / sizeof(DataType),
    ThreadTile_N,
    is_contiguous ? max_vector : 1
);

auto output_vector_size = min(
    16 / sizeof(DataType), 
    ThreadTile_M
);
```

## High-Level Kernel Pseudocode

```cpp
template<typename Problem, typename Policy>
void reduce_kernel() {
    // Stage 0: Setup and load
    auto x_window = make_tile_window(x_view, block_work_idx);
    auto y_window = make_tile_window(y_view, block_work_idx);
    
    // Stage 1: Thread-level accumulation  
    auto x_tile = load_tile(x_window);
    auto y_compute = block_reduce_2d(x_tile);
    
    // Stage 2: Intra-warp reduction
    y_compute = block_reduce_2d_sync(y_compute);
    
    // Stage 3: Cross-warp reduction (if needed)
    if constexpr (requires_cross_warp_reduction) {
        y_compute = block_reduce_2d_cross_warp_sync(y_compute);
    }
    
    // Final store
    store_tile(y_window, y_compute);
}
```

## Example Parameterization

From `reduce.cpp`:

```cpp
using Reduce2dShape = Reduce2dShape<
    256,    // Block_M 
    256,    // Block_N
    8,      // ThreadTile_M
    8,      // ThreadTile_N  
    4,      // WarpPerBlock_M
    1,      // WarpPerBlock_N
    1,      // Repeat_M
    4       // Repeat_N
>;
```

**Interpretation**:
- 4 warps along M; 1 warp along N
- Each warp covers 32 rows × 128 columns before per-thread partitioning
- Thread-level tiles shape MxN subdivides warp tile
- Vector widths (load/store) = 8 (subject to alignment and stride checks)

## Edge Cases & Constraints

**IsSupportedArgument**:
- `y_continuous_dim % ThreadTile_N == 0` (mapping correctness)
- Last input stride must be 1 (contiguous tail for vector loads)
- Padding ensures full tiles even when M or N not multiples of block tile sizes
- Identity element fills padded regions (must not affect sum)

## Memory & Synchronization Summary

| Stage | Storage | Sync Needed | Primitive |
|-------|---------|-------------|-----------|
| Thread accumulate | Registers | None | Plain loop |
| Warp reduce | Registers via shuffles | Implicit (warp-synchronous) | `__shfl_*` |
| Cross-warp reduce | Shared memory + registers | `block_sync_lds()` | LDS + loop |

## Visualization: Overall Dataflow

```{mermaid}
graph TD
    A["Input Tensor X"] --> B["Transform to 2D [M,N]"]
    B --> C["Load Tile to Registers"]
    C --> D["Stage 1: Thread Accumulation"]
    D --> E["Stage 2: Warp Shuffle Reduction"]
    E --> F{"Multiple Warps<br/>per Output?"}
    F -->|Yes| G["Stage 3: Cross-Warp<br/>LDS Reduction"]
    F -->|No| H["Final Store"]
    G --> H
    H --> I["Output Tensor Y"]
```

## Extensibility Notes

**To change reduction op**: Supply a `ReduceOp` functor with `GetIdentityValue<ComputeDataType>()` and `operator()`.

**To alter tile shape**: Redefine `Reduce2dShape` (Block sizes, warp decomposition, thread tile).

**To disable Stage 3**: Ensure only one warp maps to each output M element (`WarpPerBlock_M == 1` or mapping collapses warp dimension).

**Alternative cross-warp strategies**: (e.g. tree) can plug in by replacing `GetBlockReduce2dCrossWarpSync()` in the policy.