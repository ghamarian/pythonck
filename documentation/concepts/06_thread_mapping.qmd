---
title: "Thread Mapping - Connecting to Hardware"
format: 
  live-html:
    mermaid:
      theme: default
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import make_static_tile_distribution
from pytensor.static_distributed_tensor import make_static_distributed_tensor
from pytensor.tensor_view import make_naive_tensor_view_packed
from pytensor.tile_window import make_tile_window
from pytensor.partition_simulation import set_global_thread_position
```

The final piece of the puzzle: how threads get their unique IDs and how that maps to specific data, connecting our mathematical abstractions to physical hardware.

Up to this point, we've learned about encodings, transformations, and distributed tensors. But there's one crucial question remaining: **How do actual GPU threads know which data to process?**

This is where thread mapping comes in - the bridge between our mathematical abstractions and the physical hardware that executes our code.

## 🎮 **Interactive Exploration**

Explore thread mapping concepts interactively:

**[🧵 Thread Visualization App](../../thread_visualization_app.py)** - Visualize GPU thread coordinate mapping and access patterns. Understand how individual threads access distributed tensor data.

## Thread Identification and Partition Indices

Before threads can process data, they need to know who they are and what work they're responsible for.

### Hardware Thread Identification

In GPU hardware, threads are organized hierarchically:

```cpp
// CUDA/HIP thread identification
__device__ void get_thread_coordinates()
{
    // Grid-level coordinates (which block)
    int block_x = blockIdx.x;
    int block_y = blockIdx.y;
    int block_z = blockIdx.z;
    
    // Block-level coordinates (which thread in block)
    int thread_x = threadIdx.x;
    int thread_y = threadIdx.y;
    int thread_z = threadIdx.z;
    
    // Warp identification
    int warp_id = threadIdx.x / 32;  // 32 threads per warp
    int lane_id = threadIdx.x % 32;  // Position within warp
    
    // Global thread ID calculation
    int global_thread_id = blockIdx.x * blockDim.x + threadIdx.x;
}
```

### C++ Thread Mapping in CK

Composable Kernel abstracts thread identification into partition indices:

```cpp
// From tile_partition.hpp
template <typename ThreadLayout>
struct tile_partition
{
    CK_TILE_DEVICE static constexpr index_t get_thread_idx()
    {
        return threadIdx.x;
    }
    
    CK_TILE_DEVICE static constexpr index_t get_block_idx()
    {
        return blockIdx.x;
    }
    
    // Convert to multi-dimensional partition index
    template <index_t NumDim>
    CK_TILE_DEVICE static constexpr auto get_partition_index()
    {
        constexpr auto thread_layout = ThreadLayout{};
        
        // Convert linear thread ID to multi-dimensional index
        return thread_layout.template get_index<NumDim>(get_thread_idx());
    }
};
```

```{=html}
<div class="mermaid">
graph TB
    subgraph "GPU Device"
        subgraph "Thread Block"
            subgraph "Warp 0"
                T0["Thread 0<br/>lane_id=0"]
                T1["Thread 1<br/>lane_id=1"]
                T2["..."]
                T31["Thread 31<br/>lane_id=31"]
            end
            
            subgraph "Warp 1"
                T32["Thread 32<br/>lane_id=0"]
                T33["Thread 33<br/>lane_id=1"]
                T34["..."]
                T63["Thread 63<br/>lane_id=31"]
            end
            
            W2["Warp 2"]
            W3["..."]
            W7["Warp 7"]
        end
    end
    
    subgraph "Thread Identification"
        TID["Thread ID = blockIdx.x * blockDim.x + threadIdx.x"]
        WID["Warp ID = threadIdx.x / 32"]
        LID["Lane ID = threadIdx.x % 32"]
    end
    
    subgraph "P-space Mapping"
        P["P-coordinates<br/>NDimP=1: [thread_id]<br/>NDimP=2: [warp_id, lane_id]"]
    end
    
    T0 --> TID
    TID --> WID
    TID --> LID
    WID --> P
    LID --> P
    
    style T0 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style T32 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style P fill:#fff3e0,stroke:#f57c00,stroke-width:3px
</div>
```

```{pyodide}
print("🎯 GPU Thread Hierarchy")
print("=" * 50)

# RMSNorm example - real-world layer normalization parameters
repeat_m, warp_per_block_m, thread_per_warp_m, vector_m = 4, 2, 8, 4
repeat_n, warp_per_block_n, thread_per_warp_n, vector_n = 4, 2, 8, 4

# Create RMSNorm tile distribution encoding
encoding = TileDistributionEncoding(
    rs_lengths=[],                                           # No replication
    hs_lengthss=[[repeat_m, warp_per_block_m, thread_per_warp_m, vector_m],  # M dimension hierarchy
                 [repeat_n, warp_per_block_n, thread_per_warp_n, vector_n]], # N dimension hierarchy
    ps_to_rhss_major=[[1, 2], [1, 2]],                     # P→RH major mapping
    ps_to_rhss_minor=[[1, 1], [2, 2]],                     # P→RH minor mapping
    ys_to_rhs_major=[1, 1, 2, 2],                          # Y→RH major mapping
    ys_to_rhs_minor=[0, 3, 0, 3]                           # Y→RH minor mapping
)

print("📋 RMSNorm Configuration (Real-World Example):")
print(f"  Repeat (M, N): ({repeat_m}, {repeat_n})")
print(f"  Warps per block (M, N): ({warp_per_block_m}, {warp_per_block_n})")
print(f"  Threads per warp (M, N): ({thread_per_warp_m}, {thread_per_warp_n})")
print(f"  Vector size (M, N): ({vector_m}, {vector_n})")

# Calculate thread organization
threads_per_block = warp_per_block_m * warp_per_block_n * thread_per_warp_m * thread_per_warp_n
warps_per_block = warp_per_block_m * warp_per_block_n

print(f"\n📊 Thread Organization:")
print(f"  Threads per block: {threads_per_block}")
print(f"  Warps per block: {warps_per_block}")
print(f"  P dimensions: {encoding.ndim_p}")
print(f"  Y dimensions: {encoding.ndim_y}")
```

### Thread Hierarchy Structure

The hardware organizes threads in a specific hierarchy:

**🔹 Block Level**: Groups of warps working together
- `{warp_per_block_m}×{warp_per_block_n}` warps per block
- Shared memory and synchronization scope
- Block-level coordination possible

**🔹 Warp Level**: Groups of threads executing in lockstep
- `{thread_per_warp_m}×{thread_per_warp_n}` threads per warp
- SIMD execution (all threads execute same instruction)
- Warp-level primitives (shuffle, vote, etc.)

**🔹 Thread Level**: Individual execution units
- `{vector_m}×{vector_n}` elements per thread
- Independent register space
- Vector operations on multiple elements

### Thread ID Mapping

Each thread gets a unique ID that maps to its position in the hierarchy:

```{pyodide}
print("📊 Example Thread ID Mappings:")
print("=" * 50)

# Show example thread mappings
thread_count = 0
for warp_m in range(min(2, warp_per_block_m)):
    for warp_n in range(min(2, warp_per_block_n)):
        for thread_m in range(min(2, thread_per_warp_m)):
            for thread_n in range(min(2, thread_per_warp_n)):
                # Calculate global thread ID
                global_thread_id = (warp_m * warp_per_block_n * thread_per_warp_m * thread_per_warp_n +
                                  warp_n * thread_per_warp_m * thread_per_warp_n +
                                  thread_m * thread_per_warp_n + thread_n)
                
                print(f"  Thread {global_thread_id}: Warp[{warp_m},{warp_n}] Thread[{thread_m},{thread_n}]")
                
                thread_count += 1
                if thread_count >= 8:  # Show first 8 threads
                    print("  ... (showing first 8 threads)")
                    break
            if thread_count >= 8:
                break
        if thread_count >= 8:
            break
    if thread_count >= 8:
        break
```

## Thread-to-Data Mapping

Once threads know their IDs, they need to map those IDs to specific data elements.

```{=html}
<div class="mermaid">
graph TB
    subgraph "Thread to Data Mapping"
        subgraph "Thread Grid"
            T00["Thread[0,0]<br/>Warp 0"]
            T01["Thread[0,1]<br/>Warp 0"]
            T10["Thread[1,0]<br/>Warp 1"]
            T11["Thread[1,1]<br/>Warp 1"]
        end
        
        subgraph "Data Tiles"
            D00["Data[0:4, 0:4]<br/>16 elements"]
            D01["Data[0:4, 4:8]<br/>16 elements"]
            D10["Data[4:8, 0:4]<br/>16 elements"]
            D11["Data[4:8, 4:8]<br/>16 elements"]
        end
        
        subgraph "Memory Access"
            MA["Coalesced Access<br/>Adjacent threads → Adjacent memory"]
        end
    end
    
    T00 --> D00
    T01 --> D01
    T10 --> D10
    T11 --> D11
    
    D00 --> MA
    D01 --> MA
    
    style T00 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style D00 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style MA fill:#fff3e0,stroke:#f57c00,stroke-width:2px
</div>
```

### Data Distribution Pattern

The RMSNorm operation distributes tensor data across threads in a structured pattern:

```{pyodide}
print("🎯 RMSNorm Data Distribution Pattern")
print("=" * 50)

# Calculate total tensor size processed by this tile distribution
total_m = repeat_m * warp_per_block_m * thread_per_warp_m * vector_m
total_n = repeat_n * warp_per_block_n * thread_per_warp_n * vector_n

print(f"📊 Tensor Organization:")
print(f"  Total tensor size (M×N): {total_m}×{total_n}")
print(f"  Elements per thread: {vector_m}×{vector_n} = {vector_m * vector_n}")

print(f"\n📋 Hierarchical Data Distribution:")
print(f"  🔹 Block Level: {repeat_m}×{repeat_n} iterations")
print(f"  🔹 Warp Level: {warp_per_block_m}×{warp_per_block_n} warps per block")
print(f"  🔹 Thread Level: {thread_per_warp_m}×{thread_per_warp_n} threads per warp")
print(f"  🔹 Vector Level: {vector_m}×{vector_n} elements per thread")
```

### Thread Work Assignment

Each thread is assigned a specific rectangular region of the tensor:

```{pyodide}
print("📋 Thread Work Assignment Example")
print("=" * 50)

# Pick a specific thread and show its work
example_warp_m, example_warp_n = 0, 0
example_thread_m, example_thread_n = 0, 0

# Calculate this thread's data region
thread_start_m = example_warp_m * thread_per_warp_m * vector_m + example_thread_m * vector_m
thread_end_m = thread_start_m + vector_m
thread_start_n = example_warp_n * thread_per_warp_n * vector_n + example_thread_n * vector_n
thread_end_n = thread_start_n + vector_n

print(f"Example thread: Warp[{example_warp_m},{example_warp_n}] Thread[{example_thread_m},{example_thread_n}]")
print(f"  Data region (M): [{thread_start_m}:{thread_end_m})")
print(f"  Data region (N): [{thread_start_n}:{thread_end_n})")
print(f"  Total elements: {vector_m}×{vector_n} = {vector_m * vector_n}")

print(f"\n🔍 Thread Data Regions (first few threads):")
shown_threads = 0
for warp_m in range(min(2, warp_per_block_m)):
    for thread_m in range(min(2, thread_per_warp_m)):
        for warp_n in range(min(2, warp_per_block_n)):
            for thread_n in range(min(2, thread_per_warp_n)):
                start_m = warp_m * thread_per_warp_m * vector_m + thread_m * vector_m
                end_m = start_m + vector_m
                start_n = warp_n * thread_per_warp_n * vector_n + thread_n * vector_n
                end_n = start_n + vector_n
                
                print(f"  W[{warp_m},{warp_n}]T[{thread_m},{thread_n}]: M[{start_m}:{end_m}) N[{start_n}:{end_n})")
                
                shown_threads += 1
                if shown_threads >= 6:
                    print("  ... (showing first 6 for brevity)")
                    break
            if shown_threads >= 6:
                break
        if shown_threads >= 6:
            break
    if shown_threads >= 6:
        break
```

## Thread Cooperation Patterns

Threads don't work in isolation - they cooperate at different levels to achieve optimal performance.

### Warp-Level Cooperation

Threads within a warp execute in lockstep (SIMD):

**🤝 Warp-Level Cooperation**
- **Warps per block**: `{warp_per_block_m}×{warp_per_block_n}`
- **Threads per warp**: `{thread_per_warp_m}×{thread_per_warp_n}`
- **Cooperation pattern**: Threads within a warp process adjacent data
- **Synchronization**: Warp-level SIMD execution

### Block-Level Cooperation

Threads within a block can share data and synchronize:

**🏗️ Block-Level Cooperation**
- **Shared memory**: All threads in block can access shared memory
- **Synchronization**: `__syncthreads()` barriers available
- **Data sharing**: Threads can exchange intermediate results
- **Collective operations**: Reduction, broadcast across block

### Vector-Level Processing

Each thread processes multiple elements efficiently:

**⚡ Vector-Level Processing**
- **Elements per thread**: `{vector_m}×{vector_n}` elements
- **Memory coalescing**: Adjacent threads access adjacent memory
- **Vectorization**: Hardware can combine multiple operations
- **Register efficiency**: Multiple elements in registers

## Memory Access Patterns

The thread mapping directly affects memory access efficiency.

### C++ Implementation of Memory Access

Here's how CK implements efficient memory access patterns:

```cpp
// Coalesced memory access pattern
template <typename DataType, index_t VectorSize>
__device__ void coalesced_load(const DataType* __restrict__ src,
                               DataType* __restrict__ dst,
                               index_t tid)
{
    // Each thread loads VectorSize elements
    // Adjacent threads access adjacent memory
    constexpr index_t stride = blockDim.x;
    
    // Vectorized load for efficiency
    using vector_t = vector_type_t<DataType, VectorSize>;
    
    // Calculate aligned address
    const vector_t* src_vec = reinterpret_cast<const vector_t*>(
        src + tid * VectorSize);
    
    // Single vectorized load instruction
    vector_t data = *src_vec;
    
    // Store to registers
    reinterpret_cast<vector_t*>(dst)[0] = data;
}

// CK's distributed tensor load implementation
template <typename DistributedTensor>
__device__ void load_tile_window(DistributedTensor& dist_tensor,
                                const auto& tile_window)
{
    // Get thread's partition index
    constexpr auto partition = tile_partition::get_partition_index();
    
    // Each thread loads its assigned data
    tile_window.load(dist_tensor, partition);
    
    // Hardware automatically coalesces adjacent thread accesses
}
```

### Memory Access Optimization Techniques

CK uses several techniques to optimize memory access:

```cpp
// 1. Vector loads for maximum bandwidth
template <index_t N>
using vector_load_t = conditional_t<N == 1, float,
                     conditional_t<N == 2, float2,
                     conditional_t<N == 4, float4,
                                           float>>>;

// 2. Swizzling to avoid bank conflicts
template <index_t BankSize = 32>
__device__ index_t swizzle_offset(index_t tid, index_t offset)
{
    // Rotate access pattern to avoid conflicts
    return (offset + (tid / BankSize)) % BankSize;
}

// 3. Prefetching for latency hiding
__device__ void prefetch_next_tile(const float* src, index_t offset)
{
    // Prefetch to L2 cache
    __builtin_prefetch(src + offset, 0, 3);
}
```

### Coalesced Memory Access

```{pyodide}
print("🚀 Memory Access Pattern Analysis")
print("=" * 50)

# Create a tile distribution to analyze memory patterns
tile_distribution = make_static_tile_distribution(encoding)

# Simulate tensor data
tensor_shape = [total_m, total_n]
data = np.arange(np.prod(tensor_shape), dtype=np.float32)
tensor_view = make_naive_tensor_view_packed(data, tensor_shape)

print(f"📊 Memory Access Analysis:")
print(f"  Tensor shape: {tensor_shape}")
print(f"  Memory layout: Row-major")
print(f"  Vector size per thread: {vector_m}×{vector_n}")

# Analyze access pattern for a few threads
print(f"\n🔍 Memory Access Pattern (first few threads):")

# Set up different thread positions and show their access patterns
thread_examples = [
    (0, 0),  # First thread
    (0, 1),  # Second thread in same warp
    (1, 0),  # First thread in next warp
]

for thread_p0, thread_p1 in thread_examples:
    # Set thread position
    set_global_thread_position(thread_p0, thread_p1)
    
    # Create tile window
    tile_window = make_tile_window(
        tensor_view=tensor_view,
        window_lengths=[total_m, total_n],
        origin=[0, 0],
        tile_distribution=tile_distribution
    )
    
    # Load data to see access pattern
    loaded_tensor = tile_window.load()
    
    print(f"  Thread P=[{thread_p0},{thread_p1}]:")
    print(f"    Elements loaded: {loaded_tensor.get_num_of_elements()}")
    
    # Show first few elements accessed
    sample_values = []
    for y0 in range(min(2, vector_m)):
        for y1 in range(min(2, vector_n)):
            try:
                value = loaded_tensor.get_element([y0, y1])
                sample_values.append(value)
            except:
                pass
    
    if sample_values:
        print(f"    Sample values: {sample_values[:4]}")
```

### Memory Efficiency Benefits

The structured thread mapping provides several memory efficiency benefits:

**🎯 Memory Coalescing Benefits:**
- **Adjacent access**: Threads in same warp access adjacent memory locations
- **Cache efficiency**: Related data loaded together into cache lines
- **Bandwidth utilization**: Maximum memory bandwidth achieved
- **Reduced latency**: Fewer memory transactions needed

**⚡ Performance Characteristics:**
- **Predictable patterns**: Access patterns known at compile time
- **Vectorization**: Hardware can optimize vector operations
- **Reduced overhead**: No complex address calculations at runtime
- **Scalability**: Pattern scales efficiently with thread count

## Practical Thread Mapping Example

### Complete C++ Kernel Example

Here's a complete example showing how thread mapping works in a real CK kernel:

```cpp
// RMSNorm kernel using CK's thread mapping
template <typename DataType,
          typename ComputeType,
          index_t BlockSize,
          index_t VectorSize>
__global__ void rmsnorm_kernel(const DataType* __restrict__ x,
                              DataType* __restrict__ y,
                              const DataType* __restrict__ weight,
                              ComputeType epsilon,
                              index_t hidden_size)
{
    // 1. Thread identification
    const index_t tid = threadIdx.x;
    const index_t bid = blockIdx.x;
    
    // 2. Create tile distribution encoding
    // This would be defined based on your specific RMSNorm pattern
    using Encoding = tile_distribution_encoding<
        sequence<>,                          // No replication
        tuple<sequence<4, 2>, sequence<4, 2>>, // H dimensions
        tuple<sequence<1>, sequence<2>>,     // P to RH major
        tuple<sequence<0>, sequence<0>>,     // P to RH minor
        sequence<1, 2>,                      // Y to RH major
        sequence<0, 0>                       // Y to RH minor
    >;
    constexpr auto tile_dist = make_static_tile_distribution(Encoding{});
    
    // 3. Get thread's partition index from distribution
    const auto partition_idx = tile_dist._get_partition_index();
    
    // 4. Shared memory for reduction
    __shared__ ComputeType shared_sum[BlockSize];
    
    // 5. Create tensor view and tile window
    auto x_view = make_naive_tensor_view<address_space_enum::global>(
        x + bid * hidden_size,
        make_tuple(hidden_size),
        make_tuple(number<1>{})
    );
    
    auto x_window = make_tile_window(
        x_view,
        make_tuple(hidden_size),
        make_tuple(number<0>{}),
        tile_dist);
    
    // 6. Each thread processes its assigned elements
    ComputeType thread_sum = 0;
    static_for<0, VectorSize, 1>{}([&](auto i) {
        // Access pattern would depend on your tile window setup
        // This is conceptual - actual implementation varies
        thread_sum += val * val;
    });
    
    // 7. Warp-level reduction
    thread_sum = warp_reduce_sum<WarpSize>(thread_sum);
    
    // 8. Block-level reduction
    if (tid % WarpSize == 0) {
        shared_sum[tid / WarpSize] = thread_sum;
    }
    __syncthreads();
    
    // 9. Final reduction by first warp
    if (tid < BlockSize / WarpSize) {
        thread_sum = shared_sum[tid];
        thread_sum = warp_reduce_sum<BlockSize / WarpSize>(thread_sum);
    }
    
    // 10. Compute RMS and normalize
    if (tid == 0) {
        shared_sum[0] = rsqrt(thread_sum / hidden_size + epsilon);
    }
    __syncthreads();
    
    const ComputeType rms_recip = shared_sum[0];
    
    // 11. Write normalized output
    auto y_window = make_tile_window(
        make_tensor_view<address_space_enum::global>(y + bid * hidden_size),
        tile_dist);
    
    static_for<0, VectorSize, 1>{}([&](auto i) {
        auto idx = tile_dist.get_tensor_coordinate(partition_idx, i);
        ComputeType val = static_cast<ComputeType>(x_window.get(idx));
        ComputeType w = static_cast<ComputeType>(weight[idx[1]]);
        y_window.set(idx, static_cast<DataType>(val * rms_recip * w));
    });
}
```

### Key Thread Mapping Concepts in Action

1. **Thread-to-Data Assignment**: Each thread gets a unique `partition_idx`
2. **Vectorized Access**: Each thread processes `VectorSize` elements
3. **Warp Cooperation**: Threads within a warp perform reductions
4. **Block Synchronization**: All threads synchronize for final result
5. **Coalesced Memory**: Adjacent threads access adjacent memory

## Practical Thread Mapping Example

Let's see how thread mapping works in practice with a complete example:

```{pyodide}
print("🎯 Complete Thread Mapping Example")
print("=" * 50)

# Create the tile distribution
tile_distribution = make_static_tile_distribution(encoding)

# Create sample tensor data
tensor_shape = [64, 64]  # Smaller for demonstration
data = np.arange(np.prod(tensor_shape), dtype=np.float32)
tensor_view = make_naive_tensor_view_packed(data, tensor_shape)

print(f"📊 Example Setup:")
print(f"  Tensor shape: {tensor_shape}")
print(f"  Total elements: {np.prod(tensor_shape)}")
print(f"  Tile distribution: RMSNorm pattern")

# Show how different threads access different data
print(f"\n🔍 Thread-by-Thread Data Access:")

example_threads = [(0, 0), (0, 1), (1, 0), (1, 1)]
for i, (p0, p1) in enumerate(example_threads):
    # Set thread position
    set_global_thread_position(p0, p1)
    
    # Create tile window and load data
    tile_window = make_tile_window(
        tensor_view=tensor_view,
        window_lengths=[32, 32],  # Window size
        origin=[0, 0],
        tile_distribution=tile_distribution
    )
    
    loaded_tensor = tile_window.load()
    
    print(f"  Thread {i} (P=[{p0},{p1}]):")
    print(f"    Elements: {loaded_tensor.get_num_of_elements()}")
    
    # Show data range accessed by this thread
    values = []
    for y0 in range(min(2, vector_m)):
        for y1 in range(min(2, vector_n)):
            try:
                value = loaded_tensor.get_element([y0, y1])
                values.append(value)
            except:
                pass
    
    if values:
        print(f"    Value range: [{min(values):.0f}, {max(values):.0f}]")
        print(f"    Sample: {values[:4]}")
```

## Testing Your Understanding

Let's verify your understanding of thread mapping concepts:

```{pyodide}
print("🧪 Testing Thread Mapping Understanding")
print("=" * 50)

def test_rmsnorm_encoding_creation():
    """Test that we can create the RMSNorm encoding."""
    try:
        encoding = TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[4, 2, 8, 4], [4, 2, 8, 4]],
            ps_to_rhss_major=[[1, 2], [1, 2]],
            ps_to_rhss_minor=[[1, 1], [2, 2]],
            ys_to_rhs_major=[1, 1, 2, 2],
            ys_to_rhs_minor=[0, 3, 0, 3]
        )
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

def test_thread_organization():
    """Test that the thread organization makes sense."""
    repeat_m, warp_per_block_m, thread_per_warp_m, vector_m = 4, 2, 8, 4
    repeat_n, warp_per_block_n, thread_per_warp_n, vector_n = 4, 2, 8, 4
    
    threads_per_block = warp_per_block_m * warp_per_block_n * thread_per_warp_m * thread_per_warp_n
    expected_threads = 2 * 2 * 8 * 8  # 256 threads
    
    return threads_per_block == expected_threads

def test_memory_efficiency():
    """Test that vector access is efficient."""
    vector_m, vector_n = 4, 4
    elements_per_thread = vector_m * vector_n
    
    # Each thread should handle multiple elements for efficiency
    return elements_per_thread >= 4

def test_tile_distribution_creation():
    """Test that we can create tile distribution from encoding."""
    try:
        encoding = TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[4, 2, 8, 4], [4, 2, 8, 4]],
            ps_to_rhss_major=[[1, 2], [1, 2]],
            ps_to_rhss_minor=[[1, 1], [2, 2]],
            ys_to_rhs_major=[1, 1, 2, 2],
            ys_to_rhs_minor=[0, 3, 0, 3]
        )
        tile_distribution = make_static_tile_distribution(encoding)
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

# Run tests
tests = [
    ("RMSNorm encoding creation", test_rmsnorm_encoding_creation),
    ("Thread organization", test_thread_organization),
    ("Memory efficiency", test_memory_efficiency),
    ("Tile distribution creation", test_tile_distribution_creation)
]

print("Running thread mapping tests:")
for test_name, test_func in tests:
    try:
        result = test_func()
        status = "✅ PASS" if result else "❌ FAIL"
        print(f"  {status}: {test_name}")
    except Exception as e:
        print(f"  ❌ ERROR: {test_name} - {str(e)}")
```

## Key Takeaways

Thread mapping is the crucial bridge between mathematical abstractions and physical hardware execution:

**🎯 Thread Identification:**

1. **Hierarchical Organization**: Threads organized in blocks → warps → threads → vectors
   - ✅ Each level has specific cooperation capabilities
   - ✅ Hardware provides efficient primitives at each level
   - ✅ Thread IDs map directly to data regions
   - ✅ Predictable and efficient execution patterns

2. **Data Assignment**: Each thread gets a specific rectangular region
   - ✅ Work distributed evenly across threads
   - ✅ Memory access patterns optimized for coalescing
   - ✅ Vector operations maximize throughput
   - ✅ Scalable across different hardware configurations

3. **Cooperation Patterns**: Threads cooperate at multiple levels
   - ✅ Warp-level SIMD execution for efficiency
   - ✅ Block-level shared memory and synchronization
   - ✅ Vector-level processing for maximum throughput
   - ✅ Hierarchical coordination for complex operations

**🚀 Performance Benefits:**

- **Memory Coalescing**: Adjacent threads access adjacent memory for optimal bandwidth
- **Cache Efficiency**: Related data loaded together, reducing memory latency
- **Vectorization**: Hardware can optimize multiple operations per thread
- **Predictable Patterns**: Compile-time optimization of access patterns

**💡 Why This Matters:**

Thread mapping connects all the previous concepts (encodings, transformations, distributions) to actual hardware execution. It's the final piece that makes tile distribution practical for real-world GPU programming.

The RMSNorm example shows how a real operation uses these concepts to achieve optimal performance on modern GPU hardware. Every thread knows exactly what data to process, how to access it efficiently, and how to cooperate with other threads - all determined by the mathematical encoding we started with!

This completes the journey from basic memory concepts to hardware-optimized execution. You now understand the complete tile distribution system from mathematical foundations to practical implementation. 