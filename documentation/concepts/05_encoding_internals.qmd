---
title: "Encoding Internals - The Internal Machinery"
format: 
  live-html:
    mermaid:
      theme: default
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import make_static_tile_distribution
```

## Overview

The tile distribution encoding system represents the core mathematical framework that transforms high-level tensor distribution specifications into concrete, optimized GPU kernel implementations. This sophisticated compile-time machinery bridges the gap between abstract mathematical descriptions and executable coordinate transformations, enabling the Composable Kernel framework to generate highly efficient code for complex tensor operations.

At its heart, the encoding system defines how multi-dimensional tensor data is distributed across GPU processing elements through a hierarchical decomposition scheme. By specifying relationships between different coordinate spaces - replication (R), hierarchical (H), partition (P), and yield (Y) dimensions - the encoding provides a complete blueprint for data layout and access patterns that can be resolved entirely at compile time.

## Encoding Structure

```{=html}
<div class="mermaid">
graph TB
    subgraph "Encoding Components"
        RS["R-space Lengths<br/>Replication dimensions"]
        HS["H-space Lengths<br/>Hierarchical decomposition<br/>[[2,2],[2,2]]"]
        P2RH["P‚ÜíRH Mappings<br/>Thread to hierarchy<br/>Major/Minor"]
        Y2RH["Y‚ÜíRH Mappings<br/>Element to hierarchy<br/>Major/Minor"]
    end
    
    subgraph "Generated Components"
        ADAPTOR["ps_ys_to_xs_adaptor<br/>Coordinate transformer"]
        DESC["ys_to_d_descriptor<br/>Memory linearizer"]
        ENC["Encoding<br/>Original specification"]
    end
    
    subgraph "Transformation Chain"
        T1["Replicate<br/>Transform"]
        T2["Unmerge<br/>Transform"]
        T3["Merge<br/>Transform"]
    end
    
    RS --> T1
    HS --> T2
    P2RH --> ADAPTOR
    Y2RH --> ADAPTOR
    
    T1 --> T2
    T2 --> T3
    T3 --> ADAPTOR
    
    HS --> DESC
    Y2RH --> DESC
    
    style RS fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style HS fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style ADAPTOR fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style DESC fill:#fff3e0,stroke:#f57c00,stroke-width:3px
</div>
```

## What is Tile Distribution Encoding?

The `tile_distribution_encoding` struct serves as a compile-time blueprint for defining complex tensor data layouts in GPU programming. It addresses a fundamental challenge in high-performance computing: how to map abstract tensor operations to the hierarchical memory and execution architecture of modern GPUs while maintaining both performance and programmability.

The encoding system operates through a multi-stage transformation pipeline that converts high-level specifications into concrete coordinate mappings. This pipeline consists of three primary components:

The **ps_ys_to_xs_adaptor** performs the crucial transformation from processing element coordinates (P) and logical access pattern coordinates (Y) to physical tensor coordinates (X). This adaptor encodes the complete mapping logic through a chain of coordinate transformations including replicate, unmerge, and merge operations.

The **ys_to_d_descriptor** manages the linearization of the multi-dimensional Y coordinate space into a one-dimensional data space suitable for register allocation. This component ensures efficient register utilization by mapping logical access patterns to physical storage locations.

The **transformation chains** are automatically constructed from the encoding parameters through template metaprogramming, ensuring zero runtime overhead. Each transformation in the chain corresponds to a specific coordinate space manipulation, collectively implementing the complete distribution strategy.

The C++ implementation leverages advanced template metaprogramming techniques:

```cpp
// From ck_tile/core/tensor/tile_distribution_encoding.hpp
template <typename RsLengths_,    // Replication dimension lengths
          typename HsLengthss_,   // Hierarchical dimension lengths
          typename Ps2RHssMajor_, // P to RH mapping (major)
          typename Ps2RHssMinor_, // P to RH mapping (minor)
          typename Ys2RHsMajor_,  // Y to RH mapping (major)
          typename Ys2RHsMinor_>  // Y to RH mapping (minor)
struct tile_distribution_encoding
{
    // All computations resolved at compile time
    static constexpr index_t NDimX = HsLengthss::size();
    static constexpr index_t NDimP = Ps2RHssMajor::size();
    static constexpr index_t NDimY = Ys2RHsMajor::size();
    static constexpr index_t NDimR = RsLengths::size();
    
    // Nested detail struct performs complex compile-time calculations
    struct detail
    {
        // Precomputed mappings and transformations
        static constexpr auto get_h_dim_lengths_prefix_sum();
        static constexpr auto get_uniformed_idx_y_to_h();
        // ... extensive compile-time computation ...
    };
};
```

## Encoding Structure Deep Dive

The tile distribution encoding employs a sophisticated type system that captures the complete specification of tensor distribution patterns at compile time. Understanding this structure is essential for leveraging the full power of the CK framework's optimization capabilities.

The encoding revolves around several interconnected dimension types that collectively define the distribution strategy:

```{pyodide}
print("Tile Distribution Encoding Structure")
print("=" * 50)

# Create a simple encoding to examine
encoding = TileDistributionEncoding(
    rs_lengths=[],                    # No replication
    hs_lengthss=[[2, 2], [2, 2]],   # 2x2 hierarchical tiles
    ps_to_rhss_major=[[1], [2]],     # P0 to H1, P1 to H2  
    ps_to_rhss_minor=[[0], [0]],     # Use first component
    ys_to_rhs_major=[1, 1, 2, 2],    # Y mapping to H
    ys_to_rhs_minor=[0, 1, 0, 1]     # Y component selection
)

print("Encoding Parameters:")
print(f"  rs_lengths: {encoding.rs_lengths}")
print(f"  hs_lengthss: {encoding.hs_lengthss}")
print(f"  ps_to_rhss_major: {encoding.ps_to_rhss_major}")
print(f"  ps_to_rhss_minor: {encoding.ps_to_rhss_minor}")
print(f"  ys_to_rhs_major: {encoding.ys_to_rhs_major}")
print(f"  ys_to_rhs_minor: {encoding.ys_to_rhs_minor}")

print("\nEncoding created successfully!")
```

## C++ Template Implementation

The encoding system is implemented as a C++ template struct that leverages compile-time computation for maximum performance:

```cpp
// From include/ck_tile/core/tensor/tile_distribution_encoding.hpp

template <typename RsLengths_,      // Replication dimensions
          typename HsLengthss_,     // Hierarchical dimensions (tuple of tuples)
          typename Ps2RHssMajor_,   // P‚ÜíRH major mapping
          typename Ps2RHssMinor_,   // P‚ÜíRH minor mapping
          typename Ys2RHsMajor_,    // Y‚ÜíRH major mapping
          typename Ys2RhsMinor_,    // Y‚ÜíRH minor mapping
          typename RHs2Xs_>         // RH‚ÜíX final mapping
struct tile_distribution_encoding
{
    using rs_lengths_type = RsLengths_;
    using hs_lengthss_type = HsLengthss_;
    
    // Static member functions for compile-time access
    CK_TILE_HOST_DEVICE static constexpr auto get_rs_lengths()
    {
        return rs_lengths_type{};
    }
    
    CK_TILE_HOST_DEVICE static constexpr auto get_hs_lengthss() 
    {
        return hs_lengthss_type{};
    }
    
    // Compute total number of dimensions
    CK_TILE_HOST_DEVICE static constexpr index_t get_num_of_dimension_p()
    {
        return size(Ps2RHssMajor_{});
    }
    
    CK_TILE_HOST_DEVICE static constexpr index_t get_num_of_dimension_y()
    {
        return size(Ys2RHsMajor_{});
    }
};
```

### Key C++ Features Used

1. **Template Metaprogramming**: All parameters are types, not values, enabling compile-time optimization
2. **Constexpr Functions**: Everything is computed at compile time
3. **Type Aliases**: Clean access to template parameters
4. **Static Member Functions**: No runtime overhead

## Parameter Breakdown

Each template parameter in the encoding system serves a specific purpose in defining the overall distribution strategy. These parameters work together to create a complete specification that can be transformed into efficient GPU code.

### R-Dimensions: Replication Specification

The `RsLengths` parameter defines dimensions that are replicated across processing units, enabling data sharing patterns essential for many tensor operations. In the transformation pipeline, these dimensions generate `coord_transform_enum::replicate` operations that broadcast data across multiple processing elements.

Replication serves several critical purposes in GPU kernel optimization. It enables efficient data reuse in operations like matrix multiplication where the same input data is needed by multiple output computations. It also facilitates reduction operations where multiple threads collaborate to compute a single result. The replication pattern directly impacts memory access efficiency and register utilization.

For example, in a GEMM operation:
- Matrix A might have `RsLengths = sequence<NWarp>`, indicating replication across warps computing different N-dimension tiles
- Matrix B might have `RsLengths = sequence<MWarp>`, indicating replication across warps computing different M-dimension tiles
- The output matrix C typically has no replication at the outer level, as each element is uniquely owned

### H-Dimensions: Hierarchical Decomposition

The `HsLengthss` parameter represents the hierarchical decomposition of tensor dimensions, encoded as a tuple of sequences. Each sequence defines how a logical tensor dimension (X-dimension) is broken down into finer-grained components that map to the GPU's execution hierarchy.

This hierarchical decomposition is fundamental to achieving high performance on GPUs. It enables:
- **Memory coalescing**: By aligning the decomposition with warp and thread organization
- **Register blocking**: By defining tile sizes that fit in the register file
- **Shared memory utilization**: By creating tiles that exploit data reuse

The decomposition typically follows a pattern like `sequence<RepeatCount, WarpCount, ThreadCount, VectorSize>`, where:
- `RepeatCount`: Number of iterations each thread performs
- `WarpCount`: Number of warps assigned to this dimension
- `ThreadCount`: Number of threads per warp assigned to this dimension  
- `VectorSize`: Number of elements accessed in a single vector operation

In the transformation pipeline, each H-dimension group generates `coord_transform_enum::unmerge` operations that break down abstract dimensions into their hierarchical components:

```cpp
// Example from block GEMM implementation
constexpr auto hs_lengthss = tuple<
    sequence<MIterPerWarp, MWarp>,  // M-dimension decomposition
    sequence<KIterPerWarp>          // K-dimension decomposition
>;
```

### P-Dimensions: Partition Mapping

The `Ps2RHssMajor` and `Ps2RHssMinor` parameters define how partition dimensions map to the underlying RH-dimensions. Partition dimensions represent the fundamental unit of work assignment in the GPU's execution model, typically corresponding to hardware constructs like warps and threads.

The mapping mechanism uses a major/minor indexing scheme where:
- Major index identifies which RH-dimension group (0 for R-dimensions, 1 to NDimX for H-dimensions)
- Minor index identifies the specific component within that group

This mapping enables flexible work distribution strategies. For instance, in a typical block-level distribution:
- `NDimP = 1` might map to `{MWarp, NWarp}` for distributing work across warp groups
- `NDimP = 2` at the combined level typically maps to `{warp_id, lane_id}` for thread-level distribution

The P-dimension mapping directly influences memory access patterns and computational efficiency:

```cpp
// P-dimension retrieval in tile_distribution
CK_TILE_HOST_DEVICE static auto _get_partition_index()
{
    if constexpr(NDimP == 1)
        return array<index_t, 1>{get_lane_id()};
    else if constexpr(NDimP == 2)
        return array<index_t, 2>{get_warp_id(), get_lane_id()};
}
```

### Y-Dimensions: Logical View Mapping

The `Ys2RHsMajor` and `Ys2RHsMinor` parameters define the logical view of the tile data - how users perceive and access the distributed tensor. Y-dimensions represent the iteration space for accessing tile elements, abstracting away the complexity of the underlying distribution.

The Y-to-RH mapping serves multiple purposes:
- **Interface definition**: Provides a clean API for accessing distributed data
- **Access pattern encoding**: Defines the order in which elements are processed
- **Vectorization support**: Enables efficient vector operations by grouping contiguous elements

In practice, Y-dimensions often correspond to the logical dimensions of the operation being performed. For a matrix multiplication tile:
- Y0 might represent the M-dimension iteration space
- Y1 might represent the K-dimension iteration space

The mapping ensures that logical access patterns translate to efficient physical memory operations.

- `[[4, 4], [4, 4]]` ‚Üí 4x4 tiles (16 elements per thread)
- `[[2, 4], [4, 2]]` ‚Üí 2x4 and 4x2 tiles (8 elements per thread)

### P‚ÜíRH Mappings

**üîπ ps_to_rhss_major/minor (P‚ÜíRH Mappings)**

Maps partition coordinates to RH space:
- Controls which H dimensions each P dimension affects
- major/minor specify different levels of the mapping

**üìù Conceptual Example:**
`ps_to_rhss_major=[[1], [2]]` means:
- P dimension 0 maps to H dimension 1
- P dimension 1 maps to H dimension 2

This determines how thread coordinates affect tile placement.

### Y‚ÜíRH Mappings

**üîπ ys_to_rhs_major/minor (Y‚ÜíRH Mappings)**

Maps Y coordinates to RH space:
- Determines how logical Y coordinates map to hierarchical structure
- Controls the internal organization of each thread's tile

**üìù Example Mapping:**
`ys_to_rhs_major=[1, 1, 2, 2]` means:
- Y[0] maps to H1
- Y[1] maps to H1  
- Y[2] maps to H2
- Y[3] maps to H2

This creates the internal structure of thread tiles.

## From Encoding to Tile Distribution

The magic happens when `make_static_tile_distribution()` transforms the mathematical encoding into runtime components:

```{pyodide}
print("üîß From Encoding to Tile Distribution")
print("=" * 50)

print("The transformation process:")
print("1. Analyze encoding parameters")
print("2. Build transformation chains")
print("3. Create runtime components")
print("4. Optimize for performance")

# Create tile distribution from encoding
try:
    tile_distribution = make_static_tile_distribution(encoding)
    
    print("\n‚úÖ Tile distribution created successfully!")
    print("\nüì¶ Internal Components Created:")
    print("  ‚Ä¢ ps_ys_to_xs_adaptor: Maps (P,Y) coordinates to X coordinates")
    print("  ‚Ä¢ ys_to_d_descriptor: Maps Y coordinates to linearized storage")
    print("  ‚Ä¢ encoding: Original mathematical specification")
    
    # Show the components exist
    print(f"\nüîç Component Verification:")
    components = [
        ('ps_ys_to_xs_adaptor', 'P+Y ‚Üí X transformation'),
        ('ys_to_d_descriptor', 'Y ‚Üí D linearization'),
        ('encoding', 'Original specification')
    ]
    
    for component, description in components:
        has_component = hasattr(tile_distribution, component)
        status = "‚úÖ" if has_component else "‚ùå"
        print(f"  {status} {component}: {description}")
        
except Exception as e:
    print(f"‚ö†Ô∏è Creation failed: {e}")
    print("Note: This demonstrates the concept even if creation fails")
```

## The P+Y ‚Üí X Transformation Chain

The heart of the system is the adaptor that implements the P+Y ‚Üí X transformation.

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Input Coordinates"
        P["P-coordinates<br/>[warp_id, lane_id]"]
        Y["Y-coordinates<br/>[y0, y1, y2, y3]"]
    end
    
    subgraph "Transformation Pipeline"
        C1["Combine P+Y"]
        T1["Replicate<br/>Transform<br/>(if R-dims exist)"]
        T2["Unmerge<br/>Transform<br/>(break into H-dims)"]
        T3["Merge<br/>Transform<br/>(combine to X-dims)"]
    end
    
    subgraph "Output"
        X["X-coordinates<br/>[x0, x1]<br/>Tensor position"]
    end
    
    P --> C1
    Y --> C1
    C1 --> T1
    T1 --> T2
    T2 --> T3
    T3 --> X
    
    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

### C++ Transformation Chain Implementation

The transformation chain is built using C++ template metaprogramming to create a compile-time pipeline:

```cpp
// Building the transformation chain in make_static_tile_distribution
template <typename Encoding>
CK_TILE_HOST_DEVICE auto make_ps_ys_to_xs_adaptor(const Encoding& encoding)
{
    // Step 1: Create individual transforms
    constexpr auto replicate_transform = make_replicate_transform(
        encoding.get_rs_lengths());
    
    constexpr auto unmerge_transform = make_unmerge_transform(
        encoding.get_hs_lengthss());
    
    constexpr auto merge_transform = make_merge_transform(
        encoding.get_rhs_to_xs_mapping());
    
    // Step 2: Chain transforms together
    constexpr auto transform_chain = chain_transforms(
        replicate_transform,
        unmerge_transform, 
        merge_transform);
    
    // Step 3: Create adaptor with the chain
    return make_tile_adaptor(
        transform_chain,
        encoding.get_lower_dimension_hidden_idss());
}
```

### How Transforms Work in C++

Each transform is a compile-time object that knows how to convert coordinates:

```cpp
// Example: Replicate transform implementation
template <typename Lengths>
struct replicate_transform
{
    static constexpr index_t num_of_upper_dimension = size(Lengths{});
    static constexpr index_t num_of_lower_dimension = 2 * num_of_upper_dimension;
    
    template <typename UpperIndex>
    CK_TILE_HOST_DEVICE constexpr auto 
    calculate_lower_index(const UpperIndex& idx_upper) const
    {
        // Replicate each coordinate: [a,b] -> [a,b,0,0]
        auto idx_lower = make_zero_multi_index<num_of_lower_dimension>();
        
        static_for<0, num_of_upper_dimension, 1>{}([&](auto i) {
            idx_lower(i) = idx_upper[i];
            idx_lower(i + num_of_upper_dimension) = 0;
        });
        
        return idx_lower;
    }
};
```

**üîó The P+Y ‚Üí X Transformation Chain**

The `ps_ys_to_xs_adaptor` implements a chain of transformations:
1. Start with P coordinates (which thread)
2. Add Y coordinates (which element in thread's tile)
3. Apply replication transforms (R-space)
4. Apply hierarchical transforms (H-space)
5. Merge into final X coordinates

**üí° Why This Chain Works:**
- Each transform handles one aspect of the mapping
- Transforms are composable and efficient
- The chain is built automatically from encoding
- Same pattern works for any distribution strategy

**üìù Conceptual Example:**
- Input: P=[1,0] + Y=[0,1] ‚Üí Combined=[1,0,0,1]
- Transform 1: Handle replication (none in this case)
- Transform 2: Handle hierarchical structure
- Transform 3: Merge to final coordinates
- Output: X=[0,3] (final tensor position)

## The Y to D Linearization

The descriptor handles the linearization of Y coordinates to memory addresses.

### C++ Implementation of Y‚ÜíD Descriptor

The Y‚ÜíD descriptor is responsible for converting multi-dimensional Y coordinates into linear memory offsets:

```cpp
// Y to D descriptor implementation
template <typename YLengths, typename YStrides>
struct ys_to_d_descriptor
{
    static constexpr index_t num_of_dimension = size(YLengths{});
    
    // Calculate linear offset from Y coordinates
    template <typename YIndex>
    CK_TILE_HOST_DEVICE constexpr index_t 
    calculate_offset(const YIndex& idx_y) const
    {
        index_t offset = 0;
        
        static_for<0, num_of_dimension, 1>{}([&](auto i) {
            offset += idx_y[i] * YStrides{}[i];
        });
        
        return offset;
    }
    
    // Get element space size (total elements per thread)
    CK_TILE_HOST_DEVICE static constexpr index_t 
    get_element_space_size()
    {
        return reduce_on_sequence(
            YLengths{}, 
            multiplies{}, 
            number<1>{});
    }
};

// Usage in distributed tensor
template <typename TileDistribution>
struct static_distributed_tensor
{
    using ys_to_d_descriptor = typename TileDistribution::ys_to_d_descriptor;
    
    // Thread-local storage
    static constexpr index_t thread_buffer_size = 
        ys_to_d_descriptor::get_element_space_size();
    
    DataType thread_buffer_[thread_buffer_size];
    
    // Access element at Y coordinate
    template <typename YIndex>
    CK_TILE_HOST_DEVICE DataType& at(const YIndex& idx_y)
    {
        const index_t offset = ys_to_d_descriptor{}.calculate_offset(idx_y);
        return thread_buffer_[offset];
    }
};
```

### Memory Layout Optimization

The descriptor enables efficient register allocation:

```cpp
// Optimized layout for vector operations
template <index_t M, index_t N, index_t VectorSize>
struct make_ys_to_d_descriptor_for_gemm
{
    // Layout: [M/VectorSize][N][VectorSize]
    // This ensures vector loads are contiguous in memory
    using type = tile_descriptor<
        sequence<M/VectorSize, N, VectorSize>,
        sequence<N * VectorSize, VectorSize, 1>>;
};
```

### Y to D Linearization Details

The `ys_to_d_descriptor` handles memory layout within each thread:
1. Start with Y coordinates [y0, y1, y2, y3]
2. Apply thread's local layout (usually row-major)
3. Compute linear offset within thread's buffer
4. Result: D coordinate (memory address)

**üìù Example with [2, 2] tile:**
- Y=[0,0] ‚Üí D=0
- Y=[0,1] ‚Üí D=1
- Y=[1,0] ‚Üí D=2
- Y=[1,1] ‚Üí D=3

**üí° Why Separate from Adaptor:**
- Adaptor handles inter-thread coordination (P+Y ‚Üí X)
- Descriptor handles intra-thread layout (Y ‚Üí D)
- This separation enables different memory layouts
- Each thread can have its own descriptor

## Practical Examples

Different encodings create different behaviors:

**üéØ Example 1: Simple 2x2 Distribution**

```python
simple_encoding = TileDistributionEncoding(
    rs_lengths=[],
    hs_lengthss=[[2], [2]],
    ps_to_rhss_major=[[], []],
    ps_to_rhss_minor=[[], []],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)
```

- No replication
- Simple hierarchical structure
- Direct P‚ÜíH mapping
- Good for basic matrix operations

**üéØ Example 2: With Replication**

```python
replicated_encoding = TileDistributionEncoding(
    rs_lengths=[2],  # 2-way replication
    hs_lengthss=[[2], [2]],
    ps_to_rhss_major=[[], []],
    ps_to_rhss_minor=[[], []],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)
```

- 2-way replication for data sharing
- Same hierarchical structure
- Good for broadcast operations
- Enables thread cooperation

## Testing Your Understanding

Let's verify your understanding of encoding internals:

```{pyodide}
print("üß™ Testing Encoding Internals Understanding")
print("=" * 50)

def test_encoding_creation():
    """Test that we can create valid encodings."""
    try:
        test_encoding = TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

def test_encoding_has_required_fields():
    """Test that encoding has all required fields."""
    encoding = TileDistributionEncoding(
        rs_lengths=[],
        hs_lengthss=[[2], [2]],
        ps_to_rhss_major=[[], []],
        ps_to_rhss_minor=[[], []],
        ys_to_rhs_major=[1, 2],
        ys_to_rhs_minor=[0, 0]
    )
    
    required_fields = [
        'rs_lengths', 'hs_lengthss', 'ps_to_rhss_major', 
        'ps_to_rhss_minor', 'ys_to_rhs_major', 'ys_to_rhs_minor'
    ]
    
    for field in required_fields:
        if not hasattr(encoding, field):
            return False
    return True

def test_different_encodings():
    """Test creating different types of encodings."""
    encodings = [
        # Simple encoding
        TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        ),
        # With replication
        TileDistributionEncoding(
            rs_lengths=[2],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
    ]
    
    return len(encodings) == 2

# Run tests
tests = [
    ("Encoding creation", test_encoding_creation),
    ("Required fields", test_encoding_has_required_fields),
    ("Different encodings", test_different_encodings)
]

print("Running encoding internals tests:")
for test_name, test_func in tests:
    try:
        result = test_func()
        status = "PASS" if result else "FAIL"
        print(f"  [{status}] {test_name}")
    except Exception as e:
        print(f"  [ERROR] {test_name} - {str(e)}")
```

## Key Takeaways

The tile distribution encoding system represents a sophisticated solution to the challenge of mapping high-level tensor operations to GPU hardware. Understanding its internals reveals several key architectural principles:

### Mathematical Foundation

The encoding structure provides a complete mathematical specification of thread organization and data distribution:

1. **Dimensional Hierarchy**: The system defines four types of dimensions (R, H, P, Y) that collectively describe the complete distribution strategy
2. **Compile-Time Resolution**: All relationships and transformations are resolved at compile time, ensuring zero runtime overhead
3. **Composable Transformations**: Individual coordinate transformations (replicate, unmerge, merge) compose to create complex mappings
4. **Hardware Alignment**: The hierarchical decomposition naturally aligns with GPU hardware organization

### Automatic Code Generation

The transformation from mathematical specification to executable code happens through a sophisticated pipeline:

1. **Encoding Analysis**: The `make_adaptor_encoding_for_tile_distribution` function analyzes the encoding parameters
2. **Transform Chain Construction**: Individual transformations are instantiated and connected based on the specification
3. **Optimization Application**: Compile-time optimizations eliminate overhead and generate efficient code
4. **Component Integration**: The generated adaptors and descriptors integrate seamlessly with the broader CK framework

### Component Architecture

The clean separation of concerns enables both flexibility and performance:

- The **ps_ys_to_xs_adaptor** handles the complex mapping from logical to physical coordinates
- The **ys_to_d_descriptor** manages efficient linearization for register allocation
- Each component has a well-defined interface enabling independent optimization
- Components compose naturally to create complete distribution systems

### Performance Implications

Every aspect of the encoding system is designed with GPU performance in mind:

- **Memory Coalescing**: Hierarchical decomposition ensures adjacent threads access adjacent memory
- **Register Efficiency**: Y-to-D linearization optimizes register allocation and minimizes spills
- **Bank Conflict Avoidance**: Careful dimension ordering prevents shared memory bank conflicts
- **Vectorization Support**: The encoding naturally supports vector operations for maximum throughput

### Practical Applications

The encoding system's flexibility enables efficient implementation of diverse operations:

- **Matrix Multiplication**: Hierarchical tiling with appropriate replication patterns
- **Convolution**: Spatial tiling with proper boundary handling
- **Reduction Operations**: Collaborative patterns through R-dimension specification
- **Complex Fusion**: Multiple operations can share distribution patterns for efficiency

The encoding internals demonstrate how the Composable Kernel framework achieves both mathematical elegance and practical performance. By leveraging compile-time computation and sophisticated type system design, the same mathematical framework that provides clarity and composability also generates code that rivals hand-optimized implementations. This synergy between abstraction and performance represents the core strength of the CK approach to GPU kernel development. 