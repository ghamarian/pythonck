---
title: "Individual Transform Operations"
format: 
  live-html:
    mermaid:
      theme: default
engine: jupyter
pyodide:
  packages:
    - micropip
---

The transformation engine is built from individual transform types that each handle specific coordinate conversions. Understanding these building blocks is essential for mastering the tile distribution system.

## ðŸŽ® **Interactive Exploration**

Explore transformation concepts interactively:

**[Tensor Transform Visualizer](https://ck.silobrain.com/tensor-transform/)** - Explore tensor descriptor transformations with visual graphs and mathematical formulas. See how data layouts change through various transformations.

## What Are Transforms?

Transform operations are the fundamental building blocks that convert coordinates between different dimensional spaces. Each transform operates between two coordinate spaces:

- **Lower Dimension Space**: The source coordinate system
- **Upper Dimension Space**: The target coordinate system

### Transform Direction

Transforms work bidirectionally:

- **Forward Transform**: Converts coordinates from the lower dimension to the upper dimension
- **Inverse Transform**: Converts coordinates back from the upper dimension to the lower dimension

### Zero-Copy Logical Operations

**Critical Understanding**: All transform operations happen in **logical coordinate space** only. There is **no data copying or movement** involved - this is a zero-copy system.

- **Data Storage**: The actual tensor data remains stored in memory in linear fashion, exactly as specified by the original tensor shape and strides at creation time
- **Logical Mapping**: Transforms only change how we interpret and access coordinates - they create different logical views of the same underlying data

```{=html}
<div class="mermaid">
graph TB
    subgraph "Tensor Coordinate Transformation"
        US["Lower Dimension Space<br/>Source coordinate system"]
        LS["Upper Dimension Space<br/>Target coordinate system"]
        
        DATA["Linear Data in Memory<br/>Layout determined by tensor<br/>shape & strides"]
    end
    
    US -->|"Forward Transform"| LS
    LS -->|"Inverse Transform"| US
    
    DATA -.->|"Same data,<br/>different views"| US
    DATA -.->|"Same data,<br/>different views"| LS
    
    style US fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style LS fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

### Index Calculation Operations

The transform system provides two fundamental operations for coordinate conversion:

- **`calculate_lower_index()`**: Takes a coordinate from the **upper dimension space** and transforms it to get the corresponding index/coordinate in the **lower dimension space**. This calculates where to find the actual tensor element using the transformed coordinate system.

- **`calculate_upper_index()`**: Takes a coordinate from the **lower dimension space** and transforms it back to get the corresponding coordinate in the **upper dimension space**. This performs the inverse transformation to recover the original coordinate representation.

These operations enable bidirectional navigation between different coordinate representations of the same underlying tensor data.

### Transform System Architecture

```{=html}
<div class="mermaid">
graph TB
    
    subgraph "Transform Types"
        EMB["EmbedTransform<br/>Linear â†’ Multi-D Strided"]
        UNM["MergeTransform<br/>Multi-D â†’ Linear"]
        MRG["UnmergeTransform<br/>Linear â†’ Multi-D"]
        REP["ReplicateTransform<br/>0D â†’ Multi-D Broadcast"]
        OFF["OffsetTransform<br/>Translation"]
        PAS["PassThroughTransform<br/>Identity"]
        PAD["PadTransform<br/>Boundaries"]
    end
    
    subgraph "Operations"
        FWD["Forward<br/>calculate_lower_index()"]
        BWD["Backward<br/>calculate_upper_index()"]
        UPD["Update<br/>update_lower_index()"]
    end
    
    EMB --> FWD
    UNM --> FWD
    MRG --> FWD
    REP --> FWD
    OFF --> FWD
    PAS --> FWD
    PAD --> FWD
    
    style FWD fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")
```

## 1. MergeTransform

MergeTransform collapses multiple dimensions from the lower coordinate space into a single dimension in the upper coordinate space, effectively reducing the dimensionality of the tensor representation while preserving all data relationships.

```{=html}
<div class="mermaid">
graph TB
    subgraph "MergeTransform: Multi-D â†’ Linear"
        LS["Lower Coordinate Space<br/>2D: [4, 5]<br/>Coord: (2, 3)"]
        US["Upper Coordinate Space<br/>1D Linear<br/>Index: 13"]
        
        DATA["Same Tensor Data<br/>Layout: row-major<br/>Size: 20 elements"]
    end
    
    LS -->|"Forward Transform<br/>2Ã—5 + 3 = 13"| US
    US -->|"Inverse Transform<br/>13Ã·5=2, 13%5=3"| LS
    
    DATA -.->|"Multi-dimensional<br/>view"| LS
    DATA -.->|"Linear<br/>view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tensor_descriptor import MergeTransform
from pytensor.tensor_coordinate import MultiIndex

# Create MergeTransform for 4x5 tensor (20 elements total)
transform = MergeTransform(lengths=[4, 5])

# Forward: Lower (2D) â†’ Upper (1D) 
lower_2d = MultiIndex(size=2, values=[2, 3])
upper_linear = transform.calculate_upper_index(lower_2d)
print(f"2D coord {lower_2d.to_list()} â†’ Linear index {upper_linear.to_list()[0]}")
print(f"Calculation: {lower_2d.to_list()[0]}Ã—5 + {lower_2d.to_list()[1]} = {upper_linear.to_list()[0]}")

# Inverse: Upper (1D) â†’ Lower (2D)
upper_input = MultiIndex(size=1, values=[13])
lower_result = transform.calculate_lower_index(upper_input)
print(f"\nLinear index {upper_input.to_list()[0]} â†’ 2D coord {lower_result.to_list()}")
print(f"Calculation: 13 Ã· 5 = {lower_result.to_list()[0]} remainder {lower_result.to_list()[1]}")
```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create merge transform for 4x5 tensor
auto transform = make_merge_transform(
    make_tuple(4, 5)  // lower lengths
);

// The merge transform inherits from base_transform<LowLengths::size(), 1>
// meaning multiple lower dimensions â†’ 1 upper dimension

// Forward: Linear â†’ Multi-D (splitting)
multi_index<1> upper_coord{13};
multi_index<2> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = 13 / 5 = 2 (row)
//         lower_coord[1] = 13 % 5 = 3 (col)

// CK provides two merge implementations:
// 1. merge_v2_magic_division (default) - uses magic number division
// 2. merge_v3_division_mod - for power-of-2 dimensions

// Common usage: dimension reduction
auto desc = transform_tensor_descriptor(
    input_desc,
    make_tuple(make_merge_transform(make_tuple(M, N))),
    make_tuple(sequence<0, 1>{}),  // merge dims 0,1
    make_tuple(sequence<0>{})      // to single dim 0
);
```


## 2. UnmergeTransform

UnmergeTransform expands coordinates from a single dimension in the lower coordinate space into multiple dimensions in the upper coordinate space, effectively increasing the dimensionality of the tensor representation while preserving all data relationships.

```{=html}
<div class="mermaid">
graph TB
    subgraph "UnmergeTransform: Linear â†’ Multi-D"
        LS["Lower Coordinate Space<br/>1D Linear<br/>Index: 14"]
        US["Upper Coordinate Space<br/>3D: [3, 4, 2]<br/>Coord: (1, 3, 0)"]
        
        DATA["Same Tensor Data<br/>Layout: row-major<br/>Size: 24 elements"]
    end
    
    LS -->|"Forward Transform<br/>14 = 1Ã—8 + 3Ã—2 + 0"| US
    US -->|"Inverse Transform<br/>linearize back"| LS
    
    DATA -.->|"Linear<br/>view"| LS
    DATA -.->|"Multi-dimensional<br/>view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tensor_descriptor import UnmergeTransform
from pytensor.tensor_coordinate import MultiIndex

# Create UnmergeTransform for 3x4x2 tensor (24 elements total)
transform = UnmergeTransform(lengths=[3, 4, 2])

# Forward: Lower (1D) â†’ Upper (3D)
lower_linear = MultiIndex(size=1, values=[14])
upper_3d = transform.calculate_upper_index(lower_linear)
print(f"Linear index {lower_linear.to_list()[0]} â†’ 3D coord {upper_3d.to_list()}")
print(f"Calculation: 14 = {upper_3d.to_list()[0]}Ã—8 + {upper_3d.to_list()[1]}Ã—2 + {upper_3d.to_list()[2]}")

# Inverse: Upper (3D) â†’ Lower (1D)
upper_input = MultiIndex(size=3, values=[1, 3, 0])
lower_result = transform.calculate_lower_index(upper_input)
print(f"\n3D coord {upper_input.to_list()} â†’ Linear index {lower_result.to_list()[0]}")
print(f"Calculation: {upper_input.to_list()[0]}Ã—8 + {upper_input.to_list()[1]}Ã—2 + {upper_input.to_list()[2]} = {lower_result.to_list()[0]}")
```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create unmerge transform for 3x4x2 tensor
auto transform = make_unmerge_transform(
    make_tuple(3, 4, 2)  // upper lengths
);

// The unmerge transform inherits from base_transform<1, UpLengths::size()>
// meaning 1 lower dimension â†’ multiple upper dimensions

// Forward: Multi-D â†’ Linear
multi_index<3> upper_coord{1, 3, 0};
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = 1*(4*2) + 3*2 + 0 = 14

// Backward: Linear â†’ Multi-D (unpacking)
multi_index<1> packed_idx{14};
multi_index<3> unpacked_coord;
// This would compute: unpacked_coord = [1, 3, 0]

// Use in tensor descriptor
auto desc = make_naive_tensor_descriptor_packed(
    make_tuple(number<3>{}, number<4>{}, number<2>{})
);
// UnmergeTransform is used internally for packed layouts
```


## 3. EmbedTransform

EmbedTransform expands linear indices from the lower coordinate space into multi-dimensional coordinates in the upper coordinate space using configurable strides, enabling flexible strided tensor layouts and sub-tensor views within larger buffers.

```{=html}
<div class="mermaid">
graph TB
    subgraph "EmbedTransform: Linear â†’ Multi-D Strided"
        LS["Lower Coordinate Space<br/>1D Linear<br/>Index: 14"]
        US["Upper Coordinate Space<br/>2D: [2, 3]<br/>Coord: (1, 2)"]
        
        DATA["Linear Buffer in Memory<br/>Strides: [12, 1]<br/>Custom layout"]
    end
    
    LS -->|"Forward Transform<br/>14 Ã· 12 = 1, 14 % 12 = 2"| US
    US -->|"Inverse Transform<br/>1Ã—12 + 2Ã—1 = 14"| LS
    
    DATA -.->|"Linear<br/>index view"| LS
    DATA -.->|"Multi-dimensional<br/>strided view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tensor_descriptor import EmbedTransform
from pytensor.tensor_coordinate import MultiIndex

# Create EmbedTransform for 2x3 tensor with custom strides  
transform = EmbedTransform(
    lengths=[2, 3],     # Shape: 2 rows, 3 columns
    strides=[12, 1]     # Row stride=12, column stride=1
)

# Forward: Lower (1D) â†’ Upper (2D)
lower_linear = MultiIndex(size=1, values=[14])
upper_coord = transform.calculate_upper_index(lower_linear)
print(f"Linear index {lower_linear.to_list()[0]} â†’ 2D coord {upper_coord.to_list()}")
print(f"Calculation: 14 Ã· 12 = {upper_coord.to_list()[0]}, remainder = {upper_coord.to_list()[1]}")

# Inverse: Upper (2D) â†’ Lower (1D)
upper_input = MultiIndex(size=2, values=[1, 2])
lower_result = transform.calculate_lower_index(upper_input)
print(f"\n2D coord {upper_input.to_list()} â†’ Linear index {lower_result.to_list()[0]}")
print(f"Calculation: {upper_input.to_list()[0]}Ã—12 + {upper_input.to_list()[1]}Ã—1 = {lower_result.to_list()[0]}")
```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create embed transform for 2x3 matrix with strides [12, 1]
auto transform = make_embed_transform(
    make_tuple(2, 3),      // upper lengths
    make_tuple(12, 1)      // strides/coefficients
);

// Forward transformation: 2D â†’ 1D
multi_index<2> upper_coord{1, 2};  // Row 1, Column 2
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = 1*12 + 2*1 = 14

// The embed transform inherits from base_transform<1, UpLengths::size()>
// meaning 1 lower dimension â†’ multiple upper dimensions

// Usage in tensor descriptor
auto desc = make_naive_tensor_descriptor(
    make_tuple(number<3>{}, number<4>{}),   // shape
    make_tuple(number<8>{}, number<1>{})    // strides
);
// Internally uses EmbedTransform for strided access
```

## 4. ReplicateTransform

ReplicateTransform creates a higher-dimensional tensor by replicating (broadcasting) a lower-dimensional tensor. It's essentially a broadcasting operation that takes a tensor with fewer dimensions and logically replicates it across new dimensions without data duplication. An example is taking a scalar (0-dimensional) input and broadcasting it across multiple dimensions, enabling efficient broadcasting patterns where a single value appears at every position in a multi-dimensional coordinate space.

```{=html}
<div class="mermaid">
graph TB
    subgraph "ReplicateTransform: 0D â†’ Multi-D Broadcasting"
        LS["Lower Coordinate Space<br/>0D: Scalar<br/>Empty coordinate []"]
        US["Upper Coordinate Space<br/>2D: [3, 4]<br/>All coords: (i, j)"]
        
        DATA["Single Scalar Value"]
    end
    
    LS -->|"Forward Transform<br/>[] â†’ (i,j) for any i,j"| US
    US -->|"Inverse Transform<br/>(i,j) â†’ [] for any i,j"| LS
    
    DATA -.->|"One scalar<br/>value"| LS
    DATA -.->|"Broadcasted view<br/>at all positions"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tensor_descriptor import ReplicateTransform
from pytensor.tensor_coordinate import MultiIndex

# Create ReplicateTransform for 3x4 broadcasted dimensions
transform = ReplicateTransform(upper_lengths=[3, 4])

print(f"Upper lengths: {transform.upper_lengths}")
print(f"Lower dimensions: {transform.get_num_of_lower_dimension()}")
print(f"Upper dimensions: {transform.get_num_of_upper_dimension()}")

# Forward: Lower (0D) â†’ Upper (2D) - all map to zeros
lower_empty = MultiIndex(size=0, values=[])
upper_coord = transform.calculate_upper_index(lower_empty)
print(f"\nForward: Empty lower â†’ Upper {upper_coord.to_list()}")

# Inverse: Upper (2D) â†’ Lower (0D) - all map to empty
test_coordinates = [(0, 0), (1, 2), (2, 3)]
for coord in test_coordinates:
    upper_input = MultiIndex(size=2, values=list(coord))
    lower_result = transform.calculate_lower_index(upper_input)
    print(f"Upper {upper_input.to_list()} â†’ Lower {lower_result.to_list()} (empty)")

```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create replicate transform for 3x4 broadcasting
auto transform = make_replicate_transform(
    make_tuple(3, 4)  // upper lengths
);

// The replicate transform inherits from base_transform<0, UpLengths::size()>
// meaning 0 lower dimensions â†’ multiple upper dimensions

// Forward: Lower (empty) â†’ Upper (any valid coordinate)
multi_index<0> lower_coord;  // Empty coordinate
multi_index<2> upper_coord{1, 2};
// Any upper coordinate is valid - no actual transformation needed

// Inverse: Upper â†’ Lower (always empty)
multi_index<2> coord1{0, 0};
multi_index<2> coord2{2, 3};
multi_index<0> lower1, lower2;
transform.calculate_lower_index(lower1, coord1);
transform.calculate_lower_index(lower2, coord2);
// Both lower1 and lower2 are empty (no lower dimensions)

// Always returns true for validity checks
bool is_valid = transform.is_valid_upper_index_mapped_to_valid_lower_index(coord1);
// Result: is_valid = true (always valid for replicate)

// Common usage patterns:

// 1. Broadcasting scalar in GEMM
auto broadcast_desc = transform_tensor_descriptor(
    make_naive_tensor_descriptor_packed(make_tuple()),  // 0D scalar
    make_tuple(make_replicate_transform(make_tuple(M, N))),
    make_tuple(sequence<>{}),         // no lower dims
    make_tuple(sequence<0, 1>{})      // create upper dims 0,1
);

// 2. Bias term broadcasting in neural networks
auto bias_broadcast = make_replicate_transform(
    make_tuple(batch_size, hidden_dim)
);
// Maps single bias value to all batch and hidden positions

auto broadcast_desc = transform_tensor_descriptor(
    scalar_desc,  // 0D tensor
    make_tuple(make_replicate_transform(make_tuple(M, N))),
    make_tuple(sequence<>{}),     // no lower dims
    make_tuple(sequence<0, 1>{})  // create 2 upper dims
);
```


## 5. OffsetTransform

OffsetTransform shifts coordinates by a fixed offset, creating a translated view of the coordinate space. It performs simple translation operations where each coordinate in the upper space is mapped to a coordinate in the lower space by adding a constant offset.

```{=html}
<div class="mermaid">
graph TB
    subgraph "OffsetTransform: 1D â†’ 1D Translation"
        LS["Lower Coordinate Space<br/>1D: [0, 63]<br/>Coord: index + offset"]
        US["Upper Coordinate Space<br/>1D: [0, 47]<br/>Coord: index"]
        
        DATA["Linear Buffer in Memory"]
    end
    
    LS -->|"Forward Transform<br/>idx â†’ idx + 16"| US
    US -->|"Inverse Transform<br/>idx + 16 â†’ idx"| LS
    
    DATA -.->|"Larger buffer<br/>view"| LS
    DATA -.->|"Translated<br/>view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import OffsetTransform
from pytensor.tensor_coordinate import MultiIndex

# OffsetTransform adds a constant offset
low_length = 48         # Size of the sub-region
offset_value = 16       # Starting offset in buffer

transform = OffsetTransform(
    low_length=low_length,
    offset=offset_value
)

print(f"Low length: {low_length}")
print(f"Offset: {offset_value}")

# Forward transform: add offset
upper_coord = MultiIndex(size=1, values=[5])
lower_coord = transform.calculate_lower_index(upper_coord)
print(f"\nForward: index {upper_coord.to_list()[0]} â†’ index {lower_coord.to_list()[0]}")
print(f"Calculation: {upper_coord.to_list()[0]} + {offset_value} = {lower_coord.to_list()[0]}")

# Show offset behavior
print("\nOffset transform behavior:")
print(f"  Maps coordinates to buffer starting at offset {offset_value}")
print("  Used for accessing sub-regions of buffers")
print("  Essential for tile-based algorithms")
```

## 6. PassThroughTransform - Identity

No-op transform that passes coordinates unchanged. The PassThrough transform is the simplest coordinate transformation in CK Tile, implementing a perfect identity mapping where input coordinates are passed through unchanged to the output. This transform is essential as a placeholder in transformation chains and for dimensions that require no modification.

```{=html}
<div class="mermaid">
graph TB
    subgraph "PassThroughTransform: 1D â†’ 1D Identity"
        LS["Lower Coordinate Space<br/>1D: [0, 59]<br/>Coord: index"]
        US["Upper Coordinate Space<br/>1D: [0, 59]<br/>Coord: index"]
        
        DATA["Linear Buffer in Memory"]
    end
    
    LS -.->|"Perfect Identity<br/>idx â†’ idx"| US
    US -.->|"Perfect Identity<br/>idx â†’ idx"| LS
    
    DATA -->|"Same buffer<br/>same view"| LS
    DATA -->|"Same buffer<br/>same view"| US
    
    style LS fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px
    style US fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import PassThroughTransform
from pytensor.tensor_coordinate import MultiIndex

# Identity transform - no change
low_length = 60  # Size of dimension

transform = PassThroughTransform(low_length=low_length)

print(f"Low length: {low_length}")

# Forward transform: identity
upper_coord = MultiIndex(size=1, values=[25])
lower_coord = transform.calculate_lower_index(upper_coord)
print(f"\nForward: {upper_coord.to_list()} â†’ {lower_coord.to_list()} (unchanged)")

# Show pass-through behavior
print("\nPassThroughTransform behavior:")
print("  Input index = Output index (identity)")
print("  Used when no transformation needed")
print("  Optimized away at compile time")
```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create pass-through transform
auto transform = make_pass_through_transform(60);  // low_length

// The pass_through transform inherits from base_transform<1, 1>
// meaning 1 lower dimension â†’ 1 upper dimension (identity)

// Forward transformation: no change
multi_index<1> upper_coord{25};
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = upper_coord[0] = 25

// Often optimized away at compile time
// Used as placeholder in transform chains
```

## 7. PadTransform

PadTransform adds padding to tensor dimensions, mapping coordinates from upper dimension space (with padding) to lower dimension space (original data).

```{=html}
<div class="mermaid">
graph TB
    subgraph "PadTransform: 1D â†’ 1D with Padding"
        LS["Lower Coordinate Space<br/>1D: [0, 2] (original data)"]
        US["Upper Coordinate Space<br/>1D: [0, 4] (with padding)"]
        
        DATA["Tensor Data in Memory"]
    end
    
    LS -->|"Forward Transform<br/>idx + left_pad"| US
    US -->|"Inverse Transform<br/>idx - left_pad"| LS
    
    DATA -.->|"Original view"| LS
    DATA -.->|"Padded view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import PadTransform
from pytensor.tensor_coordinate import MultiIndex

# PadTransform handles boundary conditions
low_length = 3        # Original dimension length
left_pad = 1          # Padding on left
right_pad = 1         # Padding on right

transform = PadTransform(
    low_length=low_length,
    left_pad=left_pad,
    right_pad=right_pad
)

print(f"Low length: {low_length}")
print(f"Left pad: {left_pad}")
print(f"Right pad: {right_pad}")
print(f"Upper length: {low_length + left_pad + right_pad} (total with padding)")

# Test various coordinates
test_coords = [0, 1, 2, 3, 4]  # 0 is left pad, 1-3 are valid, 4 is right pad
for idx in test_coords:
    upper = MultiIndex(size=1, values=[idx])
    lower = transform.calculate_lower_index(upper)
    adjusted_idx = lower.to_list()[0]
    is_valid = 0 <= adjusted_idx < low_length
    print(f"Upper {idx} â†’ Lower {adjusted_idx} ({'valid' if is_valid else 'padding'})")

# Show padding behavior
print("\nPadTransform behavior:")
print("  Maps padded coordinates to valid/invalid regions")
print("  Handles out-of-bounds access gracefully")
print("  Essential for convolution and stencil operations")
```

### C++ Implementation

```cpp
// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp

// Create pad transform
auto transform = make_pad_transform(
    3,    // low_length
    1,    // left_pad
    1     // right_pad
);

// The pad transform inherits from base_transform<1, 1>
// meaning 1 lower dimension â†’ 1 upper dimension (with padding)

// Forward: upper (padded) â†’ lower (original)
multi_index<1> upper_coord{2};
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = upper_coord[0] - left_pad = 2 - 1 = 1

// Common usage: convolution boundary handling
auto padded_desc = transform_tensor_descriptor(
    input_desc,
    make_tuple(make_pad_transform(Hi_, InLeftPadH_, InRightPadH_)),
    make_tuple(sequence<0>{}),
    make_tuple(sequence<0>{})
);
```

## 8. XorTransform

XorTransform applies a 2D XOR mapping for specialized memory access patterns. It performs XOR operations on coordinates to create transformed memory layouts for specific algorithmic optimizations.

```{=html}
<div class="mermaid">
graph TB
    subgraph "XorTransform: 2D â†’ 2D XOR Mapping"
        LS["Lower Coordinate Space<br/>2D: [4, 8]<br/>XOR-transformed coords"]
        US["Upper Coordinate Space<br/>2D: [4, 8]<br/>Normal coords"]
        
        DATA["Same Tensor Data"]
    end
    
    LS -->|"Forward Transform<br/>apply XOR reverse"| US
    US -->|"Inverse Transform<br/>apply XOR mapping"| LS
    
    DATA -.->|"XOR pattern<br/>view"| LS
    DATA -.->|"Normal<br/>view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```

```cpp
// From composable_kernel - special 2D coordinate transform
auto transform = make_xor_transform(make_tuple(4, 8));  // 4x8 dimensions

// The xor_t transform inherits from base_transform<2, 2>
// Special mapping: lower[1] = upper[1] ^ (upper[0] % lengths[1])

multi_index<2> upper_coord{2, 5};
multi_index<2> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower[0] = 2, lower[1] = 5 ^ (2 % 8) = 5 ^ 2 = 7

// Used for specialized memory access patterns in some algorithms
```

## 9. SliceTransform

SliceTransform extracts a sub-region from a tensor dimension.

```{=html}
<div class="mermaid">
graph TB
    subgraph "SliceTransform: 1D â†’ 1D Sub-region"
        LS["Lower Coordinate Space<br/>1D: [0, 9] (original range)"]
        US["Upper Coordinate Space<br/>1D: [0, 4] (slice range)"]
        
        DATA["Tensor Data in Memory"]
    end
    
    LS -->|"Forward Transform<br/>idx + slice_begin"| US
    US -->|"Inverse Transform<br/>idx - slice_begin"| LS
    
    DATA -.->|"Full tensor<br/>view"| LS
    DATA -.->|"Sub-region<br/>view"| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```
```cpp
// From composable_kernel - extract a slice from a dimension
auto transform = make_slice_transform(
    10,    // low_length (total dimension)
    2,     // slice_begin
    7      // slice_end
);

// Maps upper range [0, 5) to lower range [2, 7)
multi_index<1> upper_coord{3};
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = 3 + 2 = 5
```

## 10. ModuloTransform

ModuloTransform applies cyclic wrapping to coordinates using modulo operations.

```{=html}
<div class="mermaid">
graph TB
    subgraph "ModuloTransform: 1D â†’ 1D Cyclic"
        LS["Lower Coordinate Space<br/>1D: [0, 3] (modulus range)"]
        US["Upper Coordinate Space<br/>1D: [0, 15] (full range)"]
        
        DATA["Tensor Data in Memory"]
    end
    
    LS -->|"Forward Transform<br/>idx * cycle_count"| US
    US -->|"Inverse Transform<br/>idx % modulus"| LS
    
    DATA -.->|" "| LS
    DATA -.->|" "| US
    
    style LS fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style US fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style DATA fill:#f0f9ff,stroke:#0284c7,stroke-width:2px,stroke-dasharray: 5 5
</div>
```


```cpp
// From composable_kernel - modulo operation for cyclic access
auto transform = make_modulo_transform(
    4,     // modulus
    16     // up_length
);

// Maps coordinates cyclically
multi_index<1> upper_coord{13};
multi_index<1> lower_coord;
transform.calculate_lower_index(lower_coord, upper_coord);
// Result: lower_coord[0] = 13 % 4 = 1
```

## Summary

Individual transforms provide:

- **Modularity**: Each transform does one thing well
- **Composability**: Chain transforms for complex mappings
- **Efficiency**: Compile-time optimization in C++
- **Flexibility**: Handle any coordinate conversion need

Understanding these building blocks enables you to:

1. Create custom tensor views
2. Implement efficient data access patterns
3. Handle padding and boundaries correctly
4. Optimize memory layouts for GPU access

The C++ implementations in Composable Kernel provide:

- Zero-overhead abstractions through templates
- Compile-time composition and optimization
- Support for complex coordinate transformations
- Integration with GPU kernel generation
