---
title: "Tile Distribution Encoding: Mathematical Graph Structure"
format: live-html
---

## üéØ **What is Tile Distribution Encoding?**

**Tile Distribution Encoding** is a mathematical framework that describes how tensor data is distributed across parallel processing elements (threads, warps, blocks). It represents the distribution pattern as a **graph structure** where:

- **NODES**: R, H‚ÇÄ, H‚ÇÅ, ... H‚Çô dimension sequences  
- **EDGES**: P and Y mappings that create connections
- **INCIDENCE MATRIX**: Major/Minor indices define the edge structure

The following video visualises the tile distribution encoding in a graph format. We use this graph format to explain the relevant conepts.
{{< video https://www.youtube.com/watch?v=9ONbP5Ppsi0 >}}

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import make_tile_distribution_encoding

# RMSNorm example - realistic GPU kernel distribution
print("üß™ RMSNorm Tile Distribution Encoding Example")
print("=" * 50)

# Define hierarchical tile structure
variables = {
    "Repeat_M": 4,        # Outer iteration loops
    "WarpPerBlock_M": 2,  # Warps per block
    "ThreadPerWarp_M": 8, # Threads per warp  
    "Vector_M": 4,        # SIMD vector width
    "Repeat_N": 4,
    "WarpPerBlock_N": 2,
    "ThreadPerWarp_N": 8,
    "Vector_N": 4
}

print("üìã Hierarchical Structure:")
for var, val in variables.items():
    print(f"  {var}: {val}")
```

## üï∏Ô∏è **Graph Structure Concept**

The encoding creates a **mathematical graph** that systematically maps processing elements (P) to tensor coordinates (Y). This graph structure enables efficient parallel tensor processing by establishing clear relationships between threads and data elements.

![Graph Structure Overview](../resources/tile_encoding_graph.png)

The graph consists of:

- **NODES**: R sequences (replication), H‚ÇÄ sequences (dimension 0), H‚ÇÅ sequences (dimension 1), ... H‚Çô sequences  
- **EDGES**: P‚ÜíRH mappings and Y‚ÜíRH mappings that define connectivity
- **INCIDENCE MATRIX**: Major/minor indices that specify exactly which nodes connect

![Coordinate Transform Process](../resources/tile_encoding_coordinate_transform.png)

### Major/Minor Indexing System

The graph connectivity uses a systematic indexing where:
- **0** = R space (replication sequences)
- **1** = H‚ÇÄ space (first hierarchical dimension) 
- **2** = H‚ÇÅ space (second hierarchical dimension)
- **n+1** = H‚Çô space (nth hierarchical dimension)

Both P (processing elements) and Y (tensor coordinates) map to this RH space through major/minor index pairs that define precise graph connectivity:

```{pyodide}
#| echo: true
#| output: true

# Graph nodes: R and H dimension sequences
rs_lengths = []  # No R dimension for RMSNorm
hs_lengthss = [
    [variables["Repeat_M"], variables["WarpPerBlock_M"], 
     variables["ThreadPerWarp_M"], variables["Vector_M"]],     # H‚ÇÄ: M dimension
    [variables["Repeat_N"], variables["WarpPerBlock_N"], 
     variables["ThreadPerWarp_N"], variables["Vector_N"]]      # H‚ÇÅ: N dimension  
]

print("üèóÔ∏è Graph Nodes (Dimension Sequences):")
print(f"  R sequence: {rs_lengths} (empty)")
print(f"  H‚ÇÄ sequence (M): {hs_lengthss[0]} ‚Üí total size: {np.prod(hs_lengthss[0])}")
print(f"  H‚ÇÅ sequence (N): {hs_lengthss[1]} ‚Üí total size: {np.prod(hs_lengthss[1])}")
print(f"  Total tensor elements: {np.prod(hs_lengthss[0]) * np.prod(hs_lengthss[1])}")
```

```{pyodide}
#| echo: true
#| output: true

# Graph edges: P and Y mappings using major/minor incidence matrix
print("\nüîó Graph Edges (Incidence Matrix):")
print("The major/minor system creates precise graph connectivity:")
print("  ‚Ä¢ Major index: 0=R, 1=H‚ÇÄ, 2=H‚ÇÅ, 3=H‚ÇÇ, ...")
print("  ‚Ä¢ Minor index: position within that R or H sequence")

# P dimension mappings (thread/warp positioning)
ps_to_rhss_major = [[1, 2], [1, 2]]  # P‚ÇÄ and P‚ÇÅ connect to H sequences  
ps_to_rhss_minor = [[1, 1], [2, 2]]  # Specific H components

print("\nüìç P Dimension Edges (Thread/Warp Positioning):")
print("P coordinates determine which processing element (thread/warp) you are:")
print(f"  ‚Ä¢ P‚ÇÄ major=[1,2] minor=[1,1] ‚Üí connects to H‚ÇÄ[1] and H‚ÇÅ[1]")
print(f"    ‚Üí WarpPerBlock_M ({hs_lengthss[0][1]}) and WarpPerBlock_N ({hs_lengthss[1][1]})")
print(f"  ‚Ä¢ P‚ÇÅ major=[1,2] minor=[2,2] ‚Üí connects to H‚ÇÄ[2] and H‚ÇÅ[2]") 
print(f"    ‚Üí ThreadPerWarp_M ({hs_lengthss[0][2]}) and ThreadPerWarp_N ({hs_lengthss[1][2]})")

# Y dimension mappings (data element indexing)  
ys_to_rhs_major = [1, 1, 2, 2]  # Y dims connect to H sequences
ys_to_rhs_minor = [0, 3, 0, 3]  # Specific H components

print("\nüìä Y Dimension Edges (Data Element Indexing):")
print("Y coordinates determine which data elements each thread processes:")
print(f"  ‚Ä¢ Y‚ÇÄ major=1 minor=0 ‚Üí connects to H‚ÇÄ[0] = Repeat_M ({hs_lengthss[0][0]})")
print(f"  ‚Ä¢ Y‚ÇÅ major=1 minor=3 ‚Üí connects to H‚ÇÄ[3] = Vector_M ({hs_lengthss[0][3]})")
print(f"  ‚Ä¢ Y‚ÇÇ major=2 minor=0 ‚Üí connects to H‚ÇÅ[0] = Repeat_N ({hs_lengthss[1][0]})")
print(f"  ‚Ä¢ Y‚ÇÉ major=2 minor=3 ‚Üí connects to H‚ÇÅ[3] = Vector_N ({hs_lengthss[1][3]})")

print("\nüéØ Complete Graph Construction:")
print("  1. Create R and H node sequences with specific lengths")
print("  2. Define P‚ÜíRH edges using major/minor indices")  
print("  3. Define Y‚ÜíRH edges using major/minor indices")
print("  4. Result: Bipartite graph mapping (P,Y) coordinates to tensor positions")
```

## üîß **Creating the Encoding**

The encoding combines all graph structure information:

```{pyodide}
#| echo: true
#| output: true

# Create the complete encoding
encoding = make_tile_distribution_encoding(
    rs_lengths=rs_lengths,
    hs_lengthss=hs_lengthss,
    ps_to_rhss_major=ps_to_rhss_major,
    ps_to_rhss_minor=ps_to_rhss_minor,
    ys_to_rhs_major=ys_to_rhs_major,
    ys_to_rhs_minor=ys_to_rhs_minor
)

print("‚úÖ Encoding Created Successfully!")
print(f"  Type: {type(encoding).__name__}")
print(f"  Dimensions: X={encoding.ndim_x}, P={encoding.ndim_p}, Y={encoding.ndim_y}, R={encoding.ndim_r}")

# Calculate effective processing elements
total_threads = np.prod([np.prod(hs) for hs in hs_lengthss]) // np.prod(encoding.detail.ys_lengths)
print(f"  Total processing elements: {total_threads}")
print(f"  Elements per processing element: {np.prod(encoding.detail.ys_lengths)}")
```

## üìä **Encoding Detail Information**

The encoding automatically computes detailed mapping information:

```{pyodide}
#| echo: true
#| output: true

print("üîç Detailed Encoding Information:")
print(f"  RH Major dimensions: {encoding.detail.ndim_rh_major}")
print(f"  Span Major dimensions: {encoding.detail.ndim_span_major}")
print(f"  RHS Minor dimensions: {encoding.detail.ndims_rhs_minor}")
print(f"  Y lengths: {encoding.detail.ys_lengths}")

print("\nüìê Distributed Spans per X dimension:")
for x_idx in range(encoding.ndim_x):
    spans = encoding.detail.distributed_spans_lengthss[x_idx]
    active_spans = [s for s in spans if s != -1]
    print(f"  X{x_idx}: {active_spans}")

print("\nüó∫Ô∏è Y to Span Mappings:")
for y_idx in range(encoding.ndim_y):
    span_major = encoding.detail.ys_to_span_major[y_idx]
    span_minor = encoding.detail.ys_to_span_minor[y_idx]
    print(f"  Y{y_idx} ‚Üí Span[{span_major}][{span_minor}]")
```

## üéØ **Understanding the P‚ÜíRH and Y‚ÜíRH Mappings**

The major/minor system creates a precise incidence matrix:

```{pyodide}
#| echo: true
#| output: true

print("üéØ Understanding Major/Minor Mapping System:")
print("\nüìã Major Index (RH Major):")
print("  0 = R sequence (replication)")
print("  1 = H‚ÇÄ sequence (first X dimension)")  
print("  2 = H‚ÇÅ sequence (second X dimension)")
print("  ...")

print("\nüìã Minor Index (RH Minor):")
print("  Index within the specific R or H sequence")

print("\nüîó Complete P Mappings:")
for p_idx in range(encoding.ndim_p):
    majors = ps_to_rhss_major[p_idx]
    minors = ps_to_rhss_minor[p_idx]
    print(f"  P{p_idx}:")
    for major, minor in zip(majors, minors):
        if major == 0:
            seq_name = f"R[{minor}]"
            value = rs_lengths[minor] if minor < len(rs_lengths) else "N/A"
        else:
            seq_name = f"H{major-1}[{minor}]"
            h_idx = major - 1
            value = hs_lengthss[h_idx][minor] if h_idx < len(hs_lengthss) and minor < len(hs_lengthss[h_idx]) else "N/A"
        print(f"    ‚Üí {seq_name} = {value}")

print("\nüîó Complete Y Mappings:")
for y_idx in range(encoding.ndim_y):
    major = ys_to_rhs_major[y_idx]
    minor = ys_to_rhs_minor[y_idx]
    if major == 0:
        seq_name = f"R[{minor}]"
        value = rs_lengths[minor] if minor < len(rs_lengths) else "N/A"
    else:
        seq_name = f"H{major-1}[{minor}]"
        h_idx = major - 1
        value = hs_lengthss[h_idx][minor] if h_idx < len(hs_lengthss) and minor < len(hs_lengthss[h_idx]) else "N/A"
    print(f"  Y{y_idx} ‚Üí {seq_name} = {value}")
```

## üöÄ **Real-World Interpretation**

Understanding what the RMSNorm encoding means in practice:

```{pyodide}
#| echo: true
#| output: true

print("üöÄ RMSNorm Encoding Real-World Interpretation:")
print("\nüßµ Thread Organization:")
print(f"  ‚Ä¢ {variables['WarpPerBlock_M']} √ó {variables['WarpPerBlock_N']} = {variables['WarpPerBlock_M'] * variables['WarpPerBlock_N']} warps per block")
print(f"  ‚Ä¢ {variables['ThreadPerWarp_M']} √ó {variables['ThreadPerWarp_N']} = {variables['ThreadPerWarp_M'] * variables['ThreadPerWarp_N']} threads per warp")
print(f"  ‚Ä¢ Total threads per block: {variables['WarpPerBlock_M'] * variables['WarpPerBlock_N'] * variables['ThreadPerWarp_M'] * variables['ThreadPerWarp_N']}")

print("\nüì¶ Data Processing:")
print(f"  ‚Ä¢ Each thread processes {variables['Vector_M']} √ó {variables['Vector_N']} = {variables['Vector_M'] * variables['Vector_N']} elements via SIMD")
print(f"  ‚Ä¢ Outer loops: {variables['Repeat_M']} √ó {variables['Repeat_N']} = {variables['Repeat_M'] * variables['Repeat_N']} iterations")
print(f"  ‚Ä¢ Elements per thread total: {variables['Vector_M'] * variables['Vector_N'] * variables['Repeat_M'] * variables['Repeat_N']}")

print("\nüéØ Coordinate Mapping:")
print("  ‚Ä¢ P coordinates determine which thread you are")
print("  ‚Ä¢ Y coordinates determine which data element within thread's work")
print("  ‚Ä¢ Graph structure maps (P,Y) ‚Üí final tensor coordinates")

total_elements = np.prod(hs_lengthss[0]) * np.prod(hs_lengthss[1])
total_threads = variables['WarpPerBlock_M'] * variables['WarpPerBlock_N'] * variables['ThreadPerWarp_M'] * variables['ThreadPerWarp_N']
elements_per_thread = total_elements // total_threads

print(f"\nüìä Efficiency Analysis:")
print(f"  ‚Ä¢ Tensor size: {np.prod(hs_lengthss[0])} √ó {np.prod(hs_lengthss[1])} = {total_elements:,} elements")
print(f"  ‚Ä¢ Active threads: {total_threads}")
print(f"  ‚Ä¢ Work per thread: {elements_per_thread} elements")
print(f"  ‚Ä¢ Parallelization efficiency: {100 * total_threads * elements_per_thread / total_elements:.1f}%")
```

## üé® **Spans: Distributed Computation Patterns**

Spans represent how computation is distributed spatially:

```{pyodide}
#| echo: true
#| output: true

print("üé® Understanding Distributed Spans:")

# Get distributed spans from encoding
spans = encoding.get_distributed_spans()
print(f"\nDistributed spans: {spans}")

print("\nüìè Span Interpretation:")
for x_idx, span in enumerate(spans):
    print(f"  X{x_idx} span: {span.partial_lengths}")
    print(f"    ‚Ä¢ Each processing element handles partial tiles of size: {span.partial_lengths}")
    print(f"    ‚Ä¢ Total spatial elements: {np.prod(span.partial_lengths)}")

print("\nüîç Span vs Y Dimension Relationship:")
print("  ‚Ä¢ Spans represent the spatial distribution pattern")
print("  ‚Ä¢ Y dimensions index within each span")
print("  ‚Ä¢ Together they define complete data access pattern")

# Show how Y maps to spans
print(f"\nüó∫Ô∏è Y to Span Mapping Details:")
for y_idx in range(encoding.ndim_y):
    span_major = encoding.detail.ys_to_span_major[y_idx]
    span_minor = encoding.detail.ys_to_span_minor[y_idx]
    if span_major >= 0 and span_major < len(spans):
        span_length = spans[span_major].partial_lengths[span_minor] if span_minor < len(spans[span_major].partial_lengths) else "N/A"
        print(f"  Y{y_idx} ‚Üí X{span_major} span component {span_minor} (length: {span_length})")
    else:
        print(f"  Y{y_idx} ‚Üí Invalid span mapping")
```

## üèóÔ∏è **Factory Functions**

Key functions for creating encoding structures:

```{pyodide}
#| echo: true
#| output: true

print("üèóÔ∏è Tile Distribution Encoding Factory Functions:")

print("\n1Ô∏è‚É£ make_tile_distribution_encoding()")
print("   ‚Ä¢ Creates complete encoding from graph structure")
print("   ‚Ä¢ Validates all dimension mappings")
print("   ‚Ä¢ Computes detailed derived information")

print("\n2Ô∏è‚É£ TileDistributionEncoding.get_distributed_spans()")
print("   ‚Ä¢ Extracts span information from encoding") 
print("   ‚Ä¢ Returns list of TileDistributedSpan objects")

print("\n3Ô∏è‚É£ TileDistributionEncoding.get_uniformed_idx_y_to_h()")
print("   ‚Ä¢ Maps Y indices to flattened H index space")
print("   ‚Ä¢ Useful for coordinate transformations")

# Demonstrate uniformed index mapping
uniformed_indices = encoding.get_uniformed_idx_y_to_h()
print(f"\nüìä Y to Uniformed H Index Mapping:")
for y_idx, uniform_idx in enumerate(uniformed_indices):
    print(f"  Y{y_idx} ‚Üí Uniform H index {uniform_idx}")

print("\n‚ú® The encoding provides the mathematical foundation")
print("   for creating tile distributions and coordinate mappings!")
```

## üéì **Key Takeaways**

1. **Graph Structure**: Tile distribution encoding represents parallel computation as a graph with R/H nodes and P/Y edges
2. **Incidence Matrix**: Major/minor indices create precise mappings between processing elements and data
3. **Hierarchical Organization**: R (replication) ‚Üí H (hierarchical tiles) ‚Üí spans ‚Üí Y (element indexing)
4. **Efficiency**: The encoding ensures optimal work distribution across parallel processing elements
5. **Foundation**: This mathematical structure enables all higher-level tile distribution operations

The tile distribution encoding serves as the **mathematical foundation** for all parallel tensor operations in the Composable Kernels framework! 