---
title: "Tile Window - Data Access Gateway"
format: 
  live-html:
    mermaid:
      theme: default
---

## Overview

The TileWindow abstraction represents the culmination of the CK framework's approach to efficient tensor data access on GPUs. While TileDistribution determines the mapping between threads and tensor coordinates, TileWindow provides the actual mechanism for loading and storing data with optimal memory access patterns. This abstraction encapsulates the complexity of coalesced memory accesses, vectorization, and boundary handling into a clean interface that enables developers to focus on algorithmic logic rather than low-level memory management.

At its core, TileWindow implements a sophisticated windowing mechanism that views a subset of a larger tensor through the lens of a tile distribution. This windowing is not merely a simple sub-tensor extraction but a distribution-aware view that automatically generates the most efficient memory access patterns for the underlying hardware. The system achieves this by combining knowledge of the tensor's layout, the distribution pattern, and the GPU's memory subsystem characteristics to generate optimized load and store operations.

### TileWindow Architecture

```{=html}
<div class="mermaid">
graph TB
    subgraph "Components"
        TV["TensorView<br/>Data source"]
        TD["TileDistribution<br/>Thread mapping"]
        TW["TileWindow<br/>Access gateway"]
        DT["DistributedTensor<br/>Register storage"]
    end
    
    subgraph "Operations"
        Load["Load<br/>Global → Registers"]
        Compute["Compute<br/>In registers"]
        Store["Store<br/>Registers → Global"]
    end
    
    subgraph "Optimizations"
        Coal["Coalescing<br/>Adjacent access"]
        Vec["Vectorization<br/>Multi-element ops"]
        Bank["Bank conflict<br/>avoidance"]
    end
    
    TV --> TW
    TD --> TW
    TW --> DT
    
    TW --> Load
    Load --> Compute
    Compute --> Store
    
    Load --> Coal
    Load --> Vec
    Store --> Bank
    
    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style DT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style Coal fill:#fff3e0,stroke:#f57c00,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import make_static_tile_distribution, make_tile_distribution_encoding
from pytensor.tile_window import make_tile_window, TileWindowWithStaticDistribution
from pytensor.tensor_view import make_tensor_view
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
from pytensor.static_distributed_tensor import make_static_distributed_tensor
```

## What is a TileWindow?

The fundamental challenge in GPU programming lies in the gap between logical tensor operations and the physical realities of memory access. While TileDistribution elegantly solves the problem of work assignment by mapping threads to tensor coordinates, it does not address how threads actually access the data at those coordinates. This is where TileWindow enters the picture, serving as the critical bridge between logical work assignment and physical memory operations.

TileWindow implements a distribution-aware windowing mechanism that transforms abstract coordinate mappings into concrete memory access patterns. The abstraction understands not just which data elements each thread needs, but also how to access them in a way that maximizes memory bandwidth utilization. This involves sophisticated techniques such as memory coalescing, where adjacent threads access adjacent memory locations, and vectorization, where multiple elements are loaded or stored in a single transaction.

The C++ implementation of TileWindow reveals its sophisticated architecture:

```cpp
// From ck_tile/core/tensor/tile_window.hpp
#include <ck_tile/core/tensor/tile_window.hpp>
#include <ck_tile/core/tensor/static_distributed_tensor.hpp>
#include <ck_tile/core/algorithm/coordinate_transform.hpp>

template <typename TensorView_, 
          typename WindowLengths_, 
          typename TileDistribution_>
struct tile_window_with_static_distribution
{
    using TensorView = remove_cvref_t<TensorView_>;
    using Distribution = remove_cvref_t<TileDistribution_>;
    using DataType = typename TensorView::DataType;
    
    // Core components that define the window
    TensorView tensor_view_;      // View into the underlying tensor
    Distribution distribution_;    // How to distribute data across threads
    array<index_t, TensorView::get_num_of_dimension()> origin_;
    
    // Window-specific information
    static constexpr auto window_lengths = WindowLengths{};
    static constexpr index_t num_of_dimension = TensorView::get_num_of_dimension();
    
    // Constructor
    CK_TILE_HOST_DEVICE constexpr tile_window_with_static_distribution(
        const TensorView& tensor_view,
        const WindowLengths& /*window_lengths*/,
        const array<index_t, num_of_dimension>& origin,
        const Distribution& distribution)
        : tensor_view_{tensor_view},
          distribution_{distribution},
          origin_{origin}
    {}
    
    // Load operation with automatic coalescing
    template <typename DistributedTensor>
    CK_TILE_DEVICE void load(DistributedTensor& dst_tensor) const
    {
        // Sophisticated load implementation that:
        // 1. Calculates optimal access pattern
        // 2. Handles vectorization automatically
        // 3. Ensures coalesced memory access
        // 4. Manages boundary conditions
    }
};
```

## TileWindow Data Flow

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Step 1: Create Window"
        T["Tensor<br/>[256, 256]"]
        O["Origin<br/>(64, 64)"]
        W["Window Size<br/>[32, 32]"]
    end
    
    subgraph "Step 2: Apply Distribution"
        TD["TileDistribution<br/>Thread mapping"]
        TW["TileWindow<br/>Created"]
    end
    
    subgraph "Step 3: Load Data"
        GM["Global Memory<br/>Window region"]
        REG["Registers<br/>Distributed tensor"]
    end
    
    T --> TW
    O --> TW
    W --> TW
    TD --> TW
    
    TW --> GM
    GM -->|"load()"| REG
    
    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style REG fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Creating and Using TileWindow

Let's explore how to create and use a TileWindow in practice:

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tile_window import make_tile_window
from pytensor.tensor_view import make_naive_tensor_view_packed
from pytensor.tile_distribution import make_static_tile_distribution, make_tile_distribution_encoding

# Create a tensor
data = np.arange(64, dtype=np.float32)
print("Original tensor (8x8):")
print(data.reshape(8, 8))

# Create a tensor view
tensor_view = make_naive_tensor_view_packed(data, lengths=[8, 8])

# Create a tile distribution encoding for a 4x4 window
# Each thread will handle a 1x1 tile within the window
encoding = make_tile_distribution_encoding(
    rs_lengths=[],  # No replication
    hs_lengthss=[[4], [4]],  # 4x4 hierarchical structure
    ps_to_rhss_major=[[1], [2]],  # P dimensions map to H dimensions
    ps_to_rhss_minor=[[0], [0]],
    ys_to_rhs_major=[1, 2],  # Y dimensions map to H dimensions
    ys_to_rhs_minor=[0, 0]
)
tile_dist = make_static_tile_distribution(encoding)

# Create a tile window at position (2, 2)
window = make_tile_window(
    tensor_view=tensor_view,
    window_lengths=[4, 4],
    origin=[2, 2],
    tile_distribution=tile_dist
)

print(f"\nTileWindow created:")
print(f"- Window size: {window.window_lengths}")
print(f"- Window origin: {window.window_origin}")
print(f"- Covers elements from (2,2) to (5,5)")
```

## Understanding Window Coordinates

```{pyodide}
#| echo: true
#| output: true

# Visualize what data the window covers
window_data = data[2:6, 2:6]  # The 4x4 window starting at (2,2)
print("Data covered by the window:")
print(window_data)

# Show how threads map to this data
print("\nThread-to-element mapping:")
print("Thread (i,j) -> Element value")
for i in range(4):
    for j in range(4):
        thread_id = i * 4 + j
        element = window_data[i, j]
        print(f"Thread ({i},{j}) -> {element}")
```

## Load Operation Architecture

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Load Analysis"
        Analyze["Analyze access pattern<br/>Detect coalescing opportunities"]
    end
    
    subgraph "Vectorization"
        V1["Scalar: 4 loads"]
        V2["Vector2: 2 loads"]
        V4["Vector4: 1 load"]
    end
    
    subgraph "Memory Transaction"
        Coal["Coalesced access<br/>32 threads → 1 transaction"]
        NonCoal["Non-coalesced<br/>32 threads → 32 transactions"]
    end
    
    subgraph "Result"
        Reg["Thread registers<br/>Local data"]
    end
    
    Analyze --> V1
    Analyze --> V2
    Analyze --> V4
    
    V4 --> Coal
    V1 --> NonCoal
    
    Coal --> Reg
    NonCoal --> Reg
    
    style V4 fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style Coal fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style NonCoal fill:#fee2e2,stroke:#ef4444,stroke-width:2px
</div>
```

## Memory Access Patterns

One of TileWindow's key features is generating optimal memory access patterns:

```{pyodide}
#| echo: true
#| output: true

def analyze_window_access_pattern(window, num_threads=16):
    """Analyze how threads access memory through the window"""
    print("Memory Access Pattern Analysis")
    print("=" * 50)
    
    # Simulate first wave of accesses
    print("\nFirst access wave (threads 0-7):")
    print("Thread | Window Coord | Global Coord | Linear Addr")
    print("-" * 50)
    
    origin = window.window_origin
    
    for tid in range(min(8, num_threads)):
        # Thread position in window
        local_i = tid // 4
        local_j = tid % 4
        
        # Global position
        global_i = origin[0] + local_i
        global_j = origin[1] + local_j
        
        # Linear address
        linear_addr = global_i * 8 + global_j  # 8 is tensor width
        
        print(f"  {tid:2d}   | ({local_i},{local_j})        | ({global_i},{global_j})        | {linear_addr:3d}")
    
    # Check coalescing
    print("\nCoalescing Analysis:")
    print("✓ Threads 0-3 access addresses 18-21 (consecutive)")
    print("✓ Threads 4-7 access addresses 26-29 (consecutive)")
    print("✓ Perfect coalescing within each warp!")

analyze_window_access_pattern(window)
```

## Advanced Window Operations

### Sliding Windows

```{pyodide}
#| echo: true
#| output: true

# Demonstrate sliding window pattern
def sliding_window_demo(tensor_shape, window_size, stride):
    """Show how to slide a window across a tensor"""
    print(f"Sliding {window_size}x{window_size} window with stride {stride}")
    print("Window positions:")
    
    positions = []
    for i in range(0, tensor_shape[0] - window_size + 1, stride):
        for j in range(0, tensor_shape[1] - window_size + 1, stride):
            positions.append((i, j))
            print(f"  Position ({i},{j}) covers ({i},{j}) to ({i+window_size-1},{j+window_size-1})")
    
    return positions

# Create sliding windows
positions = sliding_window_demo(tensor_shape=[8, 8], window_size=4, stride=2)
print(f"\nTotal windows: {len(positions)}")
```

### Overlapping Windows

```{pyodide}
#| echo: true
#| output: true

# Demonstrate overlapping windows for convolution-like operations
def create_overlapping_windows(tensor_shape, window_size, overlap):
    """Create overlapping windows for operations like convolution"""
    stride = window_size - overlap
    
    print(f"Creating {window_size}x{window_size} windows with {overlap} overlap")
    print(f"Effective stride: {stride}")
    
    windows = []
    for i in range(0, tensor_shape[0] - window_size + 1, stride):
        for j in range(0, tensor_shape[1] - window_size + 1, stride):
            windows.append({
                'origin': (i, j),
                'overlap_left': j > 0,
                'overlap_top': i > 0,
                'overlap_right': j + window_size < tensor_shape[1],
                'overlap_bottom': i + window_size < tensor_shape[0]
            })
    
    # Show overlap pattern
    print(f"\nWindow overlap pattern ({len(windows)} windows):")
    for idx, w in enumerate(windows[:4]):  # Show first few
        overlaps = []
        if w['overlap_left']: overlaps.append('left')
        if w['overlap_top']: overlaps.append('top')
        if w['overlap_right']: overlaps.append('right')
        if w['overlap_bottom']: overlaps.append('bottom')
        
        print(f"  Window {idx} at {w['origin']}: overlaps {', '.join(overlaps) if overlaps else 'none'}")
    
    return windows

# Create overlapping windows
windows = create_overlapping_windows([8, 8], window_size=3, overlap=1)
```

## Computation Architecture

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 30%;">
graph TB
    subgraph "Load Phase"
        LW["Load Window<br/>Global → Registers"]
    end
    
    subgraph "Compute Phase"
        C1["Thread-local compute<br/>In registers"]
        C2["Warp-level shuffle<br/>Data exchange"]
        C3["Block-level reduction<br/>Shared memory"]
    end
    
    subgraph "Store Phase"
        SW["Store Window<br/>Registers → Global"]
    end
    
    LW --> C1
    C1 --> C2
    C2 --> C3
    C3 --> SW
    
    style LW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style SW fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## C++ Usage Example

The real power of TileWindow comes from its C++ implementation:

```cpp
// Complete example: Matrix multiplication using TileWindow
template<typename AType, typename BType, typename CType>
__global__ void gemm_kernel_with_windows(
    const AType* __restrict__ a_ptr,
    const BType* __restrict__ b_ptr,
    CType* __restrict__ c_ptr,
    index_t M, index_t N, index_t K)
{
    // Create tensor views
    auto a_tensor = make_naive_tensor_view(
        a_ptr, make_tuple(M, K), make_tuple(K, 1));
    auto b_tensor = make_naive_tensor_view(
        b_ptr, make_tuple(K, N), make_tuple(N, 1));
    auto c_tensor = make_naive_tensor_view(
        c_ptr, make_tuple(M, N), make_tuple(N, 1));
    
    // Define tile sizes
    constexpr index_t tile_m = 32;
    constexpr index_t tile_n = 32;
    constexpr index_t tile_k = 8;
    
    // Create distributions
    auto a_dist = make_static_tile_distribution<...>();
    auto b_dist = make_static_tile_distribution<...>();
    auto c_dist = make_static_tile_distribution<...>();
    
    // Calculate tile position
    const index_t block_m = blockIdx.y * tile_m;
    const index_t block_n = blockIdx.x * tile_n;
    
    // Create tile windows
    auto a_window = make_tile_window(
        a_tensor,
        make_tuple(tile_m, tile_k),
        make_tuple(block_m, 0),
        a_dist);
    
    auto b_window = make_tile_window(
        b_tensor,
        make_tuple(tile_k, tile_n),
        make_tuple(0, block_n),
        b_dist);
    
    auto c_window = make_tile_window(
        c_tensor,
        make_tuple(tile_m, tile_n),
        make_tuple(block_m, block_n),
        c_dist);
    
    // Create distributed tensors for register storage
    auto a_reg = make_static_distributed_tensor<AType>(a_dist);
    auto b_reg = make_static_distributed_tensor<BType>(b_dist);
    auto c_reg = make_static_distributed_tensor<CType>(c_dist);
    
    // Initialize accumulator
    c_reg.clear();
    
    // Main GEMM loop
    for(index_t k = 0; k < K; k += tile_k) {
        // Update window positions
        a_window.set_window_origin(make_tuple(0, k));
        b_window.set_window_origin(make_tuple(k, 0));
        
        // Load tiles
        a_window.load(a_reg);
        b_window.load(b_reg);
        
        // Compute
        gemm(a_reg, b_reg, c_reg);
    }
    
    // Store result
    c_window.store(c_reg);
}
```

## Load-Compute-Store Pipeline

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Iteration i"
        L1["Load A[i]<br/>Load B[i]"]
        C1["Compute<br/>C += A[i] × B[i]"]
    end
    
    subgraph "Iteration i+1"
        L2["Load A[i+1]<br/>Load B[i+1]"]
        C2["Compute<br/>C += A[i+1] × B[i+1]"]
    end
    
    subgraph "Final"
        S["Store C"]
    end
    
    L1 --> C1
    C1 --> L2
    L2 --> C2
    C2 --> S
    
    style L1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style S fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Pipeline Optimization

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Basic Pipeline"
        B1["Load"] --> B2["Compute"] --> B3["Store"]
        BTime["Time: 3 units"]
    end
    
    subgraph "Software Pipelining"
        P1["Load[i+1]"]
        P2["Compute[i]"]
        P3["Store[i-1]"]
        PTime["Time: 1 unit per iteration"]
    end
    
    subgraph "Double Buffering"
        DB1["Buffer A: Load"]
        DB2["Buffer B: Compute"]
        DBSwap["Swap buffers"]
    end
    
    B3 --> P1
    P1 --> P2
    P2 --> P3
    
    P3 --> DB1
    DB1 --> DB2
    DB2 --> DBSwap
    
    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style P2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style P3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Window Configuration Space

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Window Parameters"
        Size["Window Size<br/>Trade-off: registers vs reuse"]
        Origin["Window Origin<br/>Starting position"]
        Stride["Window Stride<br/>Overlap pattern"]
    end
    
    subgraph "Performance Factors"
        Reg["Register Usage<br/>Limited resource"]
        Reuse["Data Reuse<br/>Bandwidth savings"]
        Coal["Coalescing<br/>Memory efficiency"]
    end
    
    subgraph "Optimal Configuration"
        Opt["Balance all factors<br/>Architecture-specific"]
    end
    
    Size --> Reg
    Size --> Reuse
    Origin --> Coal
    Stride --> Reuse
    
    Reg --> Opt
    Reuse --> Opt
    Coal --> Opt
    
    style Opt fill:#d1fae5,stroke:#10b981,stroke-width:3px
</div>
```

## Best Practices

### 1. Window Size Selection

```{pyodide}
#| echo: true
#| output: true

def recommend_window_size(tensor_shape, available_registers, elements_per_thread):
    """Recommend optimal window size based on constraints"""
    print("Window Size Recommendation")
    print("=" * 50)
    print(f"Tensor shape: {tensor_shape}")
    print(f"Available registers per thread: {available_registers}")
    print(f"Elements per thread: {elements_per_thread}")
    
    # Calculate maximum window size based on register constraints
    max_window_elements = available_registers // elements_per_thread
    max_window_dim = int(max_window_elements ** 0.5)
    
    # Consider common tile sizes
    common_sizes = [8, 16, 32, 64, 128]
    recommended = []
    
    for size in common_sizes:
        if size <= max_window_dim and all(dim % size == 0 for dim in tensor_shape):
            recommended.append(size)
    
    print(f"\nRecommended window sizes: {recommended}")
    if recommended:
        optimal = recommended[-1]  # Largest that fits
        print(f"Optimal choice: {optimal}x{optimal}")
        print(f"  - Uses {optimal*optimal*elements_per_thread} registers per thread")
        print(f"  - Achieves {(optimal*optimal)/max_window_elements*100:.1f}% register utilization")
    
    return recommended

# Example recommendation
recommend_window_size(
    tensor_shape=[256, 256],
    available_registers=256,
    elements_per_thread=4
)
```

### 2. Access Pattern Optimization

```{pyodide}
#| echo: true
#| output: true

def optimize_access_pattern(window_shape, thread_block_shape):
    """Determine optimal access pattern for coalescing"""
    print("Access Pattern Optimization")
    print("=" * 50)
    print(f"Window shape: {window_shape}")
    print(f"Thread block: {thread_block_shape}")
    
    # Calculate threads per row for coalescing
    threads_per_row = thread_block_shape[1]
    elements_per_row = window_shape[1]
    
    if elements_per_row % threads_per_row == 0:
        print("\n✓ Perfect coalescing possible!")
        print(f"  Each thread loads {elements_per_row // threads_per_row} consecutive elements")
        access_pattern = "row-major coalesced"
    else:
        print("\n⚠ Partial coalescing")
        print("  Consider adjusting window or thread block dimensions")
        access_pattern = "row-major partial"
    
    # Suggest vectorization
    vector_sizes = [4, 2, 1]
    for vec_size in vector_sizes:
        if elements_per_row % (threads_per_row * vec_size) == 0:
            print(f"\nVectorization: Use float{vec_size} for optimal loads")
            print(f"  {elements_per_row // (threads_per_row * vec_size)} vector loads per thread")
            break
    
    return access_pattern

# Optimize for common configurations
optimize_access_pattern(
    window_shape=[32, 32],
    thread_block_shape=[8, 32]
)
```

## Summary

TileWindow provides:

- **Automatic optimization**: Generates optimal memory access patterns
- **Distribution awareness**: Works seamlessly with TileDistribution
- **Flexible windowing**: Supports various access patterns and window configurations
- **Zero overhead**: Compile-time optimization in C++
- **Safety**: Automatic boundary handling

Key benefits:

1. **Performance**: Achieves peak memory bandwidth through coalescing and vectorization
2. **Productivity**: Eliminates manual memory management code
3. **Correctness**: Automatic boundary checking and handling
4. **Composability**: Integrates cleanly with other CK abstractions

The TileWindow abstraction is essential for building high-performance GPU kernels, providing a clean interface for complex memory access patterns while maintaining peak performance.