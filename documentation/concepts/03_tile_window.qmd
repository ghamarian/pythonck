---
title: "Tile Window - Data Access Gateway"
format: 
  live-html:
    mermaid:
      theme: default
---

## Overview

The TileWindow abstraction represents the culmination of the CK framework's approach to efficient tensor data access on GPUs. While TileDistribution determines the mapping between threads and tensor coordinates, TileWindow provides the actual mechanism for loading and storing data with optimal memory access patterns. This abstraction encapsulates the complexity of coalesced memory accesses, vectorization, and boundary handling into a clean interface that enables developers to focus on algorithmic logic rather than low-level memory management.

At its core, TileWindow implements a sophisticated windowing mechanism that views a subset of a larger tensor through the lens of a tile distribution. This windowing is not merely a simple sub-tensor extraction but a distribution-aware view that automatically generates the most efficient memory access patterns for the underlying hardware. The system achieves this by combining knowledge of the tensor's layout, the distribution pattern, and the GPU's memory subsystem characteristics to generate optimized load and store operations.

### TileWindow Architecture

```{=html}
<div class="mermaid">
graph TB
    subgraph "Components"
        TV["TensorView<br/>Data source"]
        TD["TileDistribution<br/>Thread mapping"]
        TW["TileWindow<br/>Access gateway"]
        LT["LoadStoreTraits<br/>Access optimizer"]
        DT["DistributedTensor<br/>Register storage"]
    end
    
    subgraph "Operations"
        Load["Load<br/>Global â†’ Registers"]
        Compute["Compute<br/>In registers"]
        Store["Store<br/>Registers â†’ Global"]
    end
    
    subgraph "Optimizations"
        Coal["Coalescing<br/>Adjacent access"]
        Vec["Vectorization<br/>Multi-element ops"]
        Bank["Bank conflict<br/>avoidance"]
        SFC["Space-filling<br/>curve traversal"]
    end
    
    TV --> TW
    TD --> TW
    TW --> LT
    LT --> DT
    
    TW --> Load
    Load --> Compute
    Compute --> Store
    
    Load --> Coal
    Load --> Vec
    Load --> SFC
    Store --> Bank
    
    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style LT fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style DT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import make_static_tile_distribution, make_tile_distribution_encoding
from pytensor.tile_window import make_tile_window, TileWindowWithStaticDistribution, LoadStoreTraits
from pytensor.tensor_view import make_tensor_view
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
from pytensor.static_distributed_tensor import make_static_distributed_tensor
from pytensor.space_filling_curve import SpaceFillingCurve
```

## What is a TileWindow?

The fundamental challenge in GPU programming lies in the gap between logical tensor operations and the physical realities of memory access. While TileDistribution elegantly solves the problem of work assignment by mapping threads to tensor coordinates, it does not address how threads actually access the data at those coordinates. This is where TileWindow enters the picture, serving as the critical bridge between logical work assignment and physical memory operations.

TileWindow implements a distribution-aware windowing mechanism that transforms abstract coordinate mappings into concrete memory access patterns. The abstraction understands not just which data elements each thread needs, but also how to access them in a way that maximizes memory bandwidth utilization. This involves sophisticated techniques such as memory coalescing, where adjacent threads access adjacent memory locations, and vectorization, where multiple elements are loaded or stored in a single transaction.

The C++ implementation of TileWindow reveals its sophisticated architecture:

<details>
<summary>Click to show C++ code</summary>

```cpp
// From ck_tile/core/tensor/tile_window.hpp
#include <ck_tile/core/tensor/tile_window.hpp>
#include <ck_tile/core/tensor/static_distributed_tensor.hpp>
#include <ck_tile/core/algorithm/coordinate_transform.hpp>

template <typename TensorView_, 
          typename WindowLengths_, 
          typename TileDistribution_>
struct tile_window_with_static_distribution
{
    using TensorView = remove_cvref_t<TensorView_>;
    using Distribution = remove_cvref_t<TileDistribution_>;
    using DataType = typename TensorView::DataType;
    
    // Core components that define the window
    TensorView tensor_view_;      // View into the underlying tensor
    Distribution distribution_;    // How to distribute data across threads
    array<index_t, TensorView::get_num_of_dimension()> origin_;
    
    // Window-specific information
    static constexpr auto window_lengths = WindowLengths{};
    static constexpr index_t num_of_dimension = TensorView::get_num_of_dimension();
    
    // Constructor
    CK_TILE_HOST_DEVICE constexpr tile_window_with_static_distribution(
        const TensorView& tensor_view,
        const WindowLengths& /*window_lengths*/,
        const array<index_t, num_of_dimension>& origin,
        const Distribution& distribution)
        : tensor_view_{tensor_view},
          distribution_{distribution},
          origin_{origin}
    {}
    
    // Load operation with automatic coalescing
    template <typename DistributedTensor>
    CK_TILE_DEVICE void load(DistributedTensor& dst_tensor) const
    {
        // Sophisticated load implementation that:
        // 1. Calculates optimal access pattern
        // 2. Handles vectorization automatically
        // 3. Ensures coalesced memory access
        // 4. Manages boundary conditions
    }
};
```

</details>

## LoadStoreTraits - The Access Pattern Engine

Behind every efficient TileWindow operation lies LoadStoreTraits, a sophisticated analysis engine that determines the optimal way to access memory. This component bridges the gap between the logical distribution pattern and the physical memory subsystem, analyzing the distribution to find opportunities for vectorization and coalescing.

LoadStoreTraits performs several critical analyses:
- **Vector dimension identification**: Finds which Y dimension has stride 1 for optimal vectorization
- **Access pattern calculation**: Determines the number and order of memory operations
- **Space-filling curve construction**: Creates an optimal traversal order for cache efficiency

Let's explore how LoadStoreTraits analyzes a distribution:

```{pyodide}
#| echo: true
#| output: true

# Create a distribution to analyze
encoding = make_tile_distribution_encoding(
    rs_lengths=[],
    hs_lengthss=[[4, 2], [4, 2]],  # 8x8 total
    ps_to_rhss_major=[[1], [2]],
    ps_to_rhss_minor=[[0], [0]],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[1, 1]  # Each thread handles 2x2
)
distribution = make_static_tile_distribution(encoding)

# Create LoadStoreTraits to analyze access patterns
traits = LoadStoreTraits(distribution, np.float32)

print("LoadStoreTraits Analysis:")
print(f"  Y dimensions: {traits.ndim_y}")
print(f"  Vector dimension Y: {traits.vector_dim_y}")
print(f"  Scalars per vector: {traits.scalar_per_vector}")
print(f"  Scalars per access: {traits.scalars_per_access}")
print(f"  Total accesses needed: {traits.num_access}")

# Explain the significance
print("\nðŸ“Š What this means:")
print(f"- Each thread manages a {distribution.get_y_vector_lengths()} tile")
print(f"- Dimension Y{traits.vector_dim_y} has stride 1 (best for vectorization)")
print(f"- Can load/store {traits.scalar_per_vector} elements in one instruction")
print(f"- Needs {traits.num_access} memory operations total")

# Show what get_index() returns for different access indices
print("\nAccess Pattern (get_index() results):")
print("Access # â†’ Y indices â†’ What it accesses")
print("-" * 40)
for i in range(min(4, traits.num_access)):
    y_indices = traits.get_y_indices(i)
    print(f"  {i} â†’ {y_indices} â†’ Element at Y{y_indices}")
```

## Space-Filling Curves for Memory Access

One of the most sophisticated aspects of TileWindow is its use of space-filling curves to determine the order in which memory is accessed. This isn't just about iterating through elements - it's about doing so in a way that maximizes cache utilization and minimizes memory latency.

A space-filling curve is a continuous curve that visits every point in a multi-dimensional space exactly once. In the context of TileWindow, it determines the order in which a thread accesses its assigned elements. The "snake" pattern is particularly effective because it minimizes the distance between consecutive accesses, keeping data in cache longer.

```{pyodide}
#| echo: true
#| output: true

# Let's examine the space-filling curve in detail
y_lengths = distribution.ys_to_d_descriptor.get_lengths()
print(f"Y space dimensions: {y_lengths} (total {np.prod(y_lengths)} elements)")

# Create and analyze the space-filling curve
sfc = traits.sfc_ys
print(f"\nSpace-Filling Curve Properties:")
print(f"  Tensor lengths: {sfc.tensor_lengths}")
print(f"  Dimension access order: {sfc.dim_access_order}")
print(f"  Scalars per access: {sfc.scalars_per_access}")
print(f"  Snake curved: {sfc.snake_curved}")
print(f"  Number of accesses: {sfc.get_num_of_access()}")

# Trace the access pattern
print("\nAccess Pattern Visualization:")
print("(showing order of Y-space traversal)")
access_grid = np.full(y_lengths, -1)
for i in range(sfc.get_num_of_access()):
    indices = sfc.get_index(i)
    if len(indices) == 2:
        access_grid[indices[0], indices[1]] = i
print(access_grid)

print("\nðŸ’¡ Why Snake Patterns Matter:")
print("- Consecutive accesses are spatially close")
print("- Better cache line utilization")
print("- Reduced memory controller pressure")
print("- Improved prefetcher effectiveness")
```

## Understanding get_vectorized_access_info()

The `get_vectorized_access_info()` method is crucial for understanding how TileWindow achieves its impressive performance. This method returns detailed information about how to perform vectorized memory operations, including which elements to access together and along which dimension.

This information is used by the GPU to:
- Generate vector load/store instructions (e.g., ld.global.v4.f32)
- Ensure memory coalescing across threads
- Minimize the total number of memory transactions

```{pyodide}
#| echo: true
#| output: true

# Detailed analysis of vectorized access information
print("Vectorized Access Information:")
print("=" * 40)

for i_access in range(min(3, traits.num_access)):
    access_info = traits.get_vectorized_access_info(i_access)
    
    print(f"\nAccess #{i_access}:")
    print(f"  Base Y indices: {access_info['base_indices']}")
    print(f"  Vector dimension: {access_info['vector_dim']}")
    print(f"  Vector size: {access_info['vector_size']}")
    print(f"  Elements accessed:")
    for j, idx in enumerate(access_info['vector_indices']):
        print(f"    [{j}] â†’ Y{idx}")
    
    # Explain what happens at the hardware level
    if i_access == 0:
        print("\n  ðŸ”§ Hardware execution:")
        print(f"  - Issues a vector{access_info['vector_size']} load instruction")
        print(f"  - Fetches {access_info['vector_size']} consecutive elements")
        print(f"  - All in a single memory transaction!")

# Demonstrate how this maps to memory
print("\nðŸ”„ Memory Access Pattern:")
print("One vectorized load fetches multiple consecutive elements")
print("along the dimension with stride 1 for perfect coalescing!")
```

## TileWindow Data Flow

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Step 1: Create Window"
        T["Tensor<br/>[256, 256]"]
        O["Origin<br/>(64, 64)"]
        W["Window Size<br/>[32, 32]"]
    end
    
    subgraph "Step 2: Apply Distribution"
        TD["TileDistribution<br/>Thread mapping"]
        TW["TileWindow<br/>Created"]
    end
    
    subgraph "Step 3: Load Data"
        GM["Global Memory<br/>Window region"]
        REG["Registers<br/>Distributed tensor"]
    end
    
    T --> TW
    O --> TW
    W --> TW
    TD --> TW
    
    TW --> GM
    GM -->|"load()"| REG
    
    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style REG fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## TileWindow Internals: Precomputation

One of the key optimizations in TileWindow is the precomputation of coordinate transformations. Rather than calculating complex coordinate mappings during each load/store operation, TileWindow precomputes these transformations during construction. This eliminates redundant calculations and enables the compiler to optimize the access patterns aggressively.

The precomputation phase:
- Analyzes the distribution pattern to identify access sequences
- Calculates all coordinate transformations upfront
- Stores them in a compact format for efficient runtime access
- Enables incremental coordinate updates for sliding windows

```{pyodide}
#| echo: true
#| output: true

# Create a tensor and window to see precomputation in action
data = np.arange(64, dtype=np.float32).reshape(8, 8)
tensor_view = make_tensor_view(data, make_naive_tensor_descriptor_packed([8, 8]))

# Create window at origin (2, 2) with size 4x4
window = make_tile_window(
    tensor_view=tensor_view,
    window_lengths=[4, 4],
    origin=[2, 2],
    tile_distribution=distribution
)

print("TileWindow Configuration:")
print(f"  Window origin: {window.window_origin}")
print(f"  Window size: {window.window_lengths}")
print(f"  Number of coordinate bundles: {window.num_coord}")
print(f"  Precomputed coordinates: {len(window.pre_computed_coords)}")

# Show what the window covers
print(f"\nWindow covers global positions:")
print(f"  Row range: [{window.window_origin[0]}, {window.window_origin[0] + window.window_lengths[0]})")
print(f"  Col range: [{window.window_origin[1]}, {window.window_origin[1] + window.window_lengths[1]})")

print("\nðŸ’¡ Precomputation Benefits:")
print("- Zero overhead during load/store operations")
print("- Enables aggressive compiler optimizations")
print("- Supports efficient window movement")
print("- Reduces register pressure")

# Visualize what data the window covers
window_data = data[2:6, 2:6]  # The 4x4 window starting at (2,2)
print("\nData covered by the window:")
print(window_data)
```

## Creating and Using TileWindow

Let's explore how to create and use a TileWindow in practice:

```{pyodide}
#| echo: true
#| output: true

# Show how threads map to this data
print("Thread-to-element mapping:")
print("Thread (P coords) â†’ Window position â†’ Global position â†’ Value")
print("-" * 60)

# This simulates what happens on the GPU
p_lengths = [2, 2]  # 2x2 threads from our distribution
for p0 in range(p_lengths[0]):
    for p1 in range(p_lengths[1]):
        # P coordinates (thread identity)
        p_coords = [p0, p1]
        
        # This thread handles a 2x2 sub-tile (from our Y dimensions)
        print(f"\nThread P{p_coords}:")
        
        # Each thread accesses multiple elements (Y dimensions)
        y_lengths = distribution.get_y_vector_lengths()
        for y0 in range(y_lengths[0]):
            for y1 in range(y_lengths[1]):
                # Calculate actual position
                # In real GPU, this is done by the PsYsâ†’Xs adaptor
                window_pos = [p0 * y_lengths[0] + y0, p1 * y_lengths[1] + y1]
                global_pos = [window.window_origin[0] + window_pos[0],
                             window.window_origin[1] + window_pos[1]]
                value = data[global_pos[0], global_pos[1]]
                print(f"  Y[{y0},{y1}] â†’ window[{window_pos[0]},{window_pos[1]}] â†’ global{global_pos} = {value}")
```

## The Load Operation Deep Dive

The load operation is where all the sophisticated analysis comes together. When you call `window.load()`, a carefully orchestrated sequence of operations occurs:

1. **Distributed tensor creation**: Automatically creates a distributed tensor sized for the distribution
2. **Coordinate calculation**: Uses precomputed coordinates for efficiency
3. **Vectorized access**: Groups elements for vector loads based on LoadStoreTraits analysis
4. **Memory coalescing**: Ensures adjacent threads access adjacent memory
5. **Boundary handling**: Manages edge cases automatically

```{pyodide}
#| echo: true
#| output: true

# Load data from the window
distributed_tensor = window.load()

print("Load Operation Analysis:")
print(f"  Created distributed tensor: {type(distributed_tensor).__name__}")
print(f"  Data type: {distributed_tensor.data_type}")
print(f"  Elements per thread: {distributed_tensor.get_num_of_elements()}")

# Show what each thread loaded
print("\nðŸ§µ Thread Data (showing first thread's data):")
y_lengths = distribution.get_y_vector_lengths()
print(f"Thread P[0,0] loaded a {y_lengths[0]}x{y_lengths[1]} tile:")
for y0 in range(y_lengths[0]):
    for y1 in range(y_lengths[1]):
        y_indices = [y0, y1]
        value = distributed_tensor.get_element(y_indices)
        # Calculate global position for verification
        global_pos = [
            window.window_origin[0] + y0,
            window.window_origin[1] + y1
        ]
        expected = data[global_pos[0], global_pos[1]]
        match = "âœ“" if abs(value - expected) < 1e-6 else "âœ—"
        print(f"  Y{y_indices} = {value} (from global{global_pos}, expected {expected}) {match}")

print("\nâœ… Load operation automatically:")
print("- Created distributed tensor of correct size")
print("- Calculated optimal access pattern via LoadStoreTraits")
print("- Used vectorization where possible")
print("- Ensured memory coalescing across threads")
print("- Handled all coordinate transformations")
```

## Load Operation Architecture

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Load Analysis"
        Analyze["Analyze access pattern<br/>Detect coalescing opportunities"]
    end
    
    subgraph "Vectorization"
        V1["Scalar: 4 loads"]
        V2["Vector2: 2 loads"]
        V4["Vector4: 1 load"]
    end
    
    subgraph "Memory Transaction"
        Coal["Coalesced access<br/>32 threads â†’ 1 transaction"]
        NonCoal["Non-coalesced<br/>32 threads â†’ 32 transactions"]
    end
    
    subgraph "Result"
        Reg["Thread registers<br/>Local data"]
    end
    
    Analyze --> V1
    Analyze --> V2
    Analyze --> V4
    
    V4 --> Coal
    V1 --> NonCoal
    
    Coal --> Reg
    NonCoal --> Reg
    
    style V4 fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style Coal fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style NonCoal fill:#fee2e2,stroke:#ef4444,stroke-width:2px
</div>
```

## Memory Access Patterns

One of TileWindow's key features is generating optimal memory access patterns:

```{pyodide}
#| echo: true
#| output: true

def analyze_window_access_pattern(window, distribution, num_threads=4):
    """Analyze how threads access memory through the window"""
    print("Memory Access Pattern Analysis")
    print("=" * 50)
    
    # Get distribution info
    y_lengths = distribution.get_y_vector_lengths()
    
    print(f"Each thread handles: {y_lengths[0]}x{y_lengths[1]} elements")
    print(f"Window total size: {window.window_lengths}")
    
    # Simulate access pattern
    print("\nThread access pattern (first 4 threads):")
    print("Thread | First access | Access pattern")
    print("-" * 50)
    
    origin = window.window_origin
    
    for tid in range(min(4, num_threads)):
        # Thread position in P-space
        p0 = tid // 2
        p1 = tid % 2
        
        # First element this thread accesses
        first_global_i = origin[0] + p0 * y_lengths[0]
        first_global_j = origin[1] + p1 * y_lengths[1]
        
        # Linear address (assuming row-major layout)
        linear_addr = first_global_i * 8 + first_global_j  # 8 is tensor width
        
        print(f"  P[{p0},{p1}] | ({first_global_i},{first_global_j}) addr={linear_addr:3d} | ", end="")
        
        # Show access pattern
        if y_lengths[1] > 1:
            print(f"Accesses {y_lengths[1]} consecutive addresses")
        else:
            print("Single element access")
    
    # Coalescing analysis
    print("\nCoalescing Analysis:")
    print("âœ“ Threads in same warp access consecutive memory regions")
    print("âœ“ LoadStoreTraits ensures optimal access ordering")
    print("âœ“ Vectorization reduces total memory transactions")

analyze_window_access_pattern(window, distribution)
```

## Window Movement and Updates

TileWindow supports efficient window movement for sliding window algorithms. The precomputed coordinate system makes updates extremely efficient:

```{pyodide}
#| echo: true
#| output: true

# Demonstrate window movement
print("Window Movement Example:")
print(f"Original window origin: {window.window_origin}")

# Move window by (1, 1)
window.move([1, 1])
print(f"After move([1, 1]): {window.window_origin}")

# Set new origin directly
window.set_window_origin([0, 4])
print(f"After set_window_origin([0, 4]): {window.window_origin}")

# Load from new position
new_data = window.load()
print(f"\nData from new window position:")
print(f"  First element (Y[0,0]): {new_data.get_element([0, 0])}")
print(f"  Expected from global[0,4]: {data[0, 4]}")

# Show the efficiency of movement
print("\nðŸ’¡ Window Movement Efficiency:")
print("- Precomputed coordinates enable O(1) updates")
print("- No need to recalculate distributions")
print("- Perfect for sliding window algorithms")
print("- Supports convolution, pooling, and similar operations")

# Reset for next examples
window.set_window_origin([2, 2])
```

## Store Operations with Vectorization

Store operations use the same sophisticated analysis as loads. The LoadStoreTraits ensures that stores are just as efficient as loads, with the same vectorization and coalescing benefits:

```{pyodide}
#| echo: true
#| output: true

# Create output data and window
output_data = np.zeros_like(data)
output_view = make_tensor_view(output_data, make_naive_tensor_descriptor_packed([8, 8]))
output_window = make_tile_window(
    tensor_view=output_view,
    window_lengths=[4, 4],
    origin=[2, 2],
    tile_distribution=distribution
)

# Create a distributed tensor with computed values
result_tensor = make_static_distributed_tensor(np.float32, distribution)
y_lengths = distribution.get_y_vector_lengths()
for y0 in range(y_lengths[0]):
    for y1 in range(y_lengths[1]):
        # Simple computation: multiply by 2
        original_value = distributed_tensor.get_element([y0, y1])
        result_tensor.set_element([y0, y1], original_value * 2)

# Store results
output_window.store(result_tensor)

print("Store Operation Results:")
print(f"Original window data (portion):")
print(data[2:4, 2:4])
print(f"\nStored results (portion):")
print(output_data[2:4, 2:4])

print("\nâœ… Store operation features:")
print("- Uses same LoadStoreTraits analysis as load")
print("- Automatic vectorization matching load pattern")
print("- Coalesced memory writes")
print("- Space-filling curve ensures cache-friendly access")
print("- Boundary checking handled automatically")
```

## Advanced Window Operations

### Sliding Windows

```{pyodide}
#| echo: true
#| output: true

# Demonstrate sliding window pattern
def sliding_window_demo(tensor_shape, window_size, stride):
    """Show how to slide a window across a tensor"""
    print(f"Sliding {window_size}x{window_size} window with stride {stride}")
    print("Window positions:")
    
    positions = []
    for i in range(0, tensor_shape[0] - window_size + 1, stride):
        for j in range(0, tensor_shape[1] - window_size + 1, stride):
            positions.append((i, j))
            print(f"  Position ({i},{j}) covers ({i},{j}) to ({i+window_size-1},{j+window_size-1})")
    
    return positions

# Create sliding windows
positions = sliding_window_demo(tensor_shape=[8, 8], window_size=4, stride=2)
print(f"\nTotal windows: {len(positions)}")

print("\nðŸ’¡ Sliding Window Benefits:")
print("- Precomputed coordinates make sliding O(1)")
print("- Perfect for convolution operations")
print("- Enables efficient pooling layers")
print("- Supports overlapping computations")
```

### Overlapping Windows

```{pyodide}
#| echo: true
#| output: true

# Demonstrate overlapping windows for convolution-like operations
def create_overlapping_windows(tensor_shape, window_size, overlap):
    """Create overlapping windows for operations like convolution"""
    stride = window_size - overlap
    
    print(f"Creating {window_size}x{window_size} windows with {overlap} overlap")
    print(f"Effective stride: {stride}")
    
    windows = []
    for i in range(0, tensor_shape[0] - window_size + 1, stride):
        for j in range(0, tensor_shape[1] - window_size + 1, stride):
            windows.append({
                'origin': (i, j),
                'overlap_left': j > 0,
                'overlap_top': i > 0,
                'overlap_right': j + window_size < tensor_shape[1],
                'overlap_bottom': i + window_size < tensor_shape[0]
            })
    
    # Show overlap pattern
    print(f"\nWindow overlap pattern ({len(windows)} windows):")
    for idx, w in enumerate(windows[:4]):  # Show first few
        overlaps = []
        if w['overlap_left']: overlaps.append('left')
        if w['overlap_top']: overlaps.append('top')
        if w['overlap_right']: overlaps.append('right')
        if w['overlap_bottom']: overlaps.append('bottom')
        
        print(f"  Window {idx} at {w['origin']}: overlaps {', '.join(overlaps) if overlaps else 'none'}")
    
    return windows

# Create overlapping windows
windows = create_overlapping_windows([8, 8], window_size=3, overlap=1)
```

## Complete Load-Compute-Store Pipeline

Here's a complete example showing how all components work together:

```{pyodide}
#| echo: true
#| output: true

def matrix_operation_with_tilewindow(input_matrix, operation_func, window_size=[4, 4]):
    """
    Demonstrates complete load-compute-store pipeline with TileWindow.
    
    This simulates how GPU kernels process data in tiles.
    """
    rows, cols = input_matrix.shape
    output_matrix = np.zeros_like(input_matrix)
    
    # Create tensor views
    input_view = make_tensor_view(
        input_matrix, 
        make_naive_tensor_descriptor_packed(list(input_matrix.shape))
    )
    output_view = make_tensor_view(
        output_matrix,
        make_naive_tensor_descriptor_packed(list(output_matrix.shape))
    )
    
    # Create distribution for window
    encoding = make_tile_distribution_encoding(
        rs_lengths=[],
        hs_lengthss=[[2, 2], [2, 2]],  # 4x4 window with 2x2 per thread
        ps_to_rhss_major=[[1], [2]],
        ps_to_rhss_minor=[[0], [0]],
        ys_to_rhs_major=[1, 2],
        ys_to_rhs_minor=[1, 1]
    )
    dist = make_static_tile_distribution(encoding)
    
    # Process in tiles
    tiles_processed = 0
    for row_start in range(0, rows, window_size[0]):
        for col_start in range(0, cols, window_size[1]):
            # Create windows at current position
            input_window = make_tile_window(
                input_view, window_size, [row_start, col_start], dist
            )
            output_window = make_tile_window(
                output_view, window_size, [row_start, col_start], dist
            )
            
            # Load â†’ Compute â†’ Store pipeline
            print(f"\nProcessing tile at ({row_start}, {col_start}):")
            
            # Load phase
            tile_data = input_window.load()
            print("  âœ“ Load: Data loaded with optimal access pattern")
            
            # Compute phase
            result = make_static_distributed_tensor(np.float32, dist)
            y_lens = dist.get_y_vector_lengths()
            for y0 in range(y_lens[0]):
                for y1 in range(y_lens[1]):
                    value = tile_data.get_element([y0, y1])
                    result.set_element([y0, y1], operation_func(value))
            print("  âœ“ Compute: Applied operation to all elements")
            
            # Store phase
            output_window.store(result)
            print("  âœ“ Store: Results written with vectorized stores")
            
            tiles_processed += 1
    
    return output_matrix, tiles_processed

# Test the pipeline
test_matrix = np.arange(64, dtype=np.float32).reshape(8, 8)
result_matrix, num_tiles = matrix_operation_with_tilewindow(
    test_matrix,
    lambda x: x ** 2  # Square each element
)

print(f"\nPipeline Summary:")
print(f"  Input matrix shape: {test_matrix.shape}")
print(f"  Tiles processed: {num_tiles}")
print(f"  Sample input: {test_matrix[0:2, 0:2]}")
print(f"  Sample output: {result_matrix[0:2, 0:2]}")

print("\nðŸš€ This demonstrates the pattern used in every GPU kernel:")
print("1. Create tile windows for input/output")
print("2. Load tile data into distributed tensor")
print("3. Compute on distributed data") 
print("4. Store results back through tile window")
print("\nAll with automatic optimization via LoadStoreTraits!")
```

## Computation Architecture

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 30%;">
graph TB
    subgraph "Load Phase"
        LW["Load Window<br/>Global â†’ Registers"]
    end
    
    subgraph "Compute Phase"
        C1["Thread-local compute<br/>In registers"]
        C2["Warp-level shuffle<br/>Data exchange"]
        C3["Block-level reduction<br/>Shared memory"]
    end
    
    subgraph "Store Phase"
        SW["Store Window<br/>Registers â†’ Global"]
    end
    
    LW --> C1
    C1 --> C2
    C2 --> C3
    C3 --> SW
    
    style LW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style SW fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## C++ Usage Example

The real power of TileWindow comes from its C++ implementation:

<details>
<summary>Click to show C++ code</summary>

```cpp
// Complete example: Matrix multiplication using TileWindow
template<typename AType, typename BType, typename CType>
__global__ void gemm_kernel_with_windows(
    const AType* __restrict__ a_ptr,
    const BType* __restrict__ b_ptr,
    CType* __restrict__ c_ptr,
    index_t M, index_t N, index_t K)
{
    // Create tensor views
    auto a_tensor = make_naive_tensor_view(
        a_ptr, make_tuple(M, K), make_tuple(K, 1));
    auto b_tensor = make_naive_tensor_view(
        b_ptr, make_tuple(K, N), make_tuple(N, 1));
    auto c_tensor = make_naive_tensor_view(
        c_ptr, make_tuple(M, N), make_tuple(N, 1));
    
    // Define tile sizes
    constexpr index_t tile_m = 32;
    constexpr index_t tile_n = 32;
    constexpr index_t tile_k = 8;
    
    // Create distributions
    auto a_dist = make_static_tile_distribution<...>();
    auto b_dist = make_static_tile_distribution<...>();
    auto c_dist = make_static_tile_distribution<...>();
    
    // Calculate tile position
    const index_t block_m = blockIdx.y * tile_m;
    const index_t block_n = blockIdx.x * tile_n;
    
    // Create tile windows
    auto a_window = make_tile_window(
        a_tensor,
        make_tuple(tile_m, tile_k),
        make_tuple(block_m, 0),
        a_dist);
    
    auto b_window = make_tile_window(
        b_tensor,
        make_tuple(tile_k, tile_n),
        make_tuple(0, block_n),
        b_dist);
    
    auto c_window = make_tile_window(
        c_tensor,
        make_tuple(tile_m, tile_n),
        make_tuple(block_m, block_n),
        c_dist);
    
    // Create distributed tensors for register storage
    auto a_reg = make_static_distributed_tensor<AType>(a_dist);
    auto b_reg = make_static_distributed_tensor<BType>(b_dist);
    auto c_reg = make_static_distributed_tensor<CType>(c_dist);
    
    // Initialize accumulator
    c_reg.clear();
    
    // Main GEMM loop
    for(index_t k = 0; k < K; k += tile_k) {
        // Update window positions
        a_window.set_window_origin(make_tuple(0, k));
        b_window.set_window_origin(make_tuple(k, 0));
        
        // Load tiles - LoadStoreTraits ensures optimal pattern
        a_window.load(a_reg);
        b_window.load(b_reg);
        
        // Compute
        gemm(a_reg, b_reg, c_reg);
    }
    
    // Store result - using same optimized pattern
    c_window.store(c_reg);
}
```

</details>

## Performance Characteristics

```{=html}
<div class="mermaid">
graph LR
    subgraph "Memory Access Optimization"
        V["Vectorization<br/>4x fewer transactions"]
        C["Coalescing<br/>32x bandwidth efficiency"]
        P["Precomputation<br/>Zero overhead addressing"]
        S["Space-filling<br/>Optimal cache usage"]
    end
    
    subgraph "Hardware Utilization"
        BW["Memory Bandwidth<br/>Near 100% utilization"]
        L["Latency Hiding<br/>Overlapped operations"]
        R["Register Reuse<br/>Minimal spills"]
    end
    
    V --> BW
    C --> BW
    P --> L
    S --> R
    
    style V fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style BW fill:#d1fae5,stroke:#10b981,stroke-width:3px
</div>
```

## Load-Compute-Store Pipeline

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Iteration i"
        L1["Load A[i]<br/>Load B[i]"]
        C1["Compute<br/>C += A[i] Ã— B[i]"]
    end
    
    subgraph "Iteration i+1"
        L2["Load A[i+1]<br/>Load B[i+1]"]
        C2["Compute<br/>C += A[i+1] Ã— B[i+1]"]
    end
    
    subgraph "Final"
        S["Store C"]
    end
    
    L1 --> C1
    C1 --> L2
    L2 --> C2
    C2 --> S
    
    style L1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style S fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Pipeline Optimization

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Basic Pipeline"
        B1["Load"] --> B2["Compute"] --> B3["Store"]
        BTime["Time: 3 units"]
    end
    
    subgraph "Software Pipelining"
        P1["Load[i+1]"]
        P2["Compute[i]"]
        P3["Store[i-1]"]
        PTime["Time: 1 unit per iteration"]
    end
    
    subgraph "Double Buffering"
        DB1["Buffer A: Load"]
        DB2["Buffer B: Compute"]
        DBSwap["Swap buffers"]
    end
    
    B3 --> P1
    P1 --> P2
    P2 --> P3
    
    P3 --> DB1
    DB1 --> DB2
    DB2 --> DBSwap
    
    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style P2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style P3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Window Configuration Space

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Window Parameters"
        Size["Window Size<br/>Trade-off: registers vs reuse"]
        Origin["Window Origin<br/>Starting position"]
        Stride["Window Stride<br/>Overlap pattern"]
    end
    
    subgraph "Performance Factors"
        Reg["Register Usage<br/>Limited resource"]
        Reuse["Data Reuse<br/>Bandwidth savings"]
        Coal["Coalescing<br/>Memory efficiency"]
    end
    
    subgraph "Optimal Configuration"
        Opt["Balance all factors<br/>Architecture-specific"]
    end
    
    Size --> Reg
    Size --> Reuse
    Origin --> Coal
    Stride --> Reuse
    
    Reg --> Opt
    Reuse --> Opt
    Coal --> Opt
    
    style Opt fill:#d1fae5,stroke:#10b981,stroke-width:3px
</div>
```

## Best Practices

### 1. Window Size Selection

```{pyodide}
#| echo: true
#| output: true

def recommend_window_size(tensor_shape, available_registers, elements_per_thread):
    """Recommend optimal window size based on constraints"""
    print("Window Size Recommendation")
    print("=" * 50)
    print(f"Tensor shape: {tensor_shape}")
    print(f"Available registers per thread: {available_registers}")
    print(f"Elements per thread: {elements_per_thread}")
    
    # Calculate maximum window size based on register constraints
    max_window_elements = available_registers // elements_per_thread
    max_window_dim = int(max_window_elements ** 0.5)
    
    # Consider common tile sizes
    common_sizes = [8, 16, 32, 64, 128]
    recommended = []
    
    for size in common_sizes:
        if size <= max_window_dim and all(dim % size == 0 for dim in tensor_shape):
            recommended.append(size)
    
    print(f"\nRecommended window sizes: {recommended}")
    if recommended:
        optimal = recommended[-1]  # Largest that fits
        print(f"Optimal choice: {optimal}x{optimal}")
        print(f"  - Uses {optimal*optimal*elements_per_thread} registers per thread")
        print(f"  - Achieves {(optimal*optimal)/max_window_elements*100:.1f}% register utilization")
        print(f"  - LoadStoreTraits will optimize access patterns")
    
    return recommended

# Example recommendation
recommend_window_size(
    tensor_shape=[256, 256],
    available_registers=256,
    elements_per_thread=4
)
```

### 2. Access Pattern Optimization

```{pyodide}
#| echo: true
#| output: true

def optimize_access_pattern(window_shape, thread_block_shape):
    """Determine optimal access pattern for coalescing"""
    print("Access Pattern Optimization")
    print("=" * 50)
    print(f"Window shape: {window_shape}")
    print(f"Thread block: {thread_block_shape}")
    
    # Calculate threads per row for coalescing
    threads_per_row = thread_block_shape[1]
    elements_per_row = window_shape[1]
    
    if elements_per_row % threads_per_row == 0:
        print("\nâœ“ Perfect coalescing possible!")
        print(f"  Each thread loads {elements_per_row // threads_per_row} consecutive elements")
        print("  LoadStoreTraits will automatically vectorize these accesses")
        access_pattern = "row-major coalesced"
    else:
        print("\nâš  Partial coalescing")
        print("  Consider adjusting window or thread block dimensions")
        access_pattern = "row-major partial"
    
    # Suggest vectorization
    vector_sizes = [4, 2, 1]
    for vec_size in vector_sizes:
        if elements_per_row % (threads_per_row * vec_size) == 0:
            print(f"\nVectorization: Use float{vec_size} for optimal loads")
            print(f"  {elements_per_row // (threads_per_row * vec_size)} vector loads per thread")
            print(f"  Space-filling curve ensures cache-friendly order")
            break
    
    return access_pattern

# Optimize for common configurations
optimize_access_pattern(
    window_shape=[32, 32],
    thread_block_shape=[8, 32]
)
```

### 3. Choose Distribution for Memory Access

```{pyodide}
#| echo: true
#| output: true

# Example: Different distributions create different access patterns
def create_and_analyze_distribution(name, hs_lengthss, ys_to_rhs_minor):
    """Create distribution and analyze its access pattern."""
    encoding = make_tile_distribution_encoding(
        rs_lengths=[],
        hs_lengthss=hs_lengthss,
        ps_to_rhss_major=[[1], [2]],
        ps_to_rhss_minor=[[0], [0]],
        ys_to_rhs_major=[1, 2],
        ys_to_rhs_minor=ys_to_rhs_minor
    )
    dist = make_static_tile_distribution(encoding)
    traits = LoadStoreTraits(dist, np.float32)
    
    print(f"\n{name} Distribution:")
    print(f"  Hierarchical structure: {hs_lengthss}")
    print(f"  Yâ†’RH minor mapping: {ys_to_rhs_minor}")
    print(f"  Vector dimension: Y{traits.vector_dim_y}")
    print(f"  Scalars per vector: {traits.scalar_per_vector}")
    print(f"  Access efficiency: {traits.scalar_per_vector}x vectorization")
    print(f"  Space-filling curve: {'Snake' if traits.sfc_ys.snake_curved else 'Linear'}")

# Compare different distributions
create_and_analyze_distribution(
    "Optimized for Vectorization",
    [[2, 4], [2, 4]],  # 4-element vectors possible
    [1, 1]  # Both Y dims use inner hierarchy
)

create_and_analyze_distribution(
    "Less Optimal",
    [[4, 2], [4, 2]],  # Only 2-element vectors
    [1, 1]
)
```

## Summary

TileWindow provides:

- **Automatic optimization**: Generates optimal memory access patterns through LoadStoreTraits
- **Distribution awareness**: Works seamlessly with TileDistribution
- **Space-filling curves**: Optimal traversal order for cache efficiency
- **Vectorization**: Automatic multi-element operations
- **Precomputation**: Zero-overhead coordinate transformations
- **Flexible windowing**: Supports various access patterns and window configurations
- **Safety**: Automatic boundary handling

Key benefits:

1. **Performance**: Achieves peak memory bandwidth through coalescing and vectorization
2. **Productivity**: Eliminates manual memory management code
3. **Correctness**: Automatic boundary checking and handling
4. **Composability**: Integrates cleanly with other CK abstractions
5. **Intelligence**: LoadStoreTraits analyzes and optimizes every access

The TileWindow abstraction is essential for building high-performance GPU kernels, providing a clean interface for complex memory access patterns while maintaining peak performance. The sophisticated analysis performed by LoadStoreTraits ensures that every memory operation is as efficient as possible, while the space-filling curve traversal maximizes cache utilization.
