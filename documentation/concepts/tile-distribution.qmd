---
title: "Tile Distribution: Parallel Processing Coordination"
format: live-html
---

## üéØ **What is Tile Distribution?**

**Tile Distribution** is the runtime coordinator that transforms the mathematical **tile distribution encoding** into actual parallel processing components. It creates the **adaptors** and **descriptors** needed to map (P,Y) coordinates to final tensor coordinates (X).

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import (
    make_tile_distribution_encoding,
    make_static_tile_distribution,
    make_tile_distributed_span,
    make_tile_distributed_index,
    make_tile_distribution,
    make_tensor_descriptor_from_adaptor
)
from pytensor.tensor_coordinate import MultiIndex, make_tensor_adaptor_coordinate

print("üéØ Tile Distribution: From Encoding to Runtime Coordination")
print("=" * 60)
```

## üîß **Creating Tile Distribution from Encoding**

Let's build the complete RMSNorm tile distribution step by step:

```{pyodide}
#| echo: true
#| output: true

# Step 1: Create the encoding (mathematical specification)
variables = {
    "Repeat_M": 4, "WarpPerBlock_M": 2, "ThreadPerWarp_M": 8, "Vector_M": 4,
    "Repeat_N": 4, "WarpPerBlock_N": 2, "ThreadPerWarp_N": 8, "Vector_N": 4
}

encoding = make_tile_distribution_encoding(
    rs_lengths=[],
    hs_lengthss=[
        [variables["Repeat_M"], variables["WarpPerBlock_M"], variables["ThreadPerWarp_M"], variables["Vector_M"]],
        [variables["Repeat_N"], variables["WarpPerBlock_N"], variables["ThreadPerWarp_N"], variables["Vector_N"]]
    ],
    ps_to_rhss_major=[[1, 2], [1, 2]],
    ps_to_rhss_minor=[[1, 1], [2, 2]],
    ys_to_rhs_major=[1, 1, 2, 2],
    ys_to_rhs_minor=[0, 3, 0, 3]
)

print("üìã Encoding Created:")
print(f"  ‚Ä¢ X dimensions: {encoding.ndim_x}")
print(f"  ‚Ä¢ P dimensions: {encoding.ndim_p}")  
print(f"  ‚Ä¢ Y dimensions: {encoding.ndim_y}")
print(f"  ‚Ä¢ R dimensions: {encoding.ndim_r}")
```

```{pyodide}
#| echo: true
#| output: true

# Step 2: Create static tile distribution (runtime coordinator)
distribution = make_static_tile_distribution(encoding)

print("üöÄ Tile Distribution Created:")
print(f"  ‚Ä¢ Type: {type(distribution).__name__}")
print(f"  ‚Ä¢ Tensor dimensions (X): {distribution.ndim_x}")
print(f"  ‚Ä¢ Thread partition dimensions (P): {distribution.ndim_p}")
print(f"  ‚Ä¢ Element indexing dimensions (Y): {distribution.ndim_y}")
print(f"  ‚Ä¢ Replication dimensions (R): {distribution.ndim_r}")

print(f"\nüìê Dimension Lengths:")
x_lengths = distribution.get_lengths()
print(f"  ‚Ä¢ X lengths: {x_lengths}")
print(f"  ‚Ä¢ Total tensor size: {np.prod(x_lengths):,} elements")
```

## üîó **Internal Components: Adaptors and Descriptors**

The tile distribution contains two key components:

```{pyodide}
#| echo: true
#| output: true

print("üîó Internal Components Analysis:")

# 1. PS_YS ‚Üí XS Adaptor (coordinate mapping)
ps_ys_to_xs_adaptor = distribution.ps_ys_to_xs_adaptor
print(f"\n1Ô∏è‚É£ PS_YS ‚Üí XS Adaptor:")
print(f"  ‚Ä¢ Type: {type(ps_ys_to_xs_adaptor).__name__}")
print(f"  ‚Ä¢ Top dimensions: {ps_ys_to_xs_adaptor.get_num_of_top_dimension()}")
print(f"  ‚Ä¢ Bottom dimensions: {ps_ys_to_xs_adaptor.get_num_of_bottom_dimension()}")
print(f"  ‚Ä¢ Hidden dimensions: {ps_ys_to_xs_adaptor.get_num_of_hidden_dimension()}")
print(f"  ‚Ä¢ Purpose: Maps (P‚ÇÄ,P‚ÇÅ,Y‚ÇÄ,Y‚ÇÅ,Y‚ÇÇ,Y‚ÇÉ) ‚Üí (X‚ÇÄ,X‚ÇÅ)")

# 2. YS ‚Üí D Descriptor (linearization)
ys_to_d_descriptor = distribution.ys_to_d_descriptor
print(f"\n2Ô∏è‚É£ YS ‚Üí D Descriptor:")
print(f"  ‚Ä¢ Type: {type(ys_to_d_descriptor).__name__}")
print(f"  ‚Ä¢ Dimensions: {ys_to_d_descriptor.get_num_of_dimension()}")
print(f"  ‚Ä¢ Element space size: {ys_to_d_descriptor.get_element_space_size()}")
print(f"  ‚Ä¢ Lengths: {ys_to_d_descriptor.get_lengths()}")
print(f"  ‚Ä¢ Purpose: Maps (Y‚ÇÄ,Y‚ÇÅ,Y‚ÇÇ,Y‚ÇÉ) ‚Üí linear index")
```

## üßµ **Partition Index: Thread Identification**

The distribution determines which thread you are via partition indices:

```{pyodide}
#| echo: true
#| output: true

print("üßµ Thread Partition System:")

# Get current partition index (simulated)
partition_idx = distribution.get_partition_index()
print(f"  ‚Ä¢ Current partition index: {partition_idx}")
print(f"  ‚Ä¢ P‚ÇÄ (thread warp ID): {partition_idx[0]}")
print(f"  ‚Ä¢ P‚ÇÅ (thread lane ID): {partition_idx[1]}")

# Calculate tensor coordinates for this thread
x_index = distribution.calculate_index(partition_idx)
print(f"  ‚Ä¢ Tensor coordinates: {x_index.to_list()}")

print(f"\nüîç Understanding Partition Indices:")
print(f"  ‚Ä¢ P coordinates identify which thread you are")
print(f"  ‚Ä¢ Different threads get different P values")
print(f"  ‚Ä¢ P values map to different starting positions in tensor")

# Show some example mappings
example_partitions = [[0, 0], [0, 1], [1, 0], [1, 1]]
print(f"\nüìä Example Thread Mappings:")
for p_idx in example_partitions:
    x_coord = distribution.calculate_index(p_idx)
    print(f"  Thread P{p_idx} ‚Üí Tensor X{x_coord.to_list()}")
```

## üìè **Distributed Spans: Spatial Distribution**

Spans define how computation is distributed across space:

```{pyodide}
#| echo: true
#| output: true

print("üìè Distributed Spans Analysis:")

# Get distributed spans
spans = distribution.get_distributed_spans()
print(f"  ‚Ä¢ Number of spans: {len(spans)}")

for x_idx, span in enumerate(spans):
    print(f"\nüé® X{x_idx} Span:")
    print(f"  ‚Ä¢ Partial lengths: {span.partial_lengths}")
    print(f"  ‚Ä¢ Total elements: {np.prod(span.partial_lengths)}")
    print(f"  ‚Ä¢ Is static: {span.is_static()}")

print(f"\nüîç Span Interpretation:")
print(f"  ‚Ä¢ Each processing element handles a 'span' of data")
print(f"  ‚Ä¢ Spans define the spatial locality pattern")
print(f"  ‚Ä¢ Y coordinates index within each span")
print(f"  ‚Ä¢ Multiple threads can work on different spans")
```

```{pyodide}
#| echo: true
#| output: true

# Create example distributed spans and indices
print("üõ†Ô∏è Working with Spans and Indices:")

# Create custom spans
custom_span = make_tile_distributed_span([4, 4])  # 4x4 spatial region
print(f"  ‚Ä¢ Custom span: {custom_span}")

# Create distributed indices
custom_index = make_tile_distributed_index([2, 3])  # Specific position in span
print(f"  ‚Ä¢ Custom index: {custom_index}")

print(f"\nüí° Practical Usage:")
print(f"  ‚Ä¢ Spans define work distribution patterns")
print(f"  ‚Ä¢ Indices specify positions within spans")
print(f"  ‚Ä¢ Together they enable efficient parallel access")
```

## üéØ **Y Dimension Processing**

Y coordinates determine which data elements each thread processes:

```{pyodide}
#| echo: true
#| output: true

print("üéØ Y Dimension Processing:")

# Get Y dimension properties
y_lengths = distribution.get_y_vector_lengths()
y_strides = distribution.get_y_vector_strides()

print(f"  ‚Ä¢ Y dimension count: {distribution.ndim_y}")
print(f"  ‚Ä¢ Y lengths: {y_lengths}")
print(f"  ‚Ä¢ Y strides: {y_strides}")

print(f"\nüîÑ Y Coordinate Mapping:")
print(f"  ‚Ä¢ Y‚ÇÄ: Repeat dimension (outer loop)")
print(f"  ‚Ä¢ Y‚ÇÅ: Vector dimension (SIMD)")
print(f"  ‚Ä¢ Y‚ÇÇ: Repeat dimension (outer loop)")
print(f"  ‚Ä¢ Y‚ÇÉ: Vector dimension (SIMD)")

# Example: convert access index to Y coordinates
access_indices = [0, 1, 5, 10, 15]
print(f"\nüìä Access Index ‚Üí Y Coordinates:")
for access_idx in access_indices:
    y_indices = distribution.get_y_indices_from_distributed_indices(access_idx)
    print(f"  Access[{access_idx}] ‚Üí Y{y_indices}")
```

## üèóÔ∏è **Creating Tensor Descriptor from Adaptor**

Transform adaptors into descriptors for tensor operations:

```{pyodide}
#| echo: true
#| output: true

print("üèóÔ∏è Tensor Descriptor from Adaptor:")

# Create descriptor from the PS_YS‚ÜíXS adaptor
adaptor = distribution.ps_ys_to_xs_adaptor
element_space_size = 1024  # Example buffer size

descriptor = make_tensor_descriptor_from_adaptor(adaptor, element_space_size)
print(f"  ‚Ä¢ Created descriptor: {descriptor}")
print(f"  ‚Ä¢ Descriptor dimensions: {descriptor.get_num_of_dimension()}")
print(f"  ‚Ä¢ Descriptor lengths: {descriptor.get_lengths()}")
print(f"  ‚Ä¢ Element space: {descriptor.get_element_space_size()}")

print(f"\nüîÑ Adaptor vs Descriptor:")
print(f"  ‚Ä¢ Adaptor: Handles coordinate transformations")
print(f"  ‚Ä¢ Descriptor: Adds linearization and memory layout")
print(f"  ‚Ä¢ Both are needed for complete tensor operations")
```

## üß™ **Advanced Distribution Operations**

Explore advanced coordinate mapping functionality:

```{pyodide}
#| echo: true
#| output: true

print("üß™ Advanced Coordinate Operations:")

# Test coordinate mapping through adaptor
ps_ys_coords = [0, 1, 2, 1, 0, 3]  # P‚ÇÄ,P‚ÇÅ,Y‚ÇÄ,Y‚ÇÅ,Y‚ÇÇ,Y‚ÇÉ
multi_idx = MultiIndex(len(ps_ys_coords), ps_ys_coords)

# Create adaptor coordinate
adaptor_coord = make_tensor_adaptor_coordinate(
    distribution.ps_ys_to_xs_adaptor, 
    multi_idx
)

print(f"  ‚Ä¢ Input PS_YS: {ps_ys_coords}")
print(f"  ‚Ä¢ Top index: {adaptor_coord.get_top_index().to_list()}")
print(f"  ‚Ä¢ Bottom index: {adaptor_coord.get_bottom_index().to_list()}")
print(f"  ‚Ä¢ Hidden dimensions: {adaptor_coord.get_hidden_index().to_list()}")

print(f"\nüîç Coordinate Flow:")
print(f"  ‚Ä¢ PS_YS input ‚Üí Hidden transformations ‚Üí X output")
print(f"  ‚Ä¢ Hidden dims capture intermediate tile structure")
print(f"  ‚Ä¢ Final X coords are actual tensor positions")
```

## üöÄ **Real-World Usage Patterns**

Understanding how tile distribution works in practice:

```{pyodide}
#| echo: true
#| output: true

print("üöÄ Real-World Usage Patterns:")

print(f"\n1Ô∏è‚É£ Thread Initialization:")
print(f"  ‚Ä¢ Each GPU thread calls get_partition_index()")
print(f"  ‚Ä¢ Gets unique P coordinates identifying the thread")
print(f"  ‚Ä¢ Uses P to calculate starting tensor position")

print(f"\n2Ô∏è‚É£ Data Element Processing:")
print(f"  ‚Ä¢ Thread iterates through Y coordinate space")
print(f"  ‚Ä¢ Each Y combination identifies a data element")
print(f"  ‚Ä¢ PS_YS‚ÜíXS adaptor maps to final tensor coordinates")

print(f"\n3Ô∏è‚É£ Memory Access:")
print(f"  ‚Ä¢ YS‚ÜíD descriptor linearizes Y coordinates")
print(f"  ‚Ä¢ Enables efficient sequential memory access")
print(f"  ‚Ä¢ Spans ensure spatial locality")

print(f"\n4Ô∏è‚É£ Parallel Efficiency:")
total_work = np.prod(distribution.get_lengths())
threads_per_block = variables['WarpPerBlock_M'] * variables['WarpPerBlock_N'] * variables['ThreadPerWarp_M'] * variables['ThreadPerWarp_N']
work_per_thread = np.prod(y_lengths)

print(f"  ‚Ä¢ Total tensor elements: {total_work:,}")
print(f"  ‚Ä¢ Threads per block: {threads_per_block}")
print(f"  ‚Ä¢ Work per thread: {work_per_thread}")
print(f"  ‚Ä¢ Parallel efficiency: {100 * threads_per_block * work_per_thread / total_work:.1f}%")
```

## üè≠ **Factory Functions Summary**

Complete reference for tile distribution creation:

```{pyodide}
#| echo: true
#| output: true

print("üè≠ Tile Distribution Factory Functions:")

print(f"\n1Ô∏è‚É£ make_tile_distribution_encoding()")
print(f"   ‚Ä¢ Creates mathematical encoding from graph structure")
print(f"   ‚Ä¢ Input: R/H sequences and P/Y mappings")
print(f"   ‚Ä¢ Output: TileDistributionEncoding")

print(f"\n2Ô∏è‚É£ make_static_tile_distribution()")
print(f"   ‚Ä¢ Creates runtime distribution from encoding")
print(f"   ‚Ä¢ Input: TileDistributionEncoding")
print(f"   ‚Ä¢ Output: TileDistribution with adaptors/descriptors")

print(f"\n3Ô∏è‚É£ make_tile_distribution()")
print(f"   ‚Ä¢ Creates distribution from custom components")
print(f"   ‚Ä¢ Input: adaptor, descriptor, encoding")
print(f"   ‚Ä¢ Output: TileDistribution")

print(f"\n4Ô∏è‚É£ make_tile_distributed_span()")
print(f"   ‚Ä¢ Creates span from partial lengths")
print(f"   ‚Ä¢ Input: List of lengths")
print(f"   ‚Ä¢ Output: TileDistributedSpan")

print(f"\n5Ô∏è‚É£ make_tile_distributed_index()")
print(f"   ‚Ä¢ Creates index from partial indices")
print(f"   ‚Ä¢ Input: List of indices")
print(f"   ‚Ä¢ Output: TileDistributedIndex")

print(f"\n6Ô∏è‚É£ make_tensor_descriptor_from_adaptor()")
print(f"   ‚Ä¢ Creates descriptor from adaptor")
print(f"   ‚Ä¢ Input: TensorAdaptor + element_space_size")
print(f"   ‚Ä¢ Output: TensorDescriptor")
```

## üéì **Key Takeaways**

1. **Runtime Bridge**: Tile distribution transforms mathematical encoding into runtime components
2. **Dual Components**: Contains both PS_YS‚ÜíXS adaptor and YS‚ÜíD descriptor for complete functionality
3. **Thread Coordination**: Partition indices identify threads and map them to tensor regions
4. **Spatial Locality**: Distributed spans ensure efficient memory access patterns
5. **Y Processing**: Y coordinates define per-thread work and enable vectorized operations
6. **Integration Ready**: Provides the foundation for tile windows, sweep operations, and tensor views

The tile distribution serves as the **runtime coordinator** that makes parallel tensor computation possible with optimal efficiency! 

**Next**: Static Distributed Tensors show how to use tile distributions for actual tensor operations. 