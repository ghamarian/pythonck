---
title: "Tile Distribution: Parallel Processing Coordination"
format: live-html
---

## 🎯 **What is Tile Distribution?**

**Tile Distribution** is the runtime coordinator that transforms the mathematical **tile distribution encoding** into actual parallel processing components. It creates the **adaptors** and **descriptors** needed to map (P,Y) coordinates to final tensor coordinates (X).

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import (
    make_tile_distribution_encoding,
    make_static_tile_distribution,
    make_tile_distributed_span,
    make_tile_distributed_index,
    make_tile_distribution,
    make_tensor_descriptor_from_adaptor
)
from pytensor.tensor_coordinate import MultiIndex, make_tensor_adaptor_coordinate

print("🎯 Tile Distribution: From Encoding to Runtime Coordination")
print("=" * 60)
```

## 🔧 **Creating Tile Distribution from Encoding**

Let's build the complete RMSNorm tile distribution step by step:

```{pyodide}
#| echo: true
#| output: true

# Step 1: Create the encoding (mathematical specification)
variables = {
    "Repeat_M": 4, "WarpPerBlock_M": 2, "ThreadPerWarp_M": 8, "Vector_M": 4,
    "Repeat_N": 4, "WarpPerBlock_N": 2, "ThreadPerWarp_N": 8, "Vector_N": 4
}

encoding = make_tile_distribution_encoding(
    rs_lengths=[],
    hs_lengthss=[
        [variables["Repeat_M"], variables["WarpPerBlock_M"], variables["ThreadPerWarp_M"], variables["Vector_M"]],
        [variables["Repeat_N"], variables["WarpPerBlock_N"], variables["ThreadPerWarp_N"], variables["Vector_N"]]
    ],
    ps_to_rhss_major=[[1, 2], [1, 2]],
    ps_to_rhss_minor=[[1, 1], [2, 2]],
    ys_to_rhs_major=[1, 1, 2, 2],
    ys_to_rhs_minor=[0, 3, 0, 3]
)

print("📋 Encoding Created:")
print(f"  • X dimensions: {encoding.ndim_x}")
print(f"  • P dimensions: {encoding.ndim_p}")  
print(f"  • Y dimensions: {encoding.ndim_y}")
print(f"  • R dimensions: {encoding.ndim_r}")
```

```{pyodide}
#| echo: true
#| output: true

# Step 2: Create static tile distribution (runtime coordinator)
distribution = make_static_tile_distribution(encoding)

print("🚀 Tile Distribution Created:")
print(f"  • Type: {type(distribution).__name__}")
print(f"  • Tensor dimensions (X): {distribution.ndim_x}")
print(f"  • Thread partition dimensions (P): {distribution.ndim_p}")
print(f"  • Element indexing dimensions (Y): {distribution.ndim_y}")
print(f"  • Replication dimensions (R): {distribution.ndim_r}")

print(f"\n📐 Dimension Lengths:")
x_lengths = distribution.get_lengths()
print(f"  • X lengths: {x_lengths}")
print(f"  • Total tensor size: {np.prod(x_lengths):,} elements")
```

## 🔗 **Internal Components: Adaptors and Descriptors**

The tile distribution contains two key components:

```{pyodide}
#| echo: true
#| output: true

print("🔗 Internal Components Analysis:")

# 1. PS_YS → XS Adaptor (coordinate mapping)
ps_ys_to_xs_adaptor = distribution.ps_ys_to_xs_adaptor
print(f"\n1️⃣ PS_YS → XS Adaptor:")
print(f"  • Type: {type(ps_ys_to_xs_adaptor).__name__}")
print(f"  • Top dimensions: {ps_ys_to_xs_adaptor.get_num_of_top_dimension()}")
print(f"  • Bottom dimensions: {ps_ys_to_xs_adaptor.get_num_of_bottom_dimension()}")
print(f"  • Hidden dimensions: {ps_ys_to_xs_adaptor.get_num_of_hidden_dimension()}")
print(f"  • Purpose: Maps (P₀,P₁,Y₀,Y₁,Y₂,Y₃) → (X₀,X₁)")

# 2. YS → D Descriptor (linearization)
ys_to_d_descriptor = distribution.ys_to_d_descriptor
print(f"\n2️⃣ YS → D Descriptor:")
print(f"  • Type: {type(ys_to_d_descriptor).__name__}")
print(f"  • Dimensions: {ys_to_d_descriptor.get_num_of_dimension()}")
print(f"  • Element space size: {ys_to_d_descriptor.get_element_space_size()}")
print(f"  • Lengths: {ys_to_d_descriptor.get_lengths()}")
print(f"  • Purpose: Maps (Y₀,Y₁,Y₂,Y₃) → linear index")
```

## 🧵 **Partition Index: Thread Identification**

The distribution determines which thread you are via partition indices:

```{pyodide}
#| echo: true
#| output: true

print("🧵 Thread Partition System:")

# Get current partition index (simulated)
partition_idx = distribution.get_partition_index()
print(f"  • Current partition index: {partition_idx}")
print(f"  • P₀ (thread warp ID): {partition_idx[0]}")
print(f"  • P₁ (thread lane ID): {partition_idx[1]}")

# Calculate tensor coordinates for this thread
x_index = distribution.calculate_index(partition_idx)
print(f"  • Tensor coordinates: {x_index.to_list()}")

print(f"\n🔍 Understanding Partition Indices:")
print(f"  • P coordinates identify which thread you are")
print(f"  • Different threads get different P values")
print(f"  • P values map to different starting positions in tensor")

# Show some example mappings
example_partitions = [[0, 0], [0, 1], [1, 0], [1, 1]]
print(f"\n📊 Example Thread Mappings:")
for p_idx in example_partitions:
    x_coord = distribution.calculate_index(p_idx)
    print(f"  Thread P{p_idx} → Tensor X{x_coord.to_list()}")
```

## 📏 **Distributed Spans: Spatial Distribution**

Spans define how computation is distributed across space:

```{pyodide}
#| echo: true
#| output: true

print("📏 Distributed Spans Analysis:")

# Get distributed spans
spans = distribution.get_distributed_spans()
print(f"  • Number of spans: {len(spans)}")

for x_idx, span in enumerate(spans):
    print(f"\n🎨 X{x_idx} Span:")
    print(f"  • Partial lengths: {span.partial_lengths}")
    print(f"  • Total elements: {np.prod(span.partial_lengths)}")
    print(f"  • Is static: {span.is_static()}")

print(f"\n🔍 Span Interpretation:")
print(f"  • Each processing element handles a 'span' of data")
print(f"  • Spans define the spatial locality pattern")
print(f"  • Y coordinates index within each span")
print(f"  • Multiple threads can work on different spans")
```

```{pyodide}
#| echo: true
#| output: true

# Create example distributed spans and indices
print("🛠️ Working with Spans and Indices:")

# Create custom spans
custom_span = make_tile_distributed_span([4, 4])  # 4x4 spatial region
print(f"  • Custom span: {custom_span}")

# Create distributed indices
custom_index = make_tile_distributed_index([2, 3])  # Specific position in span
print(f"  • Custom index: {custom_index}")

print(f"\n💡 Practical Usage:")
print(f"  • Spans define work distribution patterns")
print(f"  • Indices specify positions within spans")
print(f"  • Together they enable efficient parallel access")
```

## 🎯 **Y Dimension Processing**

Y coordinates determine which data elements each thread processes:

```{pyodide}
#| echo: true
#| output: true

print("🎯 Y Dimension Processing:")

# Get Y dimension properties
y_lengths = distribution.get_y_vector_lengths()
y_strides = distribution.get_y_vector_strides()

print(f"  • Y dimension count: {distribution.ndim_y}")
print(f"  • Y lengths: {y_lengths}")
print(f"  • Y strides: {y_strides}")

print(f"\n🔄 Y Coordinate Mapping:")
print(f"  • Y₀: Repeat dimension (outer loop)")
print(f"  • Y₁: Vector dimension (SIMD)")
print(f"  • Y₂: Repeat dimension (outer loop)")
print(f"  • Y₃: Vector dimension (SIMD)")

# Example: convert access index to Y coordinates
access_indices = [0, 1, 5, 10, 15]
print(f"\n📊 Access Index → Y Coordinates:")
for access_idx in access_indices:
    y_indices = distribution.get_y_indices_from_distributed_indices(access_idx)
    print(f"  Access[{access_idx}] → Y{y_indices}")
```

## 🏗️ **Creating Tensor Descriptor from Adaptor**

Transform adaptors into descriptors for tensor operations:

```{pyodide}
#| echo: true
#| output: true

print("🏗️ Tensor Descriptor from Adaptor:")

# Create descriptor from the PS_YS→XS adaptor
adaptor = distribution.ps_ys_to_xs_adaptor
element_space_size = 1024  # Example buffer size

descriptor = make_tensor_descriptor_from_adaptor(adaptor, element_space_size)
print(f"  • Created descriptor: {descriptor}")
print(f"  • Descriptor dimensions: {descriptor.get_num_of_dimension()}")
print(f"  • Descriptor lengths: {descriptor.get_lengths()}")
print(f"  • Element space: {descriptor.get_element_space_size()}")

print(f"\n🔄 Adaptor vs Descriptor:")
print(f"  • Adaptor: Handles coordinate transformations")
print(f"  • Descriptor: Adds linearization and memory layout")
print(f"  • Both are needed for complete tensor operations")
```

## 🧪 **Advanced Distribution Operations**

Explore advanced coordinate mapping functionality:

```{pyodide}
#| echo: true
#| output: true

print("🧪 Advanced Coordinate Operations:")

# Test coordinate mapping through adaptor
ps_ys_coords = [0, 1, 2, 1, 0, 3]  # P₀,P₁,Y₀,Y₁,Y₂,Y₃
multi_idx = MultiIndex(len(ps_ys_coords), ps_ys_coords)

# Create adaptor coordinate
adaptor_coord = make_tensor_adaptor_coordinate(
    distribution.ps_ys_to_xs_adaptor, 
    multi_idx
)

print(f"  • Input PS_YS: {ps_ys_coords}")
print(f"  • Top index: {adaptor_coord.get_top_index().to_list()}")
print(f"  • Bottom index: {adaptor_coord.get_bottom_index().to_list()}")
print(f"  • Hidden dimensions: {adaptor_coord.get_hidden_index().to_list()}")

print(f"\n🔍 Coordinate Flow:")
print(f"  • PS_YS input → Hidden transformations → X output")
print(f"  • Hidden dims capture intermediate tile structure")
print(f"  • Final X coords are actual tensor positions")
```

## 🚀 **Real-World Usage Patterns**

Understanding how tile distribution works in practice:

```{pyodide}
#| echo: true
#| output: true

print("🚀 Real-World Usage Patterns:")

print(f"\n1️⃣ Thread Initialization:")
print(f"  • Each GPU thread calls get_partition_index()")
print(f"  • Gets unique P coordinates identifying the thread")
print(f"  • Uses P to calculate starting tensor position")

print(f"\n2️⃣ Data Element Processing:")
print(f"  • Thread iterates through Y coordinate space")
print(f"  • Each Y combination identifies a data element")
print(f"  • PS_YS→XS adaptor maps to final tensor coordinates")

print(f"\n3️⃣ Memory Access:")
print(f"  • YS→D descriptor linearizes Y coordinates")
print(f"  • Enables efficient sequential memory access")
print(f"  • Spans ensure spatial locality")

print(f"\n4️⃣ Parallel Efficiency:")
total_work = np.prod(distribution.get_lengths())
threads_per_block = variables['WarpPerBlock_M'] * variables['WarpPerBlock_N'] * variables['ThreadPerWarp_M'] * variables['ThreadPerWarp_N']
work_per_thread = np.prod(y_lengths)

print(f"  • Total tensor elements: {total_work:,}")
print(f"  • Threads per block: {threads_per_block}")
print(f"  • Work per thread: {work_per_thread}")
print(f"  • Parallel efficiency: {100 * threads_per_block * work_per_thread / total_work:.1f}%")
```

## 🏭 **Factory Functions Summary**

Complete reference for tile distribution creation:

```{pyodide}
#| echo: true
#| output: true

print("🏭 Tile Distribution Factory Functions:")

print(f"\n1️⃣ make_tile_distribution_encoding()")
print(f"   • Creates mathematical encoding from graph structure")
print(f"   • Input: R/H sequences and P/Y mappings")
print(f"   • Output: TileDistributionEncoding")

print(f"\n2️⃣ make_static_tile_distribution()")
print(f"   • Creates runtime distribution from encoding")
print(f"   • Input: TileDistributionEncoding")
print(f"   • Output: TileDistribution with adaptors/descriptors")

print(f"\n3️⃣ make_tile_distribution()")
print(f"   • Creates distribution from custom components")
print(f"   • Input: adaptor, descriptor, encoding")
print(f"   • Output: TileDistribution")

print(f"\n4️⃣ make_tile_distributed_span()")
print(f"   • Creates span from partial lengths")
print(f"   • Input: List of lengths")
print(f"   • Output: TileDistributedSpan")

print(f"\n5️⃣ make_tile_distributed_index()")
print(f"   • Creates index from partial indices")
print(f"   • Input: List of indices")
print(f"   • Output: TileDistributedIndex")

print(f"\n6️⃣ make_tensor_descriptor_from_adaptor()")
print(f"   • Creates descriptor from adaptor")
print(f"   • Input: TensorAdaptor + element_space_size")
print(f"   • Output: TensorDescriptor")
```

## 🎓 **Key Takeaways**

1. **Runtime Bridge**: Tile distribution transforms mathematical encoding into runtime components
2. **Dual Components**: Contains both PS_YS→XS adaptor and YS→D descriptor for complete functionality
3. **Thread Coordination**: Partition indices identify threads and map them to tensor regions
4. **Spatial Locality**: Distributed spans ensure efficient memory access patterns
5. **Y Processing**: Y coordinates define per-thread work and enable vectorized operations
6. **Integration Ready**: Provides the foundation for tile windows, sweep operations, and tensor views

The tile distribution serves as the **runtime coordinator** that makes parallel tensor computation possible with optimal efficiency! 

**Next**: Static Distributed Tensors show how to use tile distributions for actual tensor operations. 