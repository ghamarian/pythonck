---
title: "Tile Distribution Deep Dive"
format: live-html
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

# Tile Distribution Deep Dive

**Prerequisites**: [Tensor Descriptors](tensor-descriptor.qmd), [Tile Distribution Encoding](tile-distribution-encoding.qmd)

This page provides deep explanations of how tile distributions coordinate parallel processing, focusing on the critical functions that enable GPU thread coordination.

## üéØ **Core Concept: Thread-to-Data Mapping**

A **Tile Distribution** maps thread identifiers to tensor data locations:

```
Thread ID ‚Üí P coordinates ‚Üí Y coordinates ‚Üí X coordinates ‚Üí Tensor Data
```

## üîß **make_static_tile_distribution: The Foundation**

This is the most important function - it creates the coordination system:

```{pyodide}
#| echo: true
#| output: true

from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import make_static_tile_distribution

# Create encoding that defines the mathematical distribution
encoding = TileDistributionEncoding(
    rs_lengths=[],                    # No reduction dimensions
    hs_lengthss=[[2, 4], [2, 3]],    # Hierarchical: 2x4 and 2x3 
    ps_to_rhss_major=[[1], [1]],     # P‚ÜíH mapping (major)
    ps_to_rhss_minor=[[0], [0]],     # P‚ÜíH mapping (minor)
    ys_to_rhs_major=[1, 1],          # Y‚ÜíH mapping (major)
    ys_to_rhs_minor=[0, 1]           # Y‚ÜíH mapping (minor)
)

# THIS IS THE KEY FUNCTION - creates the distribution system
distribution = make_static_tile_distribution(encoding)

print(f"Distribution created with:")
print(f"  X dimensions: {distribution.ndim_x}")
print(f"  Y dimensions: {distribution.ndim_y}")  
print(f"  P dimensions: {distribution.ndim_p}")
print("‚úÖ Static tile distribution created")
```

## üó∫Ô∏è **ps_ys_to_xs_adaptor: The Critical Adapter**

This adaptor converts parallel coordinates (P,Y) to tensor coordinates (X):

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_coordinate import MultiIndex

# Get the critical adaptor that maps (P,Y) ‚Üí X
ps_ys_to_xs_adaptor = distribution.ps_ys_to_xs_adaptor

print(f"Adaptor details:")
print(f"  Input dimensions (P+Y): {ps_ys_to_xs_adaptor.get_num_of_upper_dimension()}")
print(f"  Output dimensions (X): {ps_ys_to_xs_adaptor.get_num_of_lower_dimension()}")
print(f"  Number of transforms: {ps_ys_to_xs_adaptor.get_num_of_transform()}")

# Example: Convert thread coordinates to tensor coordinates
p_coords = MultiIndex(2, [0, 1])  # Thread partition coordinates
y_coords = MultiIndex(2, [1, 0])  # Local thread coordinates

# Combine P and Y coordinates
combined_coords = MultiIndex(4, p_coords.to_list() + y_coords.to_list())
print(f"Combined P,Y coordinates: {combined_coords.to_list()}")

# Transform to X (tensor) coordinates
x_coords = ps_ys_to_xs_adaptor.calculate_lower_index(combined_coords)
print(f"Resulting X coordinates: {x_coords.to_list()}")
print("‚úÖ P,Y ‚Üí X coordinate transformation demonstrated")
```

## üìã **d_to_ys_descriptor: Local Distribution Mapping**

This descriptor maps distribution indices to local Y coordinates:

```{pyodide}
#| echo: true
#| output: true

# Get the descriptor that maps D ‚Üí Y coordinates
d_to_ys_descriptor = distribution.d_to_ys_descriptor

print(f"D‚ÜíY Descriptor:")
print(f"  Input dimensions (D): {len(d_to_ys_descriptor.get_lengths())}")
print(f"  Output space size: {d_to_ys_descriptor.get_element_space_size()}")
print(f"  Y dimension lengths: {d_to_ys_descriptor.get_lengths()}")

# Example: Convert distribution index to Y coordinates
from pytensor.tensor_coordinate import make_tensor_coordinate

# Create a distribution coordinate
d_coord = MultiIndex(2, [1, 2])  # Distribution index [1, 2]
coord = make_tensor_coordinate(d_to_ys_descriptor, d_coord)

print(f"Distribution coordinate [1, 2]:")
print(f"  Maps to Y coordinates: {coord.get_index().to_list()}")
print(f"  Linear offset: {coord.get_offset()}")
print("‚úÖ D ‚Üí Y coordinate mapping demonstrated")
```

## üîÑ **Complete Thread Mapping Pipeline**

Here's how a thread ID becomes tensor access:

```{pyodide}
#| echo: true
#| output: true

def complete_thread_mapping(distribution, thread_p, thread_d):
    """Demonstrate complete thread ‚Üí tensor coordinate mapping"""
    
    print(f"üßµ Thread mapping for P={thread_p}, D={thread_d}")
    
    # Step 1: D coordinates ‚Üí Y coordinates
    d_coord = MultiIndex(len(thread_d), thread_d)
    d_tensor_coord = make_tensor_coordinate(distribution.d_to_ys_descriptor, d_coord)
    y_coords = d_tensor_coord.get_index()
    
    print(f"  Step 1: D{thread_d} ‚Üí Y{y_coords.to_list()}")
    
    # Step 2: Combine P and Y coordinates
    p_coord = MultiIndex(len(thread_p), thread_p)
    combined = MultiIndex(
        len(thread_p) + len(y_coords.to_list()),
        thread_p + y_coords.to_list()
    )
    
    print(f"  Step 2: P{thread_p} + Y{y_coords.to_list()} ‚Üí PY{combined.to_list()}")
    
    # Step 3: (P,Y) coordinates ‚Üí X coordinates
    x_coords = distribution.ps_ys_to_xs_adaptor.calculate_lower_index(combined)
    
    print(f"  Step 3: PY{combined.to_list()} ‚Üí X{x_coords.to_list()}")
    print(f"  üéØ Final tensor coordinates: {x_coords.to_list()}")
    
    return x_coords

# Test complete mapping for different threads
print("=== Thread Mapping Examples ===")
complete_thread_mapping(distribution, [0, 0], [0, 0])
print()
complete_thread_mapping(distribution, [1, 0], [1, 1])
print()
complete_thread_mapping(distribution, [0, 1], [2, 0])
print("‚úÖ Complete thread mapping pipeline demonstrated")
```

## üß© **Understanding the Coordinate Spaces**

```{pyodide}
#| echo: true
#| output: true

def analyze_coordinate_spaces(distribution):
    """Analyze all coordinate spaces in the distribution"""
    
    print("üìä Coordinate Space Analysis:")
    print(f"  P (Partition) space: {distribution.ndim_p} dimensions")
    print(f"  Y (Local thread) space: {distribution.ndim_y} dimensions")  
    print(f"  X (Tensor) space: {distribution.ndim_x} dimensions")
    
    # Get distributed spans (how data is partitioned)
    spans = distribution.get_distributed_spans()
    print(f"  Number of distributed spans: {len(spans)}")
    
    for i, span in enumerate(spans):
        print(f"    Span {i}: {span}")
    
    print("‚úÖ Coordinate space analysis complete")

analyze_coordinate_spaces(distribution)
```

## üéØ **Integration with Real Applications**

See how these concepts work in practice:

```{pyodide}
#| echo: true
#| output: true

# Simulate how tile distributions are used in real kernels
def simulate_kernel_execution(distribution, data_shape):
    """Simulate how GPU threads use tile distribution"""
    
    print(f"üöÄ Simulating kernel execution on {data_shape} tensor:")
    
    # Each thread block gets P coordinates
    # Each thread in block gets D coordinates
    
    threads_per_block = [2, 2]  # 2x2 threads per block
    blocks = [2, 2]            # 2x2 blocks
    
    print(f"  Grid: {blocks} blocks, {threads_per_block} threads/block")
    
    active_threads = 0
    for block_p0 in range(blocks[0]):
        for block_p1 in range(blocks[1]):
            for thread_d0 in range(threads_per_block[0]):
                for thread_d1 in range(threads_per_block[1]):
                    
                    # Get tensor coordinates for this thread
                    x_coords = complete_thread_mapping(
                        distribution, 
                        [block_p0, block_p1], 
                        [thread_d0, thread_d1]
                    )
                    
                    # Check if coordinates are within tensor bounds
                    if all(x < data_shape[i] for i, x in enumerate(x_coords.to_list())):
                        active_threads += 1
                        
                        # This thread would process data at x_coords
                        print(f"    Thread B({block_p0},{block_p1})T({thread_d0},{thread_d1}) ‚Üí Tensor{x_coords.to_list()}")
    
    print(f"  Total active threads: {active_threads}")
    print("‚úÖ Kernel execution simulation complete")

# Test with realistic tensor size
simulate_kernel_execution(distribution, [8, 12])
```

## üîó **Interactive Applications**

Explore these concepts with our full applications:

- **[Tensor Transform App](../../tensor_transform_app.py)** - Visualize how descriptors and adaptors work together
- **[Main App](../../app.py)** - Interactive tile distribution exploration
- **[Thread Mapping Example](../../tile_distr_thread_mapping.py)** - Complete real-world example

## üßÆ **Mathematical Foundations**

The key insight is the **coordinate space hierarchy**:

```
GPU Thread Grid ‚Üí P coordinates (blocks)
    ‚Üì
GPU Thread Block ‚Üí D coordinates (threads)  
    ‚Üì
d_to_ys_descriptor ‚Üí Y coordinates (local)
    ‚Üì
ps_ys_to_xs_adaptor ‚Üí X coordinates (tensor)
    ‚Üì
Tensor Descriptor ‚Üí Memory addresses
```

## üîç **Why This Matters**

Understanding these components is essential for:

1. **Tile Windows** - Use these coordinates to determine what data to load
2. **Sweep Operations** - Navigate through distributed data systematically
3. **GPU Kernels** - Map thread IDs to tensor data efficiently
4. **Memory Coalescing** - Ensure efficient memory access patterns

## ‚úÖ **Next Steps**

Now you're ready for:
- [**Tile Windows Deep Dive**](tile-window.qmd) - See how distributions integrate with data loading
- [**Complete Thread Mapping**](thread-mapping.qmd) - The full GPU coordination pipeline

---

*These coordinate mappings are the mathematical foundation that makes efficient GPU tensor processing possible!* 