---
title: "Tensor Adaptors - Chaining Transformations"
format: live-html
---

## Overview

While individual transforms are powerful, TensorAdaptors let us chain multiple transforms together to create complex coordinate transformations. Think of adaptors as transformation pipelines that can reshape, reorder, and restructure tensors in sophisticated ways.

TensorAdaptors are the bridge between individual transforms and the high-level tensor operations you'll use in real applications.

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")
```

## TensorAdaptor Basics

Let's start by understanding what a TensorAdaptor is and how it works:

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 40%;">
graph TB
    subgraph "Adaptor Composition"
        subgraph "Single Transform"
            T1["Transform<br/>(e.g., Transpose)"]
            I1["Input Coords<br/>[0,1,2]"]
            O1["Output Coords<br/>[2,0,1]"]
        end
        
        subgraph "Chained Transforms"
            T2A["Transform A<br/>(e.g., Merge)"]
            T2B["Transform B<br/>(e.g., Pad)"]
            I2["Input<br/>2D"]
            M2["Intermediate<br/>1D"]
            O2["Output<br/>1D Padded"]
        end
        
        subgraph "Complex Pipeline"
            T3A["Transpose"]
            T3B["Unmerge"]
            T3C["Pad"]
            I3["Input"]
            O3["Output"]
        end
    end
    
    I1 --> T1
    T1 --> O1
    
    I2 --> T2A
    T2A --> M2
    M2 --> T2B
    T2B --> O2
    
    I3 --> T3A
    T3A --> T3B
    T3B --> T3C
    T3C --> O3
    
    style T1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style T2A fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style T2B fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style T3A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style T3B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style T3C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

# Import required modules
from pytensor.tensor_adaptor import (
    make_single_stage_tensor_adaptor,
    transform_tensor_adaptor,
    chain_tensor_adaptors,
    chain_tensor_adaptors_multi,
    make_identity_adaptor,
    make_transpose_adaptor
)
from pytensor.tensor_descriptor import (
    TensorAdaptor,
    PassThroughTransform,
    PadTransform,
    MergeTransform,
    ReplicateTransform,
    EmbedTransform,
    UnmergeTransform,
    transform_tensor_descriptor,
    make_naive_tensor_descriptor_packed,
    make_merge_transform,
    make_pass_through_transform,
    make_unmerge_transform,
    make_tuple,
    number,
    sequence
)
from pytensor.tensor_coordinate import MultiIndex
import numpy as np

print("üîó TensorAdaptor Overview")
print("--" * 40)
print("  TensorAdaptor chains multiple transforms together")
print("  Each adaptor has:")
print("    ‚Ä¢ transforms: List of individual transforms")
print("    ‚Ä¢ lower_dimension_hidden_idss: How transforms connect")
print("    ‚Ä¢ upper_dimension_hidden_idss: Hidden dimension mappings")
print("    ‚Ä¢ bottom_dimension_hidden_ids: Input dimensions")
print("    ‚Ä¢ top_dimension_hidden_ids: Output dimensions")
```

The most important method of a TensorAdaptor is `calculate_bottom_index`, which calculates the lower index from the upper index. It achives this by applying the transforms in reverse order and calling `calculate_lower_index` on each transform. 

Let's go over some of the utility functions for creating tensor adaptors and see how they work in real life. We start with one of the simplest one.

## Transpose Adaptor: Dimension Reordering

The transpose adaptor reorders tensor dimensions according to a permutation pattern.

```{pyodide}
#| echo: true
#| output: true

print("2Ô∏è‚É£ Transpose Adaptor")
print("--" * 40)

# Create transpose adaptor: [0, 1, 2] ‚Üí [2, 0, 1]
transpose_adaptor = make_transpose_adaptor(3, [2, 0, 1])

print(f"  Permutation: [2, 0, 1] (dimension 0‚Üí2, 1‚Üí0, 2‚Üí1)")
print(f"  Number of transforms: {transpose_adaptor.get_num_of_transform()}")
print(f"  Bottom dimensions: {transpose_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {transpose_adaptor.get_num_of_top_dimension()}")

# Test coordinate transformation
test_coords = [[0, 1, 2], [1, 0, 2], [2, 1, 0]]
print("\n  Transpose transformation test:")
for coord_list in test_coords:
    top_coord = MultiIndex(3, coord_list)
    bottom_coord = transpose_adaptor.calculate_bottom_index(top_coord)
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()}")
```

### C++ Implementation

```cpp
// From composable_kernel - create transpose adaptor
// Transpose adaptor example: [0, 1, 2] ‚Üí [2, 0, 1]
auto transpose_adaptor = make_identity_tensor_adaptor<3>();  // Start with identity

// Apply transpose using transform_tensor_adaptor
// In CK, transpose is typically done through tensor descriptor transformations
auto transposed_desc = transform_tensor_descriptor(
    original_desc,
    make_tuple(make_pass_through_transform(original_desc.get_length(2)),
               make_pass_through_transform(original_desc.get_length(0)),
               make_pass_through_transform(original_desc.get_length(1))),
    make_tuple(sequence<2>{}, sequence<0>{}, sequence<1>{}),  // old dims
    make_tuple(sequence<0>{}, sequence<1>{}, sequence<2>{})   // new dims
);

// Alternative: Direct coordinate transformation
multi_index<3> top_coord{0, 1, 2};
// After transpose [2, 0, 1]: coord becomes [2, 0, 1]
```

## Single-Stage Adaptors: Custom Transform Chains

You can create custom adaptors by specifying exactly which transforms to use and how they connect, the API for that is called `make_single_stage_tensor_adaptor`:

```{pyodide}
#| echo: true
#| output: true

print("3Ô∏è‚É£ Single-Stage Custom Adaptor")
print("--" * 40)

# Create adaptor that splits 1D coordinates to 2D
# Note: MergeTransform has 1D upper (top) and 2D lower (bottom)
merge_adaptor = make_single_stage_tensor_adaptor(
    transforms=[MergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0, 1]],  # Bottom: 2D dimensions 0 and 1
    upper_dimension_new_top_idss=[[0]]       # Top: 1D dimension 0 (merged)
)

print(f"  Transform: MergeTransform([2, 3]) - splits 1D to 2D")
print(f"  Bottom dimensions: {merge_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {merge_adaptor.get_num_of_top_dimension()}")

# Test merge transformation: 1D top ‚Üí 2D bottom
test_indices = [0, 2, 3, 5]
print("\n  Merge transformation test (1D ‚Üí 2D):")
for idx in test_indices:
    top_coord = MultiIndex(1, [idx])  # 1D top coordinate
    bottom_coord = merge_adaptor.calculate_bottom_index(top_coord)
    expected_row = idx // 3
    expected_col = idx % 3
    print(f"    [{idx}] ‚Üí {bottom_coord.to_list()} (expected: [{expected_row}, {expected_col}])")
```

### C++ Implementation

```cpp
// From composable_kernel - create single-stage tensor adaptor
// Note: In CK, adaptors are typically created through tensor descriptors

// Create a descriptor that merges 2x3 dimensions into single dimension
auto base_desc = make_naive_tensor_descriptor_packed(make_tuple(2, 3));

// Apply merge transform
auto merged_desc = transform_tensor_descriptor(
    base_desc,
    make_tuple(make_merge_transform(make_tuple(2, 3))),
    make_tuple(sequence<0, 1>{}),  // merge dims 0,1
    make_tuple(sequence<0>{})      // to single dim 0
);

// The adaptor is embedded in the descriptor
// To use it:
multi_index<1> top_coord{5};  // 1D coordinate
// This internally calculates: row = 5/3 = 1, col = 5%3 = 2
```

Now that we saw how we can create an adaptor, let's see how we can combine a few of them together.

## Chaining Adaptors: Building Complex Transformations

The real power comes from chaining multiple adaptors together to create sophisticated transformations. Below we try some trivial example of combining merge and unmerge just to show how these transformations combine.

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 50%;">
graph LR
    subgraph "Adaptor Chaining Flow"
        subgraph "Adaptor 1"
            A1I["Bottom Dims<br/>[0,1]"]
            A1T["Transform:<br/>Merge[2,3]"]
            A1O["Top Dims<br/>[0]"]
        end
        
        subgraph "Adaptor 2"
            A2I["Bottom Dims<br/>[0]"]
            A2T["Transform:<br/>Unmerge[2,3]"]
            A2O["Top Dims<br/>[0,1]"]
        end
        
        subgraph "Chained Result"
            CI["Input 2D<br/>Bottom[0,1]"]
            CO["Output 2D<br/>Top[0,1]"]
        end
    end
    
    A1I --> A1T
    A1T --> A1O
    A1O --> A2I
    A2I --> A2T
    A2T --> A2O
    
    CI --> A1I
    A2O --> CO
    
    style A1T fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style A2T fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style CI fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style CO fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

print("4Ô∏è‚É£ Chaining Adaptors")
print("--" * 40)

# Create first adaptor: 1D ‚Üí 2D (merge splits)
adaptor_merge = make_single_stage_tensor_adaptor(
    transforms=[MergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0, 1]],
    upper_dimension_new_top_idss=[[0]]
)

# Create second adaptor: 2D ‚Üí 1D (unmerge combines)
adaptor_unmerge = make_single_stage_tensor_adaptor(
    transforms=[UnmergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0]],
    upper_dimension_new_top_idss=[[0, 1]]
)

# Chain them together (should be identity overall)
chained_adaptor = chain_tensor_adaptors(adaptor_merge, adaptor_unmerge)

print(f"  Chain: 1D ‚Üí 2D ‚Üí 1D (but actually results in 2D ‚Üí 2D)")
print(f"  Number of transforms: {chained_adaptor.get_num_of_transform()}")
print(f"  Bottom dimensions: {chained_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {chained_adaptor.get_num_of_top_dimension()}")

# Test the chained transformation (should be identity)
test_coords = [[0, 0], [0, 2], [1, 0], [1, 2]]
print("\n  Chained transformation test (should be identity):")
for coord_list in test_coords:
    top_coord = MultiIndex(2, coord_list)  # 2D input
    bottom_coord = chained_adaptor.calculate_bottom_index(top_coord)
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()}")
```

### C++ Implementation

```cpp
// From composable_kernel - chaining adaptors through descriptors

// Start with a 2D descriptor
auto desc1 = make_naive_tensor_descriptor_packed(make_tuple(2, 3));

// First transformation: merge 2D to 1D
auto merged_desc = transform_tensor_descriptor(
    desc1,
    make_tuple(make_merge_transform(make_tuple(2, 3))),
    make_tuple(sequence<0, 1>{}),  // merge dims 0,1
    make_tuple(sequence<0>{})      // to dim 0
);

// Second transformation: unmerge 1D back to 2D
auto final_desc = transform_tensor_descriptor(
    merged_desc,
    make_tuple(make_unmerge_transform(make_tuple(2, 3))),
    make_tuple(sequence<0>{}),     // from dim 0
    make_tuple(sequence<0, 1>{})   // to dims 0,1
);

// The chained transformation is embedded in final_desc
// Result should be identity transformation
```

## Transform Addition: Extending Existing Adaptors

You can add new transforms to existing adaptors using `transform_tensor_adaptor`. **Important**: The `new_upper_dimension_new_top_idss` parameter controls the **final output dimensions** of the adaptor.

```{pyodide}
#| echo: true
#| output: true

print("7Ô∏è‚É£ Practical Example: Matrix Transpose + Padding")
print("--" * 40)

# Create a matrix transpose adaptor
matrix_transpose = make_transpose_adaptor(2, [1, 0])

padded_transpose_2d = transform_tensor_adaptor(
    old_adaptor=matrix_transpose,
    new_transforms=[
        PadTransform(lower_length=4, left_pad=1, right_pad=1),  # Pad first dimension
        PassThroughTransform(3)                                 # Keep second dimension
    ],
    new_lower_dimension_old_top_idss=[[0], [1]],  # Apply to both dimensions
    new_upper_dimension_new_top_idss=[[0], [1]]   # Keep both in final output
)

print(f"  Operation: 2D transpose ‚Üí 2D (pad first dim, pass second)")
print(f"  Total transforms: {padded_transpose_2d.get_num_of_transform()}")
print(f"  Bottom dimensions: {padded_transpose_2d.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {padded_transpose_2d.get_num_of_top_dimension()}")

# Test with a 3x4 matrix conceptually
test_coords = [[0, 0], [0, 2], [3, 0], [3, 2]]
print("\n  Matrix transpose with padding test:")
for coord_list in test_coords:
    top_coord = MultiIndex(2, coord_list)
    bottom_coord = padded_transpose_2d.calculate_bottom_index(top_coord)
    # Manual verification: [i,j] ‚Üí transpose ‚Üí [j,i] ‚Üí pad first ‚Üí [j-1,i]
    expected = [coord_list[0], coord_list[1] - 1]
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()} (expected: {expected})")

print("\n  ‚ö†Ô∏è  Common mistake: Using [[0]] for new_upper_dimension_new_top_idss")
print("     This creates 1D output, losing the second dimension!")
```

### C++ Implementation

```cpp
// From composable_kernel - transform tensor adaptor pattern

// Start with transposed descriptor
auto base_desc = make_naive_tensor_descriptor(
    make_tuple(3, 4),
    make_tuple(1, 3)   // transposed strides
);

// Add padding to both dimensions
auto padded_desc = transform_tensor_descriptor(
    base_desc,
    make_tuple(make_pad_transform(3, 1, 1),   // pad dim 0: 3 ‚Üí 5
               make_pad_transform(4, 0, 0)),   // keep dim 1: 4 ‚Üí 4
    make_tuple(sequence<0>{}, sequence<1>{}),  // input dims
    make_tuple(sequence<0>{}, sequence<1>{})   // output dims (keep 2D)
);

// Access pattern
multi_index<2> padded_coord{1, 2};  // In padded space
// Internally calculates: unpadded = [1-1, 2] = [0, 2]
// Then applies transpose strides
```

## Advanced C++ Patterns

### Complex Nested Transforms in C++

```cpp
// From composable_kernel - complex nested transform patterns

// Example: 4D tensor with complex transformations
// Shape: [A, B, C, D] with various transforms

// 1. Create base descriptor
auto base_desc = make_naive_tensor_descriptor_packed(
    make_tuple(A, B, C, D)
);

// 2. Apply multiple transformations
// First: merge first 3 dimensions
auto step1_desc = transform_tensor_descriptor(
    base_desc,
    make_tuple(make_merge_transform(make_tuple(A, B, C)),
               make_pass_through_transform(D)),
    make_tuple(sequence<0, 1, 2>{}, sequence<3>{}),  // input mapping
    make_tuple(sequence<0>{}, sequence<1>{})         // output: 2D
);

// 3. Then unmerge back but with different grouping
auto step2_desc = transform_tensor_descriptor(
    step1_desc,
    make_tuple(make_unmerge_transform(make_tuple(A*B, C)),
               make_pass_through_transform(D)),
    make_tuple(sequence<0>{}, sequence<1>{}),        // from 2D
    make_tuple(sequence<0, 1>{}, sequence<2>{})      // to 3D
);

// The adaptor chain is embedded in the descriptors
// CK optimizes these at compile time
```

### GPU Memory Layout Example

```cpp
// From composable_kernel - typical GPU block descriptor pattern

// Create descriptor for thread block tile: 64x64
// With 8x8 vector loads per thread
constexpr auto BlockM = 64;
constexpr auto BlockN = 64;
constexpr auto VectorM = 8;
constexpr auto VectorN = 8;

// Thread arrangement: 8x8 threads
constexpr auto ThreadM = BlockM / VectorM;  // 8
constexpr auto ThreadN = BlockN / VectorN;  // 8

// Create block descriptor with proper layout
auto block_desc = transform_tensor_descriptor(
    make_naive_tensor_descriptor_packed(
        make_tuple(number<BlockM>{}, number<BlockN>{})
    ),
    make_tuple(
        make_unmerge_transform(make_tuple(
            number<ThreadM>{}, number<VectorM>{}
        )),
        make_unmerge_transform(make_tuple(
            number<ThreadN>{}, number<VectorN>{}
        ))
    ),
    make_tuple(sequence<0>{}, sequence<1>{}),           // from 2D
    make_tuple(sequence<0, 2>{}, sequence<1, 3>{})     // to 4D: [TM,TN,VM,VN]
);

// This creates the layout:
// - Dimension 0,1: Thread indices
// - Dimension 2,3: Vector indices within thread
// Enables coalesced memory access on GPU
```

## Summary

TensorAdaptors are the coordination layer that makes complex tensor operations possible:

- **Identity Adaptor**: Starting point for building transformations
- **Transpose Adaptor**: Dimension reordering with permutation patterns
- **Single-Stage Adaptors**: Custom transform chains with precise control
- **Chained Adaptors**: Complex multi-stage transformation pipelines
- **Transform Addition**: Extending existing adaptors with new transforms
- **Advanced Examples**: Complex nested transforms with flattening behavior
- **GPU Block Descriptors**: Real-world GPU memory layout patterns
- **C++ Equivalents**: **True working equivalent** of complex nested C++ transforms

Key concepts:

- **Bottom/Top Dimensions**: Input and output coordinate spaces
- **Hidden Dimensions**: Internal coordinate mappings between transforms
- **Transform Chains**: Sequential application of multiple transforms
- **Coordinate Transformation**: Bidirectional mapping between coordinate spaces
- **Nested Transforms**: Complex multi-level transformation hierarchies

### Breakthrough Discovery

We successfully created the **true C++ equivalent** of complex nested transforms:

```python
# C++ nested transform equivalent
cpp_equivalent = make_single_stage_tensor_adaptor(
    transforms=[
        UnmergeTransform([A, B, C]),  # Converts 3D (A,B,C) to 1D linear  
        PassThroughTransform(D)       # Passes through D dimension
    ],
    lower_dimension_old_top_idss=[[0], [1]],          # Transform inputs
    upper_dimension_new_top_idss=[[0, 1, 2], [3]]     # Transform outputs
)
```

**Key insights:**

- **Transform direction**: Names refer to lower‚Üíhigher, but `calculate_lower_index()` goes higher‚Üílower
- **UnmergeTransform**: Converts multi-D to linear when used with `calculate_lower_index()`
- **Parameter mapping**: Controls the coordinate flow between dimensions
- **Mathematical equivalence**: Exact same results as C++ nested structure

TensorAdaptors bridge the gap between low-level transforms and high-level tensor operations, providing the flexibility to create sophisticated data layouts and access patterns that are essential for efficient GPU computing.

### Key C++ Patterns in Composable Kernel

1. **Descriptor-Based Adaptors**: In CK, adaptors are typically embedded within tensor descriptors rather than created separately
2. **Compile-Time Optimization**: All transformations are resolved at compile time for zero overhead
3. **Type Safety**: Template metaprogramming ensures coordinate transformations are type-safe
4. **GPU Optimization**: Transform chains are designed for efficient GPU memory access patterns

### Common C++ Transform Chains

```cpp
// Padding for convolution
auto padded = transform_tensor_descriptor(
    input, 
    make_tuple(make_pad_transform(H, pad_h, pad_h),
               make_pad_transform(W, pad_w, pad_w)),
    make_tuple(sequence<0>{}, sequence<1>{}),
    make_tuple(sequence<0>{}, sequence<1>{})
);

// Dimension merging for GEMM
auto merged = transform_tensor_descriptor(
    input,
    make_tuple(make_merge_transform(make_tuple(M, K))),
    make_tuple(sequence<0, 1>{}),
    make_tuple(sequence<0>{})
);

// Broadcasting for elementwise ops
auto broadcast = transform_tensor_descriptor(
    scalar,
    make_tuple(make_replicate_transform(make_tuple(M, N))),
    make_tuple(sequence<>{}),
    make_tuple(sequence<0, 1>{})
);
```

Next, we'll see how TensorAdaptors are combined with element space information to create complete **TensorDescriptors**.
