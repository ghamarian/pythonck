---
title: "Tensor Views - Multi-Dimensional Structure"
format: 
  live-html:
    mermaid:
      theme: default
---

## Overview

TensorView adds multi-dimensional structure to raw memory. While BufferView provides linear access, TensorView enables coordinate-based access to matrices, tensors, and higher-dimensional data structures.

### TensorView Architecture

```{=html}
<div class="mermaid"  style="margin: 0 auto; display: block; width: 60%;">
graph TB
    subgraph "Memory Foundation"
        Memory["Flat Memory Array<br/>0 1 2 3 4 5 6 7 8 9 10 11"]
    end
    
    subgraph "Access Layer"
        BufferView["BufferView<br/>Linear Memory Access"]
        Descriptor["TensorDescriptor<br/>Shape & Stride Info"]
    end
    
    subgraph "Tensor Layer"
        TensorView["TensorView<br/>Multi-dimensional Access"]
    end
    
    subgraph "Logical View"
        Matrix["2D Matrix View<br/>[3×4]<br/>[[0,1,2,3]<br/>[4,5,6,7]<br/>[8,9,10,11]]"]
    end
    
    Memory --> BufferView
    Memory --> Descriptor
    BufferView --> TensorView
    Descriptor --> TensorView
    TensorView --> Matrix
    
    style Memory fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style BufferView fill:#dbeafe,stroke:#3b82f6,stroke-width:2px
    style Descriptor fill:#fed7aa,stroke:#f59e0b,stroke-width:2px
    style TensorView fill:#fce7f3,stroke:#ec4899,stroke-width:2px
    style Matrix fill:#e9d5ff,stroke:#9333ea,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.2.0-py3-none-any.whl")
```

## Creation and Basic Usage

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.buffer_view import make_buffer_view, AddressSpaceEnum
from pytensor.tensor_view import make_naive_tensor_view_packed, make_tensor_view
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed

# Create data
data = np.arange(12, dtype=np.float32)

# Method 1: Using naive tensor view with packed layout
tensor_view = make_naive_tensor_view_packed(data, lengths=[3, 4])
print("Number of dimensions:", tensor_view.get_num_of_dimension())
print("Element space size:", tensor_view.get_tensor_descriptor().get_element_space_size())

# Access elements
print("\nData as 3x4 matrix:")
for i in range(3):
    for j in range(4):
        print(f"{tensor_view.get_element([i, j]):4.0f}", end=" ")
    print()
```

### C++ Implementation Reference

**File**: `include/ck_tile/core/tensor/tensor_view.hpp`

<details>
<summary>Click to show C++ code</summary>

```cpp
#include <ck_tile/core/tensor/tensor_view.hpp>

__device__ void example_tensor_creation()
{
    // Create a 3x4 matrix in global memory
    float data[12] = {0,1,2,3,4,5,6,7,8,9,10,11};
    
    // Method 1: Create buffer and descriptor separately
    auto buffer = make_buffer_view<address_space_enum::global>(data, 12);
    auto desc = make_tensor_descriptor(make_tuple(3, 4), make_tuple(4, 1));
    
    // Create tensor view
    auto tensor = make_tensor_view<address_space_enum::global>(buffer, desc);
    
    // Method 2: Use convenience function
    auto tensor2 = make_naive_tensor_view<address_space_enum::global>(
        data,           // pointer
        make_tuple(3, 4),  // shape
        make_tuple(4, 1)   // strides
    );
    
    // Access element at (1, 2)
    float value = tensor.get_element(make_tuple(1, 2));  // Returns 6
    
    // Update element
    tensor.set_element(make_tuple(2, 1), 99.0f);
}
```

</details>

## Flow & Layouts: From Coordinates to Memory

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "User Input"
        Coord["Coordinate<br/>(1, 2)"]
    end
    
    subgraph "TensorView Processing"
        Shape["Shape Check<br/>row < 3?<br/>col < 4?"]
        Stride["Apply Strides<br/>offset = 1×4 + 2×1"]
        Buffer["BufferView Access<br/>buffer[6]"]
    end
    
    subgraph "Result"
        Value["Value: 6"]
    end
    
    Coord --> Shape
    Shape -->|Valid| Stride
    Stride --> Buffer
    Buffer --> Value
    
    style Coord fill:#e0e7ff,stroke:#4338ca,stroke-width:2px
    style Shape fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    style Stride fill:#dcfce7,stroke:#10b981,stroke-width:2px
    style Buffer fill:#dbeafe,stroke:#3b82f6,stroke-width:2px
    style Value fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

```{=html}
<div class="mermaid">
graph TB
    subgraph "Row-Major Layout (C-style)"
        RM["Memory: [0,1,2,3,4,5,6,7,8,9,10,11]<br/>Shape: (3,4)<br/>Strides: (4,1)"]
        RMMatrix["[[0, 1, 2, 3]<br/> [4, 5, 6, 7]<br/> [8, 9, 10, 11]]"]
        RM --> RMMatrix
    end
    
    subgraph "Column-Major Layout (Fortran-style)"
        CM["Memory: [0,3,6,9,1,4,7,10,2,5,8,11]<br/>Shape: (3,4)<br/>Strides: (1,3)"]
        CMMatrix["[[0, 1, 2, 3]<br/> [4, 5, 6, 7]<br/> [8, 9, 10, 11]]"]
        CM --> CMMatrix
    end
    
    subgraph "Custom Stride (Transposed View)"
        TV["Memory: [0,1,2,3,4,5,6,7,8,9,10,11]<br/>Shape: (4,3)<br/>Strides: (1,4)"]
        TVMatrix["[[0, 4, 8]<br/> [1, 5, 9]<br/> [2, 6, 10]<br/> [3, 7, 11]]"]
        TV --> TVMatrix
    end
    
    style RM fill:#e0f2fe,stroke:#0284c7,stroke-width:2px
    style CM fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    style TV fill:#f3e8ff,stroke:#9333ea,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

# Demonstrate different memory layouts
data = np.arange(12, dtype=np.float32)

# Row-major (C-style) - default
row_major = make_naive_tensor_view_packed(data, lengths=[3, 4])
print("Row-major layout:")
for i in range(3):
    for j in range(4):
        print(f"{row_major.get_element([i, j]):4.0f}", end=" ")
    print()

# Create transposed view using strides (no data copy!)
from pytensor.tensor_view import make_naive_tensor_view
transposed = make_naive_tensor_view(data, lengths=[4, 3], strides=[1, 4])
print("\nTransposed view (same memory):")
for i in range(4):
    for j in range(3):
        print(f"{transposed.get_element([i, j]):4.0f}", end=" ")
    print()

# Verify it's the same memory
print("\nElement (1,2) in original:", row_major.get_element([1, 2]))
print("Element (2,1) in transposed:", transposed.get_element([2, 1]))
print("Both access the same memory location!")
```

## Coordinate Mapping

```{=html}
<div class="mermaid">
graph LR
    subgraph "2D Coordinate"
        C2D["(row=2, col=1)"]
    end
    
    subgraph "Offset Calculation"
        Calc["offset = row × row_stride + col × col_stride<br/>offset = 2 × 4 + 1 × 1 = 9"]
    end
    
    subgraph "1D Memory"
        Mem["[0,1,2,3,4,5,6,7,8,<mark>9</mark>,10,11]"]
    end
    
    C2D --> Calc
    Calc --> Mem
    
    style C2D fill:#e0e7ff,stroke:#4338ca,stroke-width:2px
    style Calc fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    style Mem fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

# Interactive coordinate mapping
tensor = make_naive_tensor_view_packed(np.arange(12, dtype=np.float32), lengths=[3, 4])

# Function to show mapping
def show_coordinate_mapping(row, col):
    # For packed layout, calculate offset manually
    # Row-major: offset = row * num_cols + col
    offset = row * 4 + col
    value = tensor.get_element([row, col])
    print(f"Coordinate ({row}, {col}) -> Offset {offset} -> Value {value}")

# Examples
show_coordinate_mapping(0, 0)  # Top-left
show_coordinate_mapping(1, 2)  # Middle
show_coordinate_mapping(2, 3)  # Bottom-right

# Demonstrate bounds checking
try:
    tensor.get_element([3, 0])  # Out of bounds
except Exception as e:
    print(f"\nBounds checking works: Index out of bounds")
```

## Advanced Operations

### Slicing and Subviews

```{pyodide}
#| echo: true
#| output: true

# Create a larger tensor
data = np.arange(20, dtype=np.float32)
tensor = make_naive_tensor_view_packed(data, lengths=[4, 5])

print("Original 4x5 tensor:")
for i in range(4):
    for j in range(5):
        print(f"{tensor.get_element([i, j]):4.0f}", end=" ")
    print()

# Create a 2x3 subview starting at (1, 1)
# In C++ this would be done with transforms
print("\n2x3 subview starting at (1, 1):")
for i in range(2):
    for j in range(3):
        value = tensor.get_element([i + 1, j + 1])
        print(f"{value:4.0f}", end=" ")
    print()
```

### Broadcasting and Padding

```{pyodide}
#| echo: true
#| output: true

# Demonstrate padding (conceptual - actual implementation uses PadTransform)
small_data = np.array([1, 2, 3, 4], dtype=np.float32)
small_tensor = make_naive_tensor_view_packed(small_data, lengths=[2, 2])

print("Original 2x2 tensor:")
for i in range(2):
    for j in range(2):
        print(f"{small_tensor.get_element([i, j]):4.0f}", end=" ")
    print()

# Access with padding (returns 0 for out-of-bounds)
print("\nAccessing with conceptual padding:")
for i in range(-1, 3):
    for j in range(-1, 3):
        if 0 <= i < 2 and 0 <= j < 2:
            value = small_tensor.get_element([i, j])
        else:
            value = 0  # Padding value
        print(f"{value:4.0f}", end=" ")
    print()
```

## TensorView vs BufferView

```{=html}
<div class="mermaid">
graph TB
    subgraph "BufferView"
        BV1["Linear indexing only"]
        BV2["buffer[5]"]
        BV3["No shape information"]
        BV4["Direct memory access"]
    end
    
    subgraph "TensorView"
        TV1["Multi-dimensional indexing"]
        TV2["tensor[1, 2]"]
        TV3["Shape-aware operations"]
        TV4["Coordinate transformations"]
    end
    
    subgraph "Use Cases"
        UC1["BufferView: Low-level memory ops"]
        UC2["TensorView: Matrix/tensor algorithms"]
    end
    
    BV1 --> UC1
    TV1 --> UC2
    
    style BV1 fill:#dbeafe,stroke:#3b82f6,stroke-width:2px
    style TV1 fill:#fce7f3,stroke:#ec4899,stroke-width:2px
</div>
```

### C++ Advanced Features

<details>
<summary>Click to show C++ code</summary>

```cpp
__device__ void advanced_tensor_operations()
{
    // Create a 4x6 matrix
    float data[24];
    auto tensor = make_naive_tensor_view<address_space_enum::global>(
        data, make_tuple(4, 6), make_tuple(6, 1)
    );
    
    // Slicing and windowing require tile_window
    // Create a tile window for block access
    auto window = make_tile_window(
        tensor,
        make_tuple(2, 3),    // window shape
        make_tuple(1, 2)     // window origin
    );
    
    // Vectorized access using coordinates
    auto coord = make_tensor_coordinate(
        tensor.get_tensor_descriptor(),
        make_tuple(1, 0)
    );
    // Load 4 consecutive elements as float4
    using float4 = ck_tile::vector_type<float, 4>::type;
    auto vec4 = tensor.get_vectorized_elements<float4>(coord, 0);
}
```

</details>

## Performance Considerations

```{=html}
<div class="mermaid"  style="margin: 0 auto; display: block; width:70%;">
graph LR
    subgraph "Memory Access Patterns"
        Seq["Sequential Access<br/>(Good cache usage)"]
        Stride["Strided Access<br/>(May cause cache misses)"]
        Random["Random Access<br/>(Poor cache usage)"]
    end
    
    subgraph "Optimization Strategies"
        Opt1["Use row-major for row iteration"]
        Opt2["Use col-major for column iteration"]
        Opt3["Minimize stride between accesses"]
        Opt4["Vectorize when possible"]
    end
    
    Seq --> Opt1
    Stride --> Opt2
    Stride --> Opt3
    Random --> Opt4
    
    style Seq fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style Stride fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
    style Random fill:#fee2e2,stroke:#ef4444,stroke-width:2px
</div>
```

## Summary

TensorView provides:

- **Multi-dimensional indexing**: Natural coordinate-based access
- **Flexible memory layouts**: Row-major, column-major, custom strides
- **Zero-copy views**: Transpose, slice, reshape without copying data
- **Type safety**: Dimensions encoded in type (C++)
- **Integration**: Works seamlessly with BufferView and transforms

The abstraction enables writing dimension-agnostic algorithms while maintaining high performance through compile-time optimizations.