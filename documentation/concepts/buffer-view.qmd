---
title: "Buffer View"
format: live-html
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

Buffer views provide a unified abstraction for accessing memory across different address spaces and with various access patterns. They form the lowest level of the PythonCK memory hierarchy.

## Core Concepts

### Memory Address Spaces

```{pyodide}
#| echo: true
#| output: true

from pytensor.buffer_view import AddressSpaceEnum

# Different memory spaces available
print("Available address spaces:")
for space in AddressSpaceEnum:
    print(f"  {space.name}: {space.value}")
```

### Memory Operations

```{pyodide}
#| echo: true
#| output: true

from pytensor.buffer_view import MemoryOperationEnum

# Different memory operations supported
print("Available memory operations:")
for op in MemoryOperationEnum:
    print(f"  {op.name}: {op.value}")
```

## Creating Buffer Views

### Basic Buffer View

```{pyodide}
#| echo: true
#| output: true

import numpy as np
from pytensor.buffer_view import make_buffer_view, AddressSpaceEnum

# Create sample data
data = np.array([[1, 2, 3, 4],
                 [5, 6, 7, 8],
                 [9, 10, 11, 12]], dtype=np.float32)

print(f"Original data shape: {data.shape}")
print(f"Original data:\n{data}")

# Create a buffer view
buffer = make_buffer_view(
    data=data,
    buffer_size=data.size,
    address_space=AddressSpaceEnum.GLOBAL
)

print(f"\nBuffer view created:")
print(f"  Address space: {buffer.address_space}")
print(f"  Data type: {buffer.data.dtype}")
print(f"  Shape: {buffer.data.shape}")
```

### Accessing Buffer Elements

```{pyodide}
#| echo: true
#| output: true

# Access individual elements
print("Element access:")
print(f"  buffer[0, 0] = {buffer.data[0, 0]}")
print(f"  buffer[1, 2] = {buffer.data[1, 2]}")
print(f"  buffer[2, 3] = {buffer.data[2, 3]}")

# Access entire rows
print(f"\nRow access:")
print(f"  Row 0: {buffer.data[0, :]}")
print(f"  Row 1: {buffer.data[1, :]}")
```

## Memory Operations

### SET Operation

```{pyodide}
#| echo: true
#| output: true

from pytensor.buffer_view import MemoryOperationEnum

# Create a buffer for writing
write_data = np.zeros((2, 3), dtype=np.float32)
write_buffer = make_buffer_view(
    data=write_data,
    buffer_size=write_data.size,
    address_space=AddressSpaceEnum.GLOBAL
)

print(f"Before SET operations:\n{write_buffer.data}")

# Simulate SET operations (direct assignment)
write_buffer.data[0, 0] = 100.0
write_buffer.data[1, 2] = 200.0

print(f"\nAfter SET operations:\n{write_buffer.data}")
```

### ADD Operation

```{pyodide}
#| echo: true
#| output: true

# Create a buffer for accumulation
accum_data = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)
accum_buffer = make_buffer_view(
    data=accum_data,
    buffer_size=accum_data.size,
    address_space=AddressSpaceEnum.LDS  # Local Data Share
)

print(f"Before ADD operations:\n{accum_buffer.data}")

# Simulate ADD operations (accumulation)
accum_buffer.data[0, 0] += 10.0
accum_buffer.data[1, 1] += 20.0

print(f"\nAfter ADD operations:\n{accum_buffer.data}")
```

## Advanced Buffer Operations

### Buffer Views with Strides

```{pyodide}
#| echo: true
#| output: true

# Create a larger array and view it with different strides
large_data = np.arange(24, dtype=np.float32).reshape(4, 6)
print(f"Large data (4x6):\n{large_data}")

# Create a strided view (every other element)
strided_view = large_data[::2, ::2]  # Every 2nd row and column
strided_buffer = make_buffer_view(
    data=strided_view,
    buffer_size=strided_view.size,
    address_space=AddressSpaceEnum.GLOBAL
)

print(f"\nStrided view (2x3):\n{strided_buffer.data}")
print(f"Strides: {strided_buffer.data.strides}")
```

### Buffer Reshaping

```{pyodide}
#| echo: true
#| output: true

# Reshape buffer data
original = np.arange(12, dtype=np.float32).reshape(3, 4)
print(f"Original shape {original.shape}:\n{original}")

# Create buffer and reshape
buffer = make_buffer_view(data=original, buffer_size=original.size)
reshaped = buffer.data.reshape(2, 6)

print(f"\nReshaped to {reshaped.shape}:\n{reshaped}")

# Note: Both views share the same underlying memory
original[0, 0] = 999
print(f"\nAfter modifying original[0,0]:\n{reshaped}")
```

## Buffer View Integration

Buffer views serve as the foundation for all tensor operations in PythonCK. They provide the memory abstraction that higher-level components like tensor views and tile windows build upon.

```{pyodide}
#| echo: true
#| output: true

# Buffer views integrate with tensor operations
computation_data = np.arange(12, dtype=np.float32).reshape(3, 4)
computation_buffer = make_buffer_view(
    data=computation_data,
    buffer_size=computation_data.size,
    address_space=AddressSpaceEnum.GLOBAL
)

print(f"Buffer for tensor operations:")
print(f"  Shape: {computation_buffer.data.shape}")
print(f"  Address space: {computation_buffer.address_space.name}")
print(f"  Data:\n{computation_buffer.data}")
```

## Relationship to Other Concepts

Buffer views are the foundation for:

- **Tensor Views** - Combine buffers with tensor descriptors for structured access
- **Tile Windows** - Provide windowed access to buffer regions
- **Static Distributed Tensors** - Thread-local buffer management
- **Memory Operations** - All tensor operations ultimately access memory through buffer views

## Next Steps

- Learn about [Tensor Coordinates](tensor-coordinate.qmd) for multi-dimensional indexing
- Explore [Tensor Views](tensor-view.qmd) to see how buffers combine with descriptors
- Understand [Static Distributed Tensors](static-distributed-tensor.qmd) for parallel buffer management

Buffer views provide the essential memory abstraction that enables all higher-level tensor operations in PythonCK. 