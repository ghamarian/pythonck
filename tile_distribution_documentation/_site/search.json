[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tile Distribution Documentation",
    "section": "",
    "text": "Welcome to the complete tile distribution documentation! This is a standalone learning journey from basic memory concepts to advanced GPU optimization techniques.",
    "crumbs": [
      "Getting Started",
      "Tile Distribution Documentation"
    ]
  },
  {
    "objectID": "index.html#complete-learning-journey",
    "href": "index.html#complete-learning-journey",
    "title": "Tile Distribution Documentation",
    "section": "Complete Learning Journey",
    "text": "Complete Learning Journey\nFollow our structured 8-part learning path:\n\nIntroduction and Motivation\n\nIntroduction and Motivation - Why tile distribution matters, GPU memory challenges, and the solution overview\n\n\n\nFoundation\n\nBuffer Views - Raw memory access\nTensor Views - Multi-dimensional structure\n\n\n\nTransformation Engine\n\nBasic Coordinates - MultiIndex fundamentals\nIndividual Transforms - Building blocks (EmbedTransform, UnmergeTransform, etc.)\nChaining Adaptors - Combining transforms into complex operations\nComplete Descriptors - Full tensor specifications with layouts\nConvolution Example - Practical convolution implementation\nSwizzling Example - Morton ordering and memory patterns\nAdvanced Coordinates - TensorCoordinate and movement operations\n\n\n\nDistribution API\n\nTile Distribution - The core API for work assignment\nTile Window - Data access gateway with windowing\nSweep Tile - Elegant iteration patterns\n\n\n\nCoordinate Systems\n\nCoordinate Systems - The mathematical foundation (P, Y, X, R, D spaces)\n\n\n\nImplementation Deep Dive\n\nEncoding Internals - How mathematical encoding creates transformation components\nStatic Distributed Tensor - Thread-local data containers and organization\n\n\n\nThread Mapping\n\nThread Mapping - Connecting to hardware, thread cooperation patterns\n\n\n\nReference\n\nTerminology - Comprehensive glossary of all CK concepts, coordinate spaces, and operations\n\n\n\nComing Soon\n\nAdvanced Topics - Performance optimization and debugging",
    "crumbs": [
      "Getting Started",
      "Tile Distribution Documentation"
    ]
  },
  {
    "objectID": "index.html#interactive-applications",
    "href": "index.html#interactive-applications",
    "title": "Tile Distribution Documentation",
    "section": "Interactive Applications",
    "text": "Interactive Applications\nExplore tile distribution concepts through interactive web applications:\n\nTile Distribution Visualizer\nInteractive visualization of tile distribution structures and GPU memory layouts. Perfect for understanding how data is distributed across parallel processing elements. - Run locally: streamlit run ../app.py\n\n\nTensor Transform Visualizer\nExplore tensor descriptor transformations with visual graphs and mathematical formulas. See how data layouts change through various transformations. - Run locally: streamlit run ../tensor_transform_app.py\n\n\nThread Visualization App\nVisualize GPU thread coordinate mapping and access patterns. Understand how individual threads access distributed tensor data. - Run locally: streamlit run ../thread_visualization_app.py\n\n\nGPU Hierarchy Visualizer (NEW!)\nVisualize how work is distributed across vectors, warps, blocks, and tiles on a GPU with interactive controls. - Run locally: streamlit run ../gpu_hierarchy_visualizer.py",
    "crumbs": [
      "Getting Started",
      "Tile Distribution Documentation"
    ]
  },
  {
    "objectID": "index.html#quick-test",
    "href": "index.html#quick-test",
    "title": "Tile Distribution Documentation",
    "section": "Quick Test",
    "text": "Quick Test\nLet’s verify everything is working:",
    "crumbs": [
      "Getting Started",
      "Tile Distribution Documentation"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-documentation",
    "href": "index.html#how-to-use-this-documentation",
    "title": "Tile Distribution Documentation",
    "section": "How to Use This Documentation",
    "text": "How to Use This Documentation\n\nFollow the learning path - Each part builds on the previous ones\nRun the code - All examples are interactive and executable\nExperiment - Modify the code to deepen your understanding\nTest yourself - Each section includes validation exercises\n\nReady to start? Begin with Part 0: Introduction and Motivation!",
    "crumbs": [
      "Getting Started",
      "Tile Distribution Documentation"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html",
    "href": "concepts/03_tile_distribution.html",
    "title": "Tile Distribution - The Core API",
    "section": "",
    "text": "TileDistribution is the heart of Composable Kernels’ efficient GPU computation. It automatically maps logical coordinates to physical threads and memory locations, eliminating the need for manual thread management. This is the high-level API that GPU programmers actually use.\nThe fundamental architecture of tile distribution in CK revolves around a sophisticated coordinate transformation system that maps between multiple coordinate spaces. At its core, the system manages four primary coordinate dimensions: X (the physical tensor dimensions), Y (the tile access pattern dimensions), P (the processing element dimensions representing thread hierarchy), and optionally R (replication dimensions for redundant computation). This multi-dimensional mapping enables the framework to express complex data access patterns in a mathematically rigorous way while maintaining high performance on modern GPU architectures.\nThe C++ implementation encapsulates this complexity within the tile_distribution template class, which combines three essential components: a PsYs2XsAdaptor that performs the coordinate transformation from processing and pattern dimensions to physical tensor coordinates, a Ys2DDescriptor that linearizes the Y dimensions for efficient register allocation, and a StaticTileDistributionEncoding that captures the hierarchical decomposition of work across the GPU’s compute resources. This design allows the same high-level code to work efficiently across different tensor sizes and GPU configurations without manual tuning.",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#overview",
    "href": "concepts/03_tile_distribution.html#overview",
    "title": "Tile Distribution - The Core API",
    "section": "",
    "text": "TileDistribution is the heart of Composable Kernels’ efficient GPU computation. It automatically maps logical coordinates to physical threads and memory locations, eliminating the need for manual thread management. This is the high-level API that GPU programmers actually use.\nThe fundamental architecture of tile distribution in CK revolves around a sophisticated coordinate transformation system that maps between multiple coordinate spaces. At its core, the system manages four primary coordinate dimensions: X (the physical tensor dimensions), Y (the tile access pattern dimensions), P (the processing element dimensions representing thread hierarchy), and optionally R (replication dimensions for redundant computation). This multi-dimensional mapping enables the framework to express complex data access patterns in a mathematically rigorous way while maintaining high performance on modern GPU architectures.\nThe C++ implementation encapsulates this complexity within the tile_distribution template class, which combines three essential components: a PsYs2XsAdaptor that performs the coordinate transformation from processing and pattern dimensions to physical tensor coordinates, a Ys2DDescriptor that linearizes the Y dimensions for efficient register allocation, and a StaticTileDistributionEncoding that captures the hierarchical decomposition of work across the GPU’s compute resources. This design allows the same high-level code to work efficiently across different tensor sizes and GPU configurations without manual tuning.",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#complete-tile-distribution-system-overview",
    "href": "concepts/03_tile_distribution.html#complete-tile-distribution-system-overview",
    "title": "Tile Distribution - The Core API",
    "section": "Complete Tile Distribution System Overview",
    "text": "Complete Tile Distribution System Overview\n\ngraph TB\n    subgraph \"Logical View\"\n        T[\"TensorMulti-dimensional data\"]\n        TD[\"TileDistributionWork assignment\"]\n        TW[\"TileWindowData view\"]\n    end\n    \n    subgraph \"Coordinate Spaces\"\n        X[\"X: Physical tensor coords\"]\n        Y[\"Y: Tile pattern coords\"]\n        P[\"P: Processing element coords\"]\n        R[\"R: Replication coords (optional)\"]\n    end\n    \n    subgraph \"GPU Execution\"\n        W[\"Warps32 threads each\"]\n        L[\"LanesThread within warp\"]\n        REG[\"RegistersThread-local storage\"]\n    end\n    \n    T --&gt; TD\n    TD --&gt; TW\n    \n    TD --&gt; X\n    TD --&gt; Y\n    TD --&gt; P\n    TD --&gt; R\n    \n    P --&gt; W\n    P --&gt; L\n    TW --&gt; REG\n    \n    style TD fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style P fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style REG fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#interactive-exploration",
    "href": "concepts/03_tile_distribution.html#interactive-exploration",
    "title": "Tile Distribution - The Core API",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nExplore tile distribution concepts interactively:\nTile Distribution Visualizer - Interactive visualization of tile distribution structures and GPU memory layouts. Perfect for understanding how data is distributed across parallel processing elements.",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#coordinate-system-architecture",
    "href": "concepts/03_tile_distribution.html#coordinate-system-architecture",
    "title": "Tile Distribution - The Core API",
    "section": "Coordinate System Architecture",
    "text": "Coordinate System Architecture\n\nflowchart LR\n    subgraph \"Input\"\n        TC[\"Thread Coordinates(warpId, laneId)\"]\n    end\n    \n    subgraph \"Transformation Pipeline\"\n        P2Y[\"P → YThread to pattern\"]\n        Y2X[\"Y → XPattern to physical\"]\n        Y2D[\"Y → DPattern to register\"]\n    end\n    \n    subgraph \"Output\"\n        MC[\"Memory CoordinatesGlobal addresses\"]\n        RI[\"Register IndicesLocal storage\"]\n    end\n    \n    TC --&gt; P2Y\n    P2Y --&gt; Y2X\n    P2Y --&gt; Y2D\n    Y2X --&gt; MC\n    Y2D --&gt; RI\n    \n    style TC fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style MC fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style RI fill:#fef3c7,stroke:#f59e0b,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#what-is-tile-distribution",
    "href": "concepts/03_tile_distribution.html#what-is-tile-distribution",
    "title": "Tile Distribution - The Core API",
    "section": "What is Tile Distribution?",
    "text": "What is Tile Distribution?\nBefore diving into code, let’s understand the fundamental problem TileDistribution solves. In GPU programming, the challenge of efficiently distributing work across thousands of parallel threads is paramount. Consider a concrete scenario: you have a 256×256 matrix multiplication operation and 64 GPU threads organized in warps. The question becomes how to divide this computational work in a way that maximizes memory bandwidth utilization, minimizes bank conflicts, and ensures coalesced memory accesses.\nThe traditional approach without a tile distribution framework requires programmers to manually calculate global memory addresses for each thread, implement complex index arithmetic that accounts for thread hierarchy (threads within warps, warps within blocks), handle edge cases for non-divisible matrix dimensions, and create different implementations for various matrix sizes. This manual approach is not only error-prone but also fails to adapt to different GPU architectures and their specific memory access patterns.\nTileDistribution elegantly solves these challenges through a systematic approach to work distribution. It automatically assigns work to threads based on a hierarchical decomposition of the problem space, generates memory access patterns that respect GPU hardware constraints, provides a uniform interface that works across different tensor sizes and shapes, and ensures optimal thread cooperation by automatically managing data movement to thread-local registers.\nThe key insight that makes TileDistribution powerful is its ability to abstract the mapping between logical problem coordinates and physical execution resources. Given a thread’s position in the GPU’s execution hierarchy (specified by warp ID and lane ID within the warp), TileDistribution computes two critical pieces of information: the global memory addresses that this thread should access, and the specific access pattern that ensures efficient memory transactions. This abstraction is implemented in C++ through the following core structure:\ntemplate &lt;typename PsYs2XsAdaptor_,\n          typename Ys2DDescriptor_,\n          typename StaticTileDistributionEncoding_,\n          typename TileDistributionDetail_&gt;\nstruct tile_distribution\n{\n    // Core functionality: map thread coordinates to data\n    CK_TILE_HOST_DEVICE static auto _get_partition_index()\n    {\n        if constexpr(NDimP == 1)\n            return array&lt;index_t, 1&gt;{get_lane_id()};\n        else if constexpr(NDimP == 2)\n            return array&lt;index_t, 2&gt;{get_warp_id(), get_lane_id()};\n    }\n    \n    // Calculate which tensor elements this thread accesses\n    template &lt;typename PartitionIndex&gt;\n    CK_TILE_HOST_DEVICE static auto calculate_tile_Ys_index(const PartitionIndex& ps_idx)\n    {\n        return detail::calculate_tile_Ys_index(\n            StaticTileDistributionEncoding{}, ps_idx);\n    }\n};",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#problem-space-mapping",
    "href": "concepts/03_tile_distribution.html#problem-space-mapping",
    "title": "Tile Distribution - The Core API",
    "section": "Problem Space Mapping",
    "text": "Problem Space Mapping\n\ngraph TB\n    subgraph \"Problem Space (256×256 Matrix)\"\n        M[\"Full Matrix65,536 elements\"]\n        T1[\"Tile 132×32\"]\n        T2[\"Tile 232×32\"]\n        TN[\"Tile N32×32\"]\n    end\n    \n    subgraph \"Thread Assignment\"\n        W0[\"Warp 032 threads\"]\n        W1[\"Warp 132 threads\"]\n        L0[\"Lane 0-31Individual threads\"]\n    end\n    \n    subgraph \"Memory Pattern\"\n        MP[\"Coalesced AccessSequential addressesNo bank conflicts\"]\n    end\n    \n    M --&gt; T1\n    M --&gt; T2\n    M --&gt; TN\n    \n    T1 --&gt; W0\n    T1 --&gt; W1\n    W0 --&gt; L0\n    L0 --&gt; MP\n    \n    style M fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style MP fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#creating-a-tiledistribution",
    "href": "concepts/03_tile_distribution.html#creating-a-tiledistribution",
    "title": "Tile Distribution - The Core API",
    "section": "Creating a TileDistribution",
    "text": "Creating a TileDistribution\nLet’s see how to create and use a TileDistribution in practice:",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#understanding-thread-to-data-mapping",
    "href": "concepts/03_tile_distribution.html#understanding-thread-to-data-mapping",
    "title": "Tile Distribution - The Core API",
    "section": "Understanding Thread-to-Data Mapping",
    "text": "Understanding Thread-to-Data Mapping\nThe core functionality of TileDistribution is mapping thread IDs to tensor coordinates:",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#hierarchical-decomposition",
    "href": "concepts/03_tile_distribution.html#hierarchical-decomposition",
    "title": "Tile Distribution - The Core API",
    "section": "Hierarchical Decomposition",
    "text": "Hierarchical Decomposition\n\ngraph TB\n    subgraph \"Level 1: Block Distribution\"\n        B[\"Thread Block256 threads\"]\n        BT1[\"Block Tile 164×64\"]\n        BT2[\"Block Tile 264×64\"]\n    end\n    \n    subgraph \"Level 2: Warp Distribution\"\n        W[\"Warp32 threads\"]\n        WT1[\"Warp Tile 116×16\"]\n        WT2[\"Warp Tile 216×16\"]\n    end\n    \n    subgraph \"Level 3: Thread Distribution\"\n        T[\"Thread\"]\n        TT[\"Thread Tile2×2\"]\n    end\n    \n    B --&gt; BT1\n    BT1 --&gt; W\n    W --&gt; WT1\n    WT1 --&gt; T\n    T --&gt; TT\n    \n    style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style W fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style T fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#advanced-example-matrix-multiplication-distribution",
    "href": "concepts/03_tile_distribution.html#advanced-example-matrix-multiplication-distribution",
    "title": "Tile Distribution - The Core API",
    "section": "Advanced Example: Matrix Multiplication Distribution",
    "text": "Advanced Example: Matrix Multiplication Distribution",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#work-distribution-pattern",
    "href": "concepts/03_tile_distribution.html#work-distribution-pattern",
    "title": "Tile Distribution - The Core API",
    "section": "Work Distribution Pattern",
    "text": "Work Distribution Pattern\n\nflowchart TB\n    subgraph \"Matrix C (128×128)\"\n        C[\"16,384 elements\"]\n    end\n    \n    subgraph \"Thread Grid (32×32)\"\n        TG[\"1,024 threads\"]\n    end\n    \n    subgraph \"Per Thread\"\n        PT[\"4×4 tile16 elements\"]\n    end\n    \n    subgraph \"Memory Access\"\n        MA[\"Coalesced readsEfficient writesNo conflicts\"]\n    end\n    \n    C --&gt; TG\n    TG --&gt; PT\n    PT --&gt; MA\n    \n    style C fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style TG fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style PT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style MA fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#memory-access-patterns",
    "href": "concepts/03_tile_distribution.html#memory-access-patterns",
    "title": "Tile Distribution - The Core API",
    "section": "Memory Access Patterns",
    "text": "Memory Access Patterns\nOne of the key benefits of TileDistribution is generating optimal memory access patterns:",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#c-integration-example",
    "href": "concepts/03_tile_distribution.html#c-integration-example",
    "title": "Tile Distribution - The Core API",
    "section": "C++ Integration Example",
    "text": "C++ Integration Example\nThe real power of TileDistribution comes from its C++ implementation:\n// Real GEMM kernel pattern using TileDistribution\ntemplate&lt;typename AType, typename BType, typename CType&gt;\n__global__ void gemm_kernel(\n    const AType* __restrict__ a_ptr,\n    const BType* __restrict__ b_ptr,\n    CType* __restrict__ c_ptr,\n    index_t M, index_t N, index_t K)\n{\n    // Define the tile distribution encoding at compile time\n    using Encoding = tile_distribution_encoding&lt;\n        sequence&lt;&gt;,                                    // R: no replication\n        tuple&lt;sequence&lt;4, 2, 8, 4&gt;,                   // H for M dimension\n              sequence&lt;4, 2, 8, 4&gt;&gt;,                  // H for N dimension\n        tuple&lt;sequence&lt;1, 2&gt;, sequence&lt;1, 2&gt;&gt;,        // P to RH major\n        tuple&lt;sequence&lt;1, 1&gt;, sequence&lt;2, 2&gt;&gt;,        // P to RH minor\n        sequence&lt;1, 1, 2, 2&gt;,                         // Y to RH major\n        sequence&lt;0, 3, 0, 3&gt;                          // Y to RH minor\n    &gt;;\n    \n    // Create the distribution\n    constexpr auto distribution = make_static_tile_distribution(Encoding{});\n    \n    // Create tensor views\n    auto a_view = make_tensor_view&lt;const AType&gt;(\n        a_ptr, \n        make_naive_tensor_descriptor_packed(make_tuple(M, K)));\n    \n    // Create tile window for this thread block\n    auto a_window = make_tile_window(\n        a_view,\n        make_tuple(number&lt;256&gt;{}, number&lt;64&gt;{}),  // window size\n        {blockIdx.x * 256, 0},                    // origin\n        distribution);\n    \n    // Load data to distributed tensor (registers)\n    auto a_reg = make_static_distributed_tensor&lt;AType&gt;(distribution);\n    \n    a_window.load(a_reg);\n    \n    // Computation happens in registers\n    // Results written back through another window\n}",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#transformation-pipeline",
    "href": "concepts/03_tile_distribution.html#transformation-pipeline",
    "title": "Tile Distribution - The Core API",
    "section": "Transformation Pipeline",
    "text": "Transformation Pipeline\n\ngraph LR\n    subgraph \"Input\"\n        TID[\"Thread ID(0-1023)\"]\n    end\n    \n    subgraph \"Stage 1\"\n        P[\"P-coordinates(warp, lane)\"]\n    end\n    \n    subgraph \"Stage 2\"\n        Y[\"Y-coordinates(tile position)\"]\n    end\n    \n    subgraph \"Stage 3\"\n        X[\"X-coordinates(tensor indices)\"]\n    end\n    \n    subgraph \"Output\"\n        ADDR[\"Memory addressesRegister indices\"]\n    end\n    \n    TID --&gt; P\n    P --&gt; Y\n    Y --&gt; X\n    X --&gt; ADDR\n    \n    style TID fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style ADDR fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#performance-comparison",
    "href": "concepts/03_tile_distribution.html#performance-comparison",
    "title": "Tile Distribution - The Core API",
    "section": "Performance Comparison",
    "text": "Performance Comparison\n\ngraph TB\n    subgraph \"Manual Implementation\"\n        M1[\"Calculate indices manually\"]\n        M2[\"Handle boundary conditions\"]\n        M3[\"Ensure coalescing\"]\n        M4[\"Manage bank conflicts\"]\n        M5[\"~200 lines of code\"]\n    end\n    \n    subgraph \"With TileDistribution\"\n        T1[\"make_tile_distribution()\"]\n        T2[\"Automatic optimization\"]\n        T3[\"~10 lines of code\"]\n    end\n    \n    subgraph \"Performance\"\n        P1[\"Same performance\"]\n        P2[\"Fewer bugs\"]\n        P3[\"Portable across GPUs\"]\n    end\n    \n    M1 --&gt; M5\n    T1 --&gt; T3\n    \n    M5 --&gt; P1\n    T3 --&gt; P1\n    P1 --&gt; P2\n    P2 --&gt; P3\n    \n    style M5 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style T3 fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style P3 fill:#fef3c7,stroke:#f59e0b,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/03_tile_distribution.html#summary",
    "href": "concepts/03_tile_distribution.html#summary",
    "title": "Tile Distribution - The Core API",
    "section": "Summary",
    "text": "Summary\nTileDistribution provides: - Automatic work distribution: Maps threads to data efficiently - Optimal memory patterns: Ensures coalesced access and minimal conflicts - Hierarchical decomposition: Handles complex tiling strategies - Zero overhead: Compile-time optimization in C++ - Portability: Same code works across different GPU architectures\nKey benefits: 1. Correctness: Eliminates manual index calculation errors 2. Performance: Achieves hand-tuned performance automatically 3. Productivity: Reduces code from hundreds of lines to just a few 4. Maintainability: Clear separation of algorithm from distribution\nThe TileDistribution API is the foundation that enables Composable Kernels to achieve both high performance and high productivity in GPU programming.",
    "crumbs": [
      "Distribution API",
      "Tile Distribution - The Core API"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html",
    "href": "concepts/01_buffer_view.html",
    "title": "Buffer Views - Raw Memory Access",
    "section": "",
    "text": "BufferView provides structured access to raw memory regions for GPU kernels. It handles different memory address spaces (global, shared, register) with support for vectorized operations.\n\nHandles out-of-bounds access safely with configurable invalid values\nSupports both scalar and vector data types\nImplements AMD GPU-specific optimizations (buffer addressing, atomic operations)\nManages memory coherence and caching policies\n\n\n\n\nflowchart TB\n    subgraph CF [\"Compute Flow\"]\n        direction LR\n        GM1[\"Global MemoryInput Data\"] --&gt; LDS[\"LDSTile Cache\"]\n        LDS --&gt; VGPR[\"VGPRWorking Set\"]\n        VGPR --&gt; Compute[\"ComputeOperations\"]\n        Compute --&gt; VGPR\n        VGPR --&gt; LDS2[\"LDSReduction\"]\n        LDS2 --&gt; GM2[\"Global MemoryOutput Data\"]\n    end\n    \n    subgraph UP [\"Usage Pattern\"]\n        direction LR\n        P1[\"1. Load tile from Global → LDS\"]\n        P2[\"2. Load working set LDS → VGPR\"]\n        P3[\"3. Compute in VGPR\"]\n        P4[\"4. Store results VGPR → LDS\"]\n        P5[\"5. Reduce in LDS\"]\n        P6[\"6. Write final LDS → Global\"]\n        \n        P1 --&gt; P2 --&gt; P3 --&gt; P4 --&gt; P5 --&gt; P6\n    end\n    \n    CF ~~~ UP\n    \n    style GM1 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style LDS fill:#fed7aa,stroke:#f59e0b,stroke-width:2px\n    style VGPR fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Compute fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n\nNote: The data pointers for creating the buffer views should match the address space. In the given examples below the pointers are created in register space and passed to make_buffer_view in global space for example purpose. That would give an error if replicated.\n\n\n\n\n\n\nC++: BufferView uses template metaprogramming with compile-time constants\n\nSize is encoded in the type: number&lt;8&gt;{}\nAddress space is a template parameter\nEnables aggressive compiler optimizations\n\nPython: Everything is runtime\n\nSize is a regular integer\nAddress space is an enum value\nDesigned for learning and experimentation\n\n\n\n\n\n\nC++: Direct pointer to GPU memory\n\nNo memory allocation - uses existing memory\nPointer arithmetic for addressing\nZero-copy access to device memory\n\nPython: NumPy array simulation\n\nUses host memory to simulate GPU memory\nPython manages memory lifetime\nEducational approximation of GPU behavior\n\n\n\n\n\n\nC++: Strong compile-time typing\nbuffer_view&lt;float*, address_space_enum::global&gt;  // Type encodes everything\nPython: Dynamic typing with runtime checks\nBufferView  # Generic class, properties stored as members",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#address-space-usage-patterns",
    "href": "concepts/01_buffer_view.html#address-space-usage-patterns",
    "title": "Buffer Views - Raw Memory Access",
    "section": "",
    "text": "flowchart TB\n    subgraph CF [\"Compute Flow\"]\n        direction LR\n        GM1[\"Global MemoryInput Data\"] --&gt; LDS[\"LDSTile Cache\"]\n        LDS --&gt; VGPR[\"VGPRWorking Set\"]\n        VGPR --&gt; Compute[\"ComputeOperations\"]\n        Compute --&gt; VGPR\n        VGPR --&gt; LDS2[\"LDSReduction\"]\n        LDS2 --&gt; GM2[\"Global MemoryOutput Data\"]\n    end\n    \n    subgraph UP [\"Usage Pattern\"]\n        direction LR\n        P1[\"1. Load tile from Global → LDS\"]\n        P2[\"2. Load working set LDS → VGPR\"]\n        P3[\"3. Compute in VGPR\"]\n        P4[\"4. Store results VGPR → LDS\"]\n        P5[\"5. Reduce in LDS\"]\n        P6[\"6. Write final LDS → Global\"]\n        \n        P1 --&gt; P2 --&gt; P3 --&gt; P4 --&gt; P5 --&gt; P6\n    end\n    \n    CF ~~~ UP\n    \n    style GM1 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style LDS fill:#fed7aa,stroke:#f59e0b,stroke-width:2px\n    style VGPR fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Compute fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n\nNote: The data pointers for creating the buffer views should match the address space. In the given examples below the pointers are created in register space and passed to make_buffer_view in global space for example purpose. That would give an error if replicated.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#python-vs-c-key-differences",
    "href": "concepts/01_buffer_view.html#python-vs-c-key-differences",
    "title": "Buffer Views - Raw Memory Access",
    "section": "",
    "text": "C++: BufferView uses template metaprogramming with compile-time constants\n\nSize is encoded in the type: number&lt;8&gt;{}\nAddress space is a template parameter\nEnables aggressive compiler optimizations\n\nPython: Everything is runtime\n\nSize is a regular integer\nAddress space is an enum value\nDesigned for learning and experimentation\n\n\n\n\n\n\nC++: Direct pointer to GPU memory\n\nNo memory allocation - uses existing memory\nPointer arithmetic for addressing\nZero-copy access to device memory\n\nPython: NumPy array simulation\n\nUses host memory to simulate GPU memory\nPython manages memory lifetime\nEducational approximation of GPU behavior\n\n\n\n\n\n\nC++: Strong compile-time typing\nbuffer_view&lt;float*, address_space_enum::global&gt;  // Type encodes everything\nPython: Dynamic typing with runtime checks\nBufferView  # Generic class, properties stored as members",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-implementation-reference",
    "href": "concepts/01_buffer_view.html#c-implementation-reference",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Implementation Reference",
    "text": "C++ Implementation Reference\nFile: include/ck_tile/core/tensor/buffer_view.hpp\n#include &lt;ck_tile/core/tensor/buffer_view.hpp&gt;\n#include &lt;ck_tile/core/numeric/integral_constant.hpp&gt;\n\n// Create buffer view in C++\n__device__ void example_buffer_creation()\n{\n    // Static array in global memory\n    float data[8] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    constexpr index_t buffer_size = 8;\n\n    // Create buffer view for global memory\n    // Template parameters: &lt;AddressSpace&gt;\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(\n        data,        // pointer to data\n        buffer_size  // number of elements\n    );\n\n    // Alternative: Create with explicit type\n    using buffer_t = buffer_view&lt;float*, address_space_enum::global&gt;;\n    buffer_t explicit_buffer{data, number&lt;buffer_size&gt;{}};\n\n    // Access properties at compile time\n    constexpr auto size = buffer_view.get_buffer_size();\n    constexpr auto space = buffer_view.get_address_space();\n\n    // The buffer_view type encodes:\n    // - Data type (float)\n    // - Address space (global memory)\n    // - Size (known at compile time for optimization)\n    static_assert(size == 8, \"Buffer size should be 8\");\n    static_assert(space == address_space_enum::global, \"Should be global memory\");\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#creation-modes",
    "href": "concepts/01_buffer_view.html#creation-modes",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Creation Modes",
    "text": "Creation Modes\n\nZero Value Mode\n// Basic buffer view creation with automatic zero for invalid elements\nvoid basic_creation_example() {\n    // Create data array\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    \n    // Create global memory buffer view\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size);\n}\n\n\nCustom Value Mode\nvoid custom_invalid_value_example() {\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    float custom_invalid = 13.0f;\n    \n    // Create buffer view with custom invalid value\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(\n        data, buffer_size, custom_invalid);\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#scalar-access",
    "href": "concepts/01_buffer_view.html#scalar-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Scalar Access",
    "text": "Scalar Access\nScalar Access Parameters:\n\ni (index): Base offset in terms of T elements\nlinear_offset: Additional offset to add to the base index\nis_valid_element: Boolean controlling whether the access is valid\n\nInvalid Value Modes:\nIs specified while making the buffer view as shown in the above example.\n\nZero mode (InvalidElementUseNumericalZeroValue = true): Returns zero for invalid accesses.\nCustom mode (InvalidElementUseNumericalZeroValue = false): Returns the specified invalid value\n\nOut-of-Bounds Handling: The buffer view automatically handles bounds checking when AMD buffer addressing is enabled.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#vector-access",
    "href": "concepts/01_buffer_view.html#vector-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Vector Access",
    "text": "Vector Access\nVector operations use template parameters to specify the vector type (e.g., ext_vector_t&lt;float, N&gt; for N elements). The same parameters apply as scalar access, but the operation reads/writes multiple contiguous elements.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#scalar-vs-vectorized-memory-access",
    "href": "concepts/01_buffer_view.html#scalar-vs-vectorized-memory-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Scalar vs Vectorized Memory Access",
    "text": "Scalar vs Vectorized Memory Access\n\ngraph LR\n    subgraph \"Scalar Access (4 instructions)\"\n        S1[\"Load float[0]\"] --&gt; R1[\"Register 1\"]\n        S2[\"Load float[1]\"] --&gt; R2[\"Register 2\"]\n        S3[\"Load float[2]\"] --&gt; R3[\"Register 3\"]\n        S4[\"Load float[3]\"] --&gt; R4[\"Register 4\"]\n    end\n    \n    subgraph \"Vectorized Access (1 instruction)\"\n        V1[\"Load float4[0]\"] --&gt; VR[\"Vector Register(4 floats)\"]\n    end\n    \n    subgraph \"Performance Impact\"\n        Perf[\"4x fewer instructionsBetter memory bandwidthReduced latency\"]\n    end\n    \n    R1 & R2 & R3 & R4 --&gt; Perf\n    VR --&gt; Perf\n    \n    style S1 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S2 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S3 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S4 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style V1 fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Perf fill:#fef3c7,stroke:#f59e0b,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#understanding-bufferview-indexing",
    "href": "concepts/01_buffer_view.html#understanding-bufferview-indexing",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Understanding BufferView Indexing",
    "text": "Understanding BufferView Indexing\n\nflowchart LR\n    subgraph \"Input Parameters\"\n        Offset[\"Offset(e.g., 5)\"]\n        ValidFlag[\"Valid Flag(optional)\"]\n    end\n    \n    subgraph \"Processing\"\n        BoundsCheck{{\"Bounds Checkoffset &lt; buffer_size?\"}}\n        FlagCheck{{\"Flag Checkvalid_flag == True?\"}}\n        Access[\"Access Memorybuffer[offset]\"]\n    end\n    \n    subgraph \"Output\"\n        ValidResult[\"Valid ResultReturn value\"]\n        Invalid[\"Invalid ResultReturn 0 or default\"]\n    end\n    \n    Offset --&gt; BoundsCheck\n    ValidFlag --&gt; FlagCheck\n    \n    BoundsCheck --&gt;|Yes| FlagCheck\n    BoundsCheck --&gt;|No| Invalid\n    \n    FlagCheck --&gt;|Yes| Access\n    FlagCheck --&gt;|No| Invalid\n    \n    Access --&gt; ValidResult\n    \n    style Offset fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style ValidFlag fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style ValidResult fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Invalid fill:#fee2e2,stroke:#ef4444,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-get-operations",
    "href": "concepts/01_buffer_view.html#c-get-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Get Operations",
    "text": "C++ Get Operations\n__device__ void example_get_operations()\n{\n    // Create buffer view\n    float data[8] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, 8);\n\n    // Simple get - compile-time bounds checking when possible\n    auto value_buf = buffer_view.template get&lt;float&gt;(0,1,true); //get the buffer from the buffer view\n    float value = value_buf.get(0); //get the value from the buffer\n\n    // Get with valid flag - branchless conditional access\n    bool valid_flag = false;\n    value_buf = buffer_view.template get&lt;float&gt;(0,1,valid_flag);\n    value = value_buf.get(0);\n    // Returns 0 valid_flag is false\n\n    // vectorized get\n    using float2 = ext_vector_t&lt;float, 2&gt;;\n    auto vector_buf = buffer_view.template get&lt;float2&gt;(0, 0, true);\n    // Loads 2 floats in a single instruction\n    float val1 = vector_buf.get(0);\n    float val2 = vector_buf.get(1);\n}\n\nCustom Value Return Mode for OOB & Invalid Access\nvoid scalar_get_operations_example() {\n\n   // Create data array\n   constexpr size_t buffer_size = 8;\n   float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n   float custom_invalid = 13.0f;\n   \n   // Create global memory buffer view with zero invalid value mode (default)\n   auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size, custom_invalid);\n   \n   // Invalid element access with is_valid_element=false\n   // Returns custom_invalid due to custom invalid value mode\n   auto invalid_value = buffer_view.template get&lt;float&gt;(0, 0, false);\n   printf(\"Invalid element: %.1f\\n\", invalid_value.get(0));\n   \n   // Out of bounds access - AMD buffer addressing handles bounds checking\n   // Will return custom_invalid when accessing beyond buffer_size\n   auto oob_value = buffer_view.template get&lt;float&gt;(0, 100, true);\n   printf(\"Out of bounds: %.1f\\n\", oob_value.get(0));\n}\nNOTE: Partial Out Of Bound (OOB) access during vector reads will return ‘junk’ values for the OOB access. Zero or custom invalid value is only returned for complete invalid/OOB access, i.e. when the first address of the vector is invalid.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-update-operations",
    "href": "concepts/01_buffer_view.html#c-update-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Update Operations",
    "text": "C++ Update Operations\nvoid scalar_set_operations_example() {\n        \n    // Create data array\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    \n    // Create global memory buffer view\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size);\n    \n    // Basic set: set&lt;T&gt;(i, linear_offset, is_valid_element, value)\n    // Sets element at position i + linear_offset = 0 + 2 = 2\n    buffer_view.template set&lt;float&gt;(0, 2, true, 99.0f);\n    \n    // Invalid write with is_valid_element=false (ignored)\n    buffer_view.template set&lt;float&gt;(0, 3, false, 777.0f);\n    \n    // Out of bounds write - handled safely by AMD buffer addressing\n    buffer_view.template set&lt;float&gt;(0, 100, true, 555.0f);\n\n    // Vector set\n    using float2 = ext_vector_t&lt;float, 2&gt;;\n    float2 pair_values{100.0f, 200.0f};\n    buffer_view.template set&lt;float2&gt;(0, 5, true, pair_values);\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#atomic-vs-non-atomic-operations",
    "href": "concepts/01_buffer_view.html#atomic-vs-non-atomic-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Atomic vs Non-Atomic Operations",
    "text": "Atomic vs Non-Atomic Operations\n\ngraph TB\n    subgraph \"Non-Atomic Operation (Race Condition)\"\n        NA1[\"Thread 1: Read value (10)\"] --&gt; NA2[\"Thread 1: Add 5 (15)\"]\n        NA3[\"Thread 2: Read value (10)\"] --&gt; NA4[\"Thread 2: Add 3 (13)\"]\n        NA2 --&gt; NA5[\"Thread 1: Write 15\"]\n        NA4 --&gt; NA6[\"Thread 2: Write 13\"]\n        NA5 & NA6 --&gt; NA7[\"Final value: 13 ❌(Lost update from Thread 1)\"]\n    end\n    \n    subgraph \"Atomic Operation (Thread-Safe)\"\n        A1[\"Thread 1: atomic_add(5)\"] --&gt; A2[\"Hardware ensuresserialization\"]\n        A3[\"Thread 2: atomic_add(3)\"] --&gt; A2\n        A2 --&gt; A4[\"Final value: 18 ✓(Both updates applied)\"]\n    end\n    \n    style NA7 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style A4 fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-atomic-operations",
    "href": "concepts/01_buffer_view.html#c-atomic-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Atomic Operations",
    "text": "C++ Atomic Operations\n__device__ void example_atomic_operations()\n{\n    // Shared memory for workgroup-level reductions\n    __shared__ float shared_sum[256];\n    auto shared_buffer_view = make_buffer_view&lt;address_space_enum::lds&gt;(\n        shared_sum, 256\n    );\n\n    // Initialize shared memory\n    if (threadIdx.x &lt; 256) {\n        shared_buffer_view.template set&lt;float&gt;(threadIdx.x, 0.0f, true);\n    }\n    __syncthreads();\n\n    // Each thread atomically adds to shared memory\n    auto my_value = static_cast&lt;float&gt;(threadIdx.x);\n    shared_buffer_view.template update&lt;memory_operation_enum::atomic_add, float&gt;(0,0,true,my_value);\n    \n    // Atomic max for finding maximum value\n    shared_buffer_view.template update&lt;memory_operation_enum::atomic_max, float&gt;(0,1,true,my_value);\n    \n    __syncthreads();\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html",
    "href": "concepts/02_coordinate_movement.html",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "Now that you understand transforms, adaptors, and descriptors, it’s time to learn advanced coordinate operations, and how they are used to navigate through complex tensor layouts. Basically, there are two main operations:\n\nmake_tensor_coordinate: Create a tensor coordinate from a descriptor and an index\nmake_tensor_adaptor_coordinate: Create a tensor adaptor coordinate from an adaptor and an index\nmove_tensor_coordinate: Move a tensor coordinate through a descriptor with a given offset\nmove_tensor_adaptor_coordinate: Move a tensor adaptor coordinate through an adaptor with a given offset\n\nThese operations are basically applying the transforms to the coordinates, that are top dimensions of the descriptor or adaptor, and return the new coordinates. tensor_coordinate and tensor_adaptor_coordinate are the classes that are used to cache the applications of the transforms to the top dimensions coordinates. It stores the results of the applications of the transforms to the top dimensions coordinates.\n\n\nThis builds on everything we’ve learned:\n\nMultiIndex: Basic coordinates\nTransforms: Individual coordinate mappings\n\nAdaptors: Transform chains\nDescriptors: Complete tensor specifications\n\n\ngraph TB\n    subgraph \"Coordinate Movement System\"\n        TC[\"TensorCoordinatePosition + Descriptor Context\"]\n        TAC[\"TensorAdaptorCoordinatePosition + Transform Context\"]\n        MC[\"move_coordinate()Efficient Navigation\"]\n    end\n    \n    subgraph \"Movement Example\"\n        S[\"Start: [1,1]Offset: 5\"]\n        M1[\"Move [0,1]→ [1,2]Offset: 6\"]\n        M2[\"Move [1,0]→ [2,2]Offset: 10\"]\n        M3[\"Move [1,1]→ [3,3]Offset: 15\"]\n    end\n    \n    TC --&gt; MC\n    TAC --&gt; MC\n    \n    S --&gt; M1\n    M1 --&gt; M2\n    M2 --&gt; M3\n    \n    style TC fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style TAC fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style MC fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorCoordinate works with TensorDescriptor to provide descriptor-aware coordinate navigation:\n\n\n\n\n\n\n\n\n\nTensorAdaptorCoordinate works with adaptors to track coordinates through transformation chains:\n\n\n\n\n\n\n\n\n\nmove_tensor_coordinate is the key function for navigating tensor layouts. It updates coordinates efficiently through descriptor-defined transformations by applying the transforms to the top dimensions coordinates if necessary. It recrusively checks and updates the coordinates that are affected by the higher level transforms. If they are unchanged, no transform operation is propagated to the lower level coordinates.\n\n\n\n\n\n\n\n\n\nThe real power shows when moving through complex adaptor transformations:\n\n\n\n\n\n\n\n\n\nThese coordinate operations are used everywhere in real applications:\n\n\n\n\n\n\n\n\n\nOne key insight: move_tensor_coordinate is much more efficient than recreating coordinates:\n\n\n\n\n\n\n\n\n\nReal applications use sophisticated movement patterns:\n\n\n\n\n\n\n\n\n\nLet’s test your understanding of advanced coordinate operations:\n\n\n\n\n\n\n\n\n\nAdvanced coordinate operations are the bridge between mathematical transforms and practical tensor manipulation:\n1. TensorCoordinate: Descriptor-aware navigation\n\nTracks the position of a tensor in a descriptor\nProvides linear offset for memory access (bottom index)\nEnables descriptor-validated operations\n\n2. TensorAdaptorCoordinate: Adaptor-aware tracking\n\nTracks coordinates through transformation chains\nHandles complex multi-stage transformations\nEnables efficient pipeline navigation\n\n3. move_tensor_coordinate: Efficient navigation\n\nUpdates coordinates without full recalculation\nEssential for high-performance operations\nUsed by all tile and distribution operations\n\n4. Real-world applications:\n\nTile window positioning and traversal\nThread coordinate mapping and distribution\nMemory layout navigation and optimization\n\nThese operations form the foundation for all advanced tensor operations in the system. Master them, and you’re ready for the Distribution API!",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#prerequisites",
    "href": "concepts/02_coordinate_movement.html#prerequisites",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "This builds on everything we’ve learned:\n\nMultiIndex: Basic coordinates\nTransforms: Individual coordinate mappings\n\nAdaptors: Transform chains\nDescriptors: Complete tensor specifications\n\n\ngraph TB\n    subgraph \"Coordinate Movement System\"\n        TC[\"TensorCoordinatePosition + Descriptor Context\"]\n        TAC[\"TensorAdaptorCoordinatePosition + Transform Context\"]\n        MC[\"move_coordinate()Efficient Navigation\"]\n    end\n    \n    subgraph \"Movement Example\"\n        S[\"Start: [1,1]Offset: 5\"]\n        M1[\"Move [0,1]→ [1,2]Offset: 6\"]\n        M2[\"Move [1,0]→ [2,2]Offset: 10\"]\n        M3[\"Move [1,1]→ [3,3]Offset: 15\"]\n    end\n    \n    TC --&gt; MC\n    TAC --&gt; MC\n    \n    S --&gt; M1\n    M1 --&gt; M2\n    M2 --&gt; M3\n    \n    style TC fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style TAC fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style MC fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#tensorcoordinate-descriptor-aware-coordinates",
    "href": "concepts/02_coordinate_movement.html#tensorcoordinate-descriptor-aware-coordinates",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "TensorCoordinate works with TensorDescriptor to provide descriptor-aware coordinate navigation:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#tensoradaptorcoordinate-adaptor-aware-coordinates",
    "href": "concepts/02_coordinate_movement.html#tensoradaptorcoordinate-adaptor-aware-coordinates",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "TensorAdaptorCoordinate works with adaptors to track coordinates through transformation chains:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#the-power-of-move_tensor_coordinate",
    "href": "concepts/02_coordinate_movement.html#the-power-of-move_tensor_coordinate",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "move_tensor_coordinate is the key function for navigating tensor layouts. It updates coordinates efficiently through descriptor-defined transformations by applying the transforms to the top dimensions coordinates if necessary. It recrusively checks and updates the coordinates that are affected by the higher level transforms. If they are unchanged, no transform operation is propagated to the lower level coordinates.",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#movement-through-complex-transformations",
    "href": "concepts/02_coordinate_movement.html#movement-through-complex-transformations",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "The real power shows when moving through complex adaptor transformations:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#practical-applications",
    "href": "concepts/02_coordinate_movement.html#practical-applications",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "These coordinate operations are used everywhere in real applications:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#understanding-movement-efficiency",
    "href": "concepts/02_coordinate_movement.html#understanding-movement-efficiency",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "One key insight: move_tensor_coordinate is much more efficient than recreating coordinates:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#advanced-movement-patterns",
    "href": "concepts/02_coordinate_movement.html#advanced-movement-patterns",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "Real applications use sophisticated movement patterns:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#testing-your-understanding",
    "href": "concepts/02_coordinate_movement.html#testing-your-understanding",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "Let’s test your understanding of advanced coordinate operations:",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/02_coordinate_movement.html#key-takeaways",
    "href": "concepts/02_coordinate_movement.html#key-takeaways",
    "title": "Advanced Coordinate Operations",
    "section": "",
    "text": "Advanced coordinate operations are the bridge between mathematical transforms and practical tensor manipulation:\n1. TensorCoordinate: Descriptor-aware navigation\n\nTracks the position of a tensor in a descriptor\nProvides linear offset for memory access (bottom index)\nEnables descriptor-validated operations\n\n2. TensorAdaptorCoordinate: Adaptor-aware tracking\n\nTracks coordinates through transformation chains\nHandles complex multi-stage transformations\nEnables efficient pipeline navigation\n\n3. move_tensor_coordinate: Efficient navigation\n\nUpdates coordinates without full recalculation\nEssential for high-performance operations\nUsed by all tile and distribution operations\n\n4. Real-world applications:\n\nTile window positioning and traversal\nThread coordinate mapping and distribution\nMemory layout navigation and optimization\n\nThese operations form the foundation for all advanced tensor operations in the system. Master them, and you’re ready for the Distribution API!",
    "crumbs": [
      "Transformation Engine",
      "Advanced Coordinate Operations"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html",
    "href": "concepts/03_tile_window.html",
    "title": "Tile Window - Data Access Gateway",
    "section": "",
    "text": "The TileWindow abstraction represents the culmination of the CK framework’s approach to efficient tensor data access on GPUs. While TileDistribution determines the mapping between threads and tensor coordinates, TileWindow provides the actual mechanism for loading and storing data with optimal memory access patterns. This abstraction encapsulates the complexity of coalesced memory accesses, vectorization, and boundary handling into a clean interface that enables developers to focus on algorithmic logic rather than low-level memory management.\nAt its core, TileWindow implements a sophisticated windowing mechanism that views a subset of a larger tensor through the lens of a tile distribution. This windowing is not merely a simple sub-tensor extraction but a distribution-aware view that automatically generates the most efficient memory access patterns for the underlying hardware. The system achieves this by combining knowledge of the tensor’s layout, the distribution pattern, and the GPU’s memory subsystem characteristics to generate optimized load and store operations.\n\n\n\ngraph TB\n    subgraph \"Components\"\n        TV[\"TensorViewData source\"]\n        TD[\"TileDistributionThread mapping\"]\n        TW[\"TileWindowAccess gateway\"]\n        DT[\"DistributedTensorRegister storage\"]\n    end\n    \n    subgraph \"Operations\"\n        Load[\"LoadGlobal → Registers\"]\n        Compute[\"ComputeIn registers\"]\n        Store[\"StoreRegisters → Global\"]\n    end\n    \n    subgraph \"Optimizations\"\n        Coal[\"CoalescingAdjacent access\"]\n        Vec[\"VectorizationMulti-element ops\"]\n        Bank[\"Bank conflictavoidance\"]\n    end\n    \n    TV --&gt; TW\n    TD --&gt; TW\n    TW --&gt; DT\n    \n    TW --&gt; Load\n    Load --&gt; Compute\n    Compute --&gt; Store\n    \n    Load --&gt; Coal\n    Load --&gt; Vec\n    Store --&gt; Bank\n    \n    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style DT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style Coal fill:#fff3e0,stroke:#f57c00,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#overview",
    "href": "concepts/03_tile_window.html#overview",
    "title": "Tile Window - Data Access Gateway",
    "section": "",
    "text": "The TileWindow abstraction represents the culmination of the CK framework’s approach to efficient tensor data access on GPUs. While TileDistribution determines the mapping between threads and tensor coordinates, TileWindow provides the actual mechanism for loading and storing data with optimal memory access patterns. This abstraction encapsulates the complexity of coalesced memory accesses, vectorization, and boundary handling into a clean interface that enables developers to focus on algorithmic logic rather than low-level memory management.\nAt its core, TileWindow implements a sophisticated windowing mechanism that views a subset of a larger tensor through the lens of a tile distribution. This windowing is not merely a simple sub-tensor extraction but a distribution-aware view that automatically generates the most efficient memory access patterns for the underlying hardware. The system achieves this by combining knowledge of the tensor’s layout, the distribution pattern, and the GPU’s memory subsystem characteristics to generate optimized load and store operations.\n\n\n\ngraph TB\n    subgraph \"Components\"\n        TV[\"TensorViewData source\"]\n        TD[\"TileDistributionThread mapping\"]\n        TW[\"TileWindowAccess gateway\"]\n        DT[\"DistributedTensorRegister storage\"]\n    end\n    \n    subgraph \"Operations\"\n        Load[\"LoadGlobal → Registers\"]\n        Compute[\"ComputeIn registers\"]\n        Store[\"StoreRegisters → Global\"]\n    end\n    \n    subgraph \"Optimizations\"\n        Coal[\"CoalescingAdjacent access\"]\n        Vec[\"VectorizationMulti-element ops\"]\n        Bank[\"Bank conflictavoidance\"]\n    end\n    \n    TV --&gt; TW\n    TD --&gt; TW\n    TW --&gt; DT\n    \n    TW --&gt; Load\n    Load --&gt; Compute\n    Compute --&gt; Store\n    \n    Load --&gt; Coal\n    Load --&gt; Vec\n    Store --&gt; Bank\n    \n    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style DT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style Coal fill:#fff3e0,stroke:#f57c00,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#what-is-a-tilewindow",
    "href": "concepts/03_tile_window.html#what-is-a-tilewindow",
    "title": "Tile Window - Data Access Gateway",
    "section": "What is a TileWindow?",
    "text": "What is a TileWindow?\nThe fundamental challenge in GPU programming lies in the gap between logical tensor operations and the physical realities of memory access. While TileDistribution elegantly solves the problem of work assignment by mapping threads to tensor coordinates, it does not address how threads actually access the data at those coordinates. This is where TileWindow enters the picture, serving as the critical bridge between logical work assignment and physical memory operations.\nTileWindow implements a distribution-aware windowing mechanism that transforms abstract coordinate mappings into concrete memory access patterns. The abstraction understands not just which data elements each thread needs, but also how to access them in a way that maximizes memory bandwidth utilization. This involves sophisticated techniques such as memory coalescing, where adjacent threads access adjacent memory locations, and vectorization, where multiple elements are loaded or stored in a single transaction.\nThe C++ implementation of TileWindow reveals its sophisticated architecture:\n// From ck_tile/core/tensor/tile_window.hpp\n#include &lt;ck_tile/core/tensor/tile_window.hpp&gt;\n#include &lt;ck_tile/core/tensor/static_distributed_tensor.hpp&gt;\n#include &lt;ck_tile/core/algorithm/coordinate_transform.hpp&gt;\n\ntemplate &lt;typename TensorView_, \n          typename WindowLengths_, \n          typename TileDistribution_&gt;\nstruct tile_window_with_static_distribution\n{\n    using TensorView = remove_cvref_t&lt;TensorView_&gt;;\n    using Distribution = remove_cvref_t&lt;TileDistribution_&gt;;\n    using DataType = typename TensorView::DataType;\n    \n    // Core components that define the window\n    TensorView tensor_view_;      // View into the underlying tensor\n    Distribution distribution_;    // How to distribute data across threads\n    array&lt;index_t, TensorView::get_num_of_dimension()&gt; origin_;\n    \n    // Window-specific information\n    static constexpr auto window_lengths = WindowLengths{};\n    static constexpr index_t num_of_dimension = TensorView::get_num_of_dimension();\n    \n    // Constructor\n    CK_TILE_HOST_DEVICE constexpr tile_window_with_static_distribution(\n        const TensorView& tensor_view,\n        const WindowLengths& /*window_lengths*/,\n        const array&lt;index_t, num_of_dimension&gt;& origin,\n        const Distribution& distribution)\n        : tensor_view_{tensor_view},\n          distribution_{distribution},\n          origin_{origin}\n    {}\n    \n    // Load operation with automatic coalescing\n    template &lt;typename DistributedTensor&gt;\n    CK_TILE_DEVICE void load(DistributedTensor& dst_tensor) const\n    {\n        // Sophisticated load implementation that:\n        // 1. Calculates optimal access pattern\n        // 2. Handles vectorization automatically\n        // 3. Ensures coalesced memory access\n        // 4. Manages boundary conditions\n    }\n};",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#tilewindow-data-flow",
    "href": "concepts/03_tile_window.html#tilewindow-data-flow",
    "title": "Tile Window - Data Access Gateway",
    "section": "TileWindow Data Flow",
    "text": "TileWindow Data Flow\n\nflowchart LR\n    subgraph \"Step 1: Create Window\"\n        T[\"Tensor[256, 256]\"]\n        O[\"Origin(64, 64)\"]\n        W[\"Window Size[32, 32]\"]\n    end\n    \n    subgraph \"Step 2: Apply Distribution\"\n        TD[\"TileDistributionThread mapping\"]\n        TW[\"TileWindowCreated\"]\n    end\n    \n    subgraph \"Step 3: Load Data\"\n        GM[\"Global MemoryWindow region\"]\n        REG[\"RegistersDistributed tensor\"]\n    end\n    \n    T --&gt; TW\n    O --&gt; TW\n    W --&gt; TW\n    TD --&gt; TW\n    \n    TW --&gt; GM\n    GM --&gt;|\"load()\"| REG\n    \n    style TW fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style REG fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#creating-and-using-tilewindow",
    "href": "concepts/03_tile_window.html#creating-and-using-tilewindow",
    "title": "Tile Window - Data Access Gateway",
    "section": "Creating and Using TileWindow",
    "text": "Creating and Using TileWindow\nLet’s explore how to create and use a TileWindow in practice:",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#understanding-window-coordinates",
    "href": "concepts/03_tile_window.html#understanding-window-coordinates",
    "title": "Tile Window - Data Access Gateway",
    "section": "Understanding Window Coordinates",
    "text": "Understanding Window Coordinates\n\nC++ Implementation Reference\nThe TileWindow creation pattern shown above corresponds to the following C++ implementation:\nFile: include/ck_tile/core/tensor/tile_window.hpp\n// Creating a tile window in practice\ntemplate &lt;typename BlockTileShape, typename DataType&gt;\n__device__ auto create_input_window(\n    const DataType* __restrict__ p_global,\n    index_t M, index_t N)\n{\n    // Create tensor view\n    auto tensor_view = make_tensor_view&lt;AddressSpaceEnum::Global&gt;(\n        p_global,\n        make_naive_tensor_descriptor_packed(make_tuple(M, N)));\n    \n    // Define window parameters\n    constexpr auto window_lengths = BlockTileShape{};\n    const auto origin = make_tuple(blockIdx.x * window_lengths[I0],\n                                   blockIdx.y * window_lengths[I1]);\n    \n    // Create window with distribution\n    return make_tile_window(\n        tensor_view,\n        window_lengths,\n        origin,\n        make_static_tile_distribution(TileDistribution{}));\n}\nThe window creation process demonstrates several important principles of the CK framework. First, the window dimensions (2×2 in this example) are chosen to match the tile distribution pattern, ensuring that each thread has work to do. Second, the origin coordinates allow the window to target specific regions of the tensor, enabling techniques like overlapped tiling for convolutions or sliding window operations. Third, the integration with tile distribution means that the window automatically knows how to map its data to the appropriate threads, eliminating the need for manual index calculations.\nIn the C++ implementation, the tile window creation involves template instantiation that captures all these parameters at compile time:\n// From ck_tile/core/tensor/tile_window.hpp\ntemplate &lt;typename TensorView,\n          typename WindowLengths,\n          typename Origin,\n          typename Distribution&gt;\nCK_TILE_HOST_DEVICE constexpr auto\nmake_tile_window(const TensorView& tensor_view,\n                 const WindowLengths& window_lengths,\n                 const Origin& origin,\n                 const Distribution& distribution)\n{\n    // Create specialized window type based on distribution\n    using WindowType = tile_window_with_static_distribution&lt;\n        TensorView, WindowLengths, Distribution&gt;;\n    \n    // Construct window with all parameters\n    return WindowType{tensor_view, window_lengths, origin, distribution};\n}",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#loading-data-with-tilewindow",
    "href": "concepts/03_tile_window.html#loading-data-with-tilewindow",
    "title": "Tile Window - Data Access Gateway",
    "section": "Loading Data with TileWindow",
    "text": "Loading Data with TileWindow\nThe load operation represents one of the most critical performance-sensitive operations in GPU kernel development. TileWindow’s load functionality transforms a high-level request (“load this tile of data”) into a series of optimized memory transactions that respect GPU hardware constraints while maximizing bandwidth utilization. This transformation involves several sophisticated techniques working in concert to achieve near-optimal performance.\nThe loading process begins with the tile window analyzing the distribution pattern to determine which data elements each thread needs to load. This analysis considers not just the logical mapping but also the physical memory layout, identifying opportunities for coalesced access where adjacent threads load from adjacent memory locations. The system then generates load instructions that take advantage of the GPU’s memory subsystem capabilities, including vector loads that can fetch multiple elements in a single transaction.",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#load-operation-architecture",
    "href": "concepts/03_tile_window.html#load-operation-architecture",
    "title": "Tile Window - Data Access Gateway",
    "section": "Load Operation Architecture",
    "text": "Load Operation Architecture\n\ngraph TB\n    subgraph \"Load Analysis\"\n        Analyze[\"Analyze access patternDetect coalescing opportunities\"]\n    end\n    \n    subgraph \"Vectorization\"\n        V1[\"Scalar: 4 loads\"]\n        V2[\"Vector2: 2 loads\"]\n        V4[\"Vector4: 1 load\"]\n    end\n    \n    subgraph \"Memory Transaction\"\n        Coal[\"Coalesced access32 threads → 1 transaction\"]\n        NonCoal[\"Non-coalesced32 threads → 32 transactions\"]\n    end\n    \n    subgraph \"Result\"\n        Reg[\"Thread registersLocal data\"]\n    end\n    \n    Analyze --&gt; V1\n    Analyze --&gt; V2\n    Analyze --&gt; V4\n    \n    V4 --&gt; Coal\n    V1 --&gt; NonCoal\n    \n    Coal --&gt; Reg\n    NonCoal --&gt; Reg\n    \n    style V4 fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Coal fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style NonCoal fill:#fee2e2,stroke:#ef4444,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#memory-access-patterns",
    "href": "concepts/03_tile_window.html#memory-access-patterns",
    "title": "Tile Window - Data Access Gateway",
    "section": "Memory Access Patterns",
    "text": "Memory Access Patterns\nOne of TileWindow’s key features is generating optimal memory access patterns:",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#advanced-window-operations",
    "href": "concepts/03_tile_window.html#advanced-window-operations",
    "title": "Tile Window - Data Access Gateway",
    "section": "Advanced Window Operations",
    "text": "Advanced Window Operations\n\nSliding Windows\n\n\n\n\n\n\n\n\nOverlapping Windows",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#computation-architecture",
    "href": "concepts/03_tile_window.html#computation-architecture",
    "title": "Tile Window - Data Access Gateway",
    "section": "Computation Architecture",
    "text": "Computation Architecture\n\ngraph TB\n    subgraph \"Load Phase\"\n        LW[\"Load WindowGlobal → Registers\"]\n    end\n    \n    subgraph \"Compute Phase\"\n        C1[\"Thread-local computeIn registers\"]\n        C2[\"Warp-level shuffleData exchange\"]\n        C3[\"Block-level reductionShared memory\"]\n    end\n    \n    subgraph \"Store Phase\"\n        SW[\"Store WindowRegisters → Global\"]\n    end\n    \n    LW --&gt; C1\n    C1 --&gt; C2\n    C2 --&gt; C3\n    C3 --&gt; SW\n    \n    style LW fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style SW fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#c-usage-example",
    "href": "concepts/03_tile_window.html#c-usage-example",
    "title": "Tile Window - Data Access Gateway",
    "section": "C++ Usage Example",
    "text": "C++ Usage Example\nThe real power of TileWindow comes from its C++ implementation:\n// Complete example: Matrix multiplication using TileWindow\ntemplate&lt;typename AType, typename BType, typename CType&gt;\n__global__ void gemm_kernel_with_windows(\n    const AType* __restrict__ a_ptr,\n    const BType* __restrict__ b_ptr,\n    CType* __restrict__ c_ptr,\n    index_t M, index_t N, index_t K)\n{\n    // Create tensor views\n    auto a_tensor = make_naive_tensor_view(\n        a_ptr, make_tuple(M, K), make_tuple(K, 1));\n    auto b_tensor = make_naive_tensor_view(\n        b_ptr, make_tuple(K, N), make_tuple(N, 1));\n    auto c_tensor = make_naive_tensor_view(\n        c_ptr, make_tuple(M, N), make_tuple(N, 1));\n    \n    // Define tile sizes\n    constexpr index_t tile_m = 32;\n    constexpr index_t tile_n = 32;\n    constexpr index_t tile_k = 8;\n    \n    // Create distributions\n    auto a_dist = make_static_tile_distribution&lt;...&gt;();\n    auto b_dist = make_static_tile_distribution&lt;...&gt;();\n    auto c_dist = make_static_tile_distribution&lt;...&gt;();\n    \n    // Calculate tile position\n    const index_t block_m = blockIdx.y * tile_m;\n    const index_t block_n = blockIdx.x * tile_n;\n    \n    // Create tile windows\n    auto a_window = make_tile_window(\n        a_tensor,\n        make_tuple(tile_m, tile_k),\n        make_tuple(block_m, 0),\n        a_dist);\n    \n    auto b_window = make_tile_window(\n        b_tensor,\n        make_tuple(tile_k, tile_n),\n        make_tuple(0, block_n),\n        b_dist);\n    \n    auto c_window = make_tile_window(\n        c_tensor,\n        make_tuple(tile_m, tile_n),\n        make_tuple(block_m, block_n),\n        c_dist);\n    \n    // Create distributed tensors for register storage\n    auto a_reg = make_static_distributed_tensor&lt;AType&gt;(a_dist);\n    auto b_reg = make_static_distributed_tensor&lt;BType&gt;(b_dist);\n    auto c_reg = make_static_distributed_tensor&lt;CType&gt;(c_dist);\n    \n    // Initialize accumulator\n    c_reg.clear();\n    \n    // Main GEMM loop\n    for(index_t k = 0; k &lt; K; k += tile_k) {\n        // Update window positions\n        a_window.set_window_origin(make_tuple(0, k));\n        b_window.set_window_origin(make_tuple(k, 0));\n        \n        // Load tiles\n        a_window.load(a_reg);\n        b_window.load(b_reg);\n        \n        // Compute\n        gemm(a_reg, b_reg, c_reg);\n    }\n    \n    // Store result\n    c_window.store(c_reg);\n}",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#load-compute-store-pipeline",
    "href": "concepts/03_tile_window.html#load-compute-store-pipeline",
    "title": "Tile Window - Data Access Gateway",
    "section": "Load-Compute-Store Pipeline",
    "text": "Load-Compute-Store Pipeline\n\nflowchart LR\n    subgraph \"Iteration i\"\n        L1[\"Load A[i]Load B[i]\"]\n        C1[\"ComputeC += A[i] × B[i]\"]\n    end\n    \n    subgraph \"Iteration i+1\"\n        L2[\"Load A[i+1]Load B[i+1]\"]\n        C2[\"ComputeC += A[i+1] × B[i+1]\"]\n    end\n    \n    subgraph \"Final\"\n        S[\"Store C\"]\n    end\n    \n    L1 --&gt; C1\n    C1 --&gt; L2\n    L2 --&gt; C2\n    C2 --&gt; S\n    \n    style L1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style C1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style S fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#pipeline-optimization",
    "href": "concepts/03_tile_window.html#pipeline-optimization",
    "title": "Tile Window - Data Access Gateway",
    "section": "Pipeline Optimization",
    "text": "Pipeline Optimization\n\ngraph TB\n    subgraph \"Basic Pipeline\"\n        B1[\"Load\"] --&gt; B2[\"Compute\"] --&gt; B3[\"Store\"]\n        BTime[\"Time: 3 units\"]\n    end\n    \n    subgraph \"Software Pipelining\"\n        P1[\"Load[i+1]\"]\n        P2[\"Compute[i]\"]\n        P3[\"Store[i-1]\"]\n        PTime[\"Time: 1 unit per iteration\"]\n    end\n    \n    subgraph \"Double Buffering\"\n        DB1[\"Buffer A: Load\"]\n        DB2[\"Buffer B: Compute\"]\n        DBSwap[\"Swap buffers\"]\n    end\n    \n    B3 --&gt; P1\n    P1 --&gt; P2\n    P2 --&gt; P3\n    \n    P3 --&gt; DB1\n    DB1 --&gt; DB2\n    DB2 --&gt; DBSwap\n    \n    style P1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P2 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style P3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#window-configuration-space",
    "href": "concepts/03_tile_window.html#window-configuration-space",
    "title": "Tile Window - Data Access Gateway",
    "section": "Window Configuration Space",
    "text": "Window Configuration Space\n\ngraph TB\n    subgraph \"Window Parameters\"\n        Size[\"Window SizeTrade-off: registers vs reuse\"]\n        Origin[\"Window OriginStarting position\"]\n        Stride[\"Window StrideOverlap pattern\"]\n    end\n    \n    subgraph \"Performance Factors\"\n        Reg[\"Register UsageLimited resource\"]\n        Reuse[\"Data ReuseBandwidth savings\"]\n        Coal[\"CoalescingMemory efficiency\"]\n    end\n    \n    subgraph \"Optimal Configuration\"\n        Opt[\"Balance all factorsArchitecture-specific\"]\n    end\n    \n    Size --&gt; Reg\n    Size --&gt; Reuse\n    Origin --&gt; Coal\n    Stride --&gt; Reuse\n    \n    Reg --&gt; Opt\n    Reuse --&gt; Opt\n    Coal --&gt; Opt\n    \n    style Opt fill:#d1fae5,stroke:#10b981,stroke-width:3px",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#best-practices",
    "href": "concepts/03_tile_window.html#best-practices",
    "title": "Tile Window - Data Access Gateway",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Window Size Selection\n\n\n\n\n\n\n\n\n2. Access Pattern Optimization",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/03_tile_window.html#summary",
    "href": "concepts/03_tile_window.html#summary",
    "title": "Tile Window - Data Access Gateway",
    "section": "Summary",
    "text": "Summary\nTileWindow provides: - Automatic optimization: Generates optimal memory access patterns - Distribution awareness: Works seamlessly with TileDistribution - Flexible windowing: Supports various access patterns and window configurations - Zero overhead: Compile-time optimization in C++ - Safety: Automatic boundary handling\nKey benefits: 1. Performance: Achieves peak memory bandwidth through coalescing and vectorization 2. Productivity: Eliminates manual memory management code 3. Correctness: Automatic boundary checking and handling 4. Composability: Integrates cleanly with other CK abstractions\nThe TileWindow abstraction is essential for building high-performance GPU kernels, providing a clean interface for complex memory access patterns while maintaining peak performance.",
    "crumbs": [
      "Distribution API",
      "Tile Window - Data Access Gateway"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#overview",
    "href": "concepts/02_descriptors.html#overview",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "Overview",
    "text": "Overview\nA TensorDescriptor is the complete blueprint for a tensor. It combines a shape, stride information, and a series of transformations into a single object that defines exactly how a tensor’s data is laid out in memory. This guide will walk you through creating tensors, from basic layouts to complex, transformed views.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#creating-basic-tensor-layouts",
    "href": "concepts/02_descriptors.html#creating-basic-tensor-layouts",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "Creating Basic Tensor Layouts",
    "text": "Creating Basic Tensor Layouts\nYou can create descriptors for several common memory layouts.\n\n1. Custom Strides\nThe most fundamental way to define a tensor is with custom strides. This gives you full control over how many elements to “jump” in memory to move to the next item along each dimension. This is useful for creating padded layouts.\n\n\n\n\n\n\n\n\n2. Packed (Row-Major) Layout\nFor most cases, a tightly packed, row-major layout is sufficient. The strides are calculated automatically, leaving no unused space between elements.\n\n\n\n\n\n\n\n\n3. Aligned Layout\nFor GPU performance, memory layouts often need to be aligned. This function creates a row-major layout but ensures that each row’s starting address is a multiple of a given alignment value, adding padding if necessary.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#the-pipeline-concept",
    "href": "concepts/02_descriptors.html#the-pipeline-concept",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "The Pipeline Concept",
    "text": "The Pipeline Concept\nIt’s useful to think of every TensorDescriptor as a transformation pipeline. The functions above (make_naive...) create the first stage of this pipeline: they define the initial transformation that takes a simple, one-dimensional block of memory and presents it as a logical, multi-dimensional tensor view.\nThe power of the library comes from adding more stages to this pipeline to create increasingly complex layouts.\n\ngraph LR\n    subgraph \"Pipeline Stages\"\n        S1[\"Stage 1Base Layout[M, N]\"] \n        S2[\"Stage 2TransformUnmerge\"]\n        S3[\"Stage 3New View[M1, M2, N]\"]\n        S4[\"Stage NFinal View[...]\"]\n    end\n    \n    subgraph \"Same Data\"\n        D[\"Physical MemoryNo data movement\"]\n    end\n    \n    S1 --&gt; S2\n    S2 --&gt; S3\n    S3 --&gt; S4\n    \n    S1 -.-&gt; D\n    S2 -.-&gt; D\n    S3 -.-&gt; D\n    S4 -.-&gt; D\n    \n    style D fill:#ffebee,stroke:#d32f2f,stroke-width:2px\n    style S1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style S3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\nThe Initial Pipeline Stage: A Closer Look\nLet’s inspect the pipeline of a simple packed descriptor to see this first stage in action.\n\n\n\n\n\n\nAs the output shows, creating a simple [3, 4] tensor sets up a pipeline with a single transform.\n\nLower IDs (Inputs): [[0]]: This means the transform takes one input: the raw, one-dimensional memory buffer, which is always at hidden dimension ID 0.\nUpper IDs (Outputs): [[1, 2]]: This means the transform produces two outputs, which are assigned to hidden dimension IDs 1 and 2. These become the logical dimensions 0 and 1 that you interact with when you access the tensor.\n\nUnderstanding this initial stage is key to seeing how transform_tensor_descriptor later adds new stages that take these output dimensions (1 and 2) as their inputs.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#advanced-layouts-a-step-by-step-transformation",
    "href": "concepts/02_descriptors.html#advanced-layouts-a-step-by-step-transformation",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "Advanced Layouts: A Step-by-Step Transformation",
    "text": "Advanced Layouts: A Step-by-Step Transformation\nThe transform_tensor_descriptor function adds new stages to an existing descriptor’s pipeline. Let’s walk through this with a detailed example, mirroring the process of a debug script.\n\nGoal: Transform a [2, 6] Tensor into a [2, 2, 3] View\nWe will reinterpret a 2D tensor with shape [2, 6] as a 3D tensor with shape [2, 2, 3], without changing the underlying 12-element memory buffer.\n\nStep 1: Define and Analyze the Base Descriptor\nFirst, we create the [2, 6] base descriptor. As we established, this creates an initial pipeline stage.\n\n\n\n\n\n\n\n\nStep 2: Define the New Transformation Stage\nTo get from [2, 6] to [2, 2, 3], we must add a new transform for each of the base descriptor’s logical dimensions.\n\nFor logical dimension 0 (length 2): We want to preserve it, so we’ll use a PassThroughTransform.\nFor logical dimension 1 (length 6): We want to split it, so we’ll use an UnmergeTransform([2, 3]).\n\nWe wire this new stage into the pipeline using the lower and upper ID parameters, which operate on the logical dimensions of their respective descriptors (input and output).\n\n\nStep 3: Apply Transformation and Analyze the Result\nNow we apply the transform and inspect the final, complete pipeline.\n\n\n\n\n\n\n\n\n\nAnalysis of the Final Pipeline\n\ngraph TB\n    subgraph \"Transform Pipeline\"\n        T0[\"Transform 0Base UnmergeInput: [0]Output: [1,2]\"]\n        T1[\"Transform 1PassThroughInput: [1]Output: [3]\"]\n        T2[\"Transform 2UnmergeInput: [2]Output: [4,5]\"]\n    end\n    \n    subgraph \"Hidden Dimensions\"\n        H0[\"Hidden ID 0Raw Buffer\"]\n        H1[\"Hidden ID 1Dim 0 (size 2)\"]\n        H2[\"Hidden ID 2Dim 1 (size 6)\"]\n        H3[\"Hidden ID 3Final Dim 0\"]\n        H4[\"Hidden ID 4Final Dim 1\"]\n        H5[\"Hidden ID 5Final Dim 2\"]\n    end\n    \n    H0 --&gt; T0\n    T0 --&gt; H1\n    T0 --&gt; H2\n    H1 --&gt; T1\n    H2 --&gt; T2\n    T1 --&gt; H3\n    T2 --&gt; H4\n    T2 --&gt; H5\n    \n    style H0 fill:#ffebee,stroke:#d32f2f,stroke-width:2px\n    style H3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style H4 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style H5 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\nThe final debug output shows the full story of our three-stage pipeline:\n\nTransform [0] (The Base UnmergeTransform):\n\nLower hidden IDs: [[0]]: Takes the raw memory buffer as input.\nUpper hidden IDs: [[1, 2]]: Produces the two original logical dimensions, backed by hidden IDs 1 and 2.\n\nTransform [1] (Our New PassThroughTransform):\n\nLower hidden IDs: [[1]]: Correctly takes hidden ID 1 as its input. This is because we specified input logical dimension 0, which was backed by hidden ID 1.\nUpper hidden IDs: [[3]]: Produces a new output, hidden ID 3.\n\nTransform [2] (Our New UnmergeTransform):\n\nLower hidden IDs: [[2]]: Correctly takes hidden ID 2 as its input, as it was wired from logical dimension 1.\nUpper hidden IDs: [[4, 5]]: Splits its input into two new outputs, hidden IDs 4 and 5.\n\nFinal Result: The Top dimension hidden IDs of the final descriptor are [3, 4, 5]. These are the outputs of our new transforms, and they now back the final logical dimensions 0, 1, 2 of the [2, 2, 3] tensor.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#real-world-gpu-example-5d-to-3d-block-transformation",
    "href": "concepts/02_descriptors.html#real-world-gpu-example-5d-to-3d-block-transformation",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "Real-World GPU Example: 5D to 3D Block Transformation",
    "text": "Real-World GPU Example: 5D to 3D Block Transformation\nThese concepts are critical in GPU programming. This example transforms a 5D tensor representing a GPU thread block’s workload into a simpler 3D view using MergeTransform.\n\nThe logic is the same: we start with a 5D descriptor and apply new transforms (PassThrough and Merge) to its logical dimensions to produce a new 3D descriptor.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_descriptors.html#summary",
    "href": "concepts/02_descriptors.html#summary",
    "title": "Tensor Descriptors - Complete Tensor Specifications",
    "section": "Summary",
    "text": "Summary\n\nTensorDescriptor is a Pipeline: It describes transformations from a 1D buffer to a logical tensor view.\nmake_naive... Creates the First Stage: It sets up the initial transform from a buffer to a simple packed layout.\ntransform_tensor_descriptor Adds New Stages: It allows you to build complex views by adding transforms to the pipeline.\nHandle All Input Dimensions: When transforming, you must provide a new transform for each logical dimension of the input descriptor to avoid losing data.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Descriptors - Complete Tensor Specifications"
    ]
  },
  {
    "objectID": "concepts/02_swizzling_example.html",
    "href": "concepts/02_swizzling_example.html",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "This chapter demonstrates a practical application of tensor descriptors for implementing memory swizzling patterns. We’ll implement Morton ordering (also known as Z-order curve) within tiles, which is crucial for optimizing GPU memory access patterns and reducing bank conflicts.\nMemory swizzling rearranges data in memory to improve spatial locality and access patterns. Morton ordering provides a space-filling curve that maintains spatial locality while enabling efficient parallel access. This pattern is widely used in:\n\nGPU Texture Memory: Optimizing cache efficiency for 2D texture access\nMatrix Operations: Reducing memory bank conflicts in shared memory\nImage Processing: Improving locality for block-based algorithms\nScientific Computing: Enhancing data access patterns for stencil operations\n\n\n\nMorton ordering interleaves the bits of 2D coordinates to create a 1D ordering that preserves spatial locality. For a 2D coordinate (y, x), we split each coordinate into its binary bits and interleave them:\n\ny = y₁y₀ (2 bits)\nx = x₁x₀ (2 bits)\n\nMorton index = y₁x₁y₀x₀ (4 bits)\n\nThis creates a Z-shaped traversal pattern within each tile, which is exactly what MergeTransform enables us to express mathematically.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s first understand the Morton ordering pattern and why it’s beneficial:\n\n\n\n\n\n\n\n\n\nLet’s create our 8×8 test texture and visualize the data organization:\n\n\n\n\n\n\n\n\n\nFirst, we split our 8×8 texture into 4×4 tiles using tensor descriptors. This creates a hierarchical structure: (Y_blk, y_in, X_blk, x_in).\n\n\n\n\n\n\nWhy do we need the coordinate conversion code? Let’s examine what the tensor descriptor returns:\n\n\n\n\n\n\nNow let’s verify the Stage 1 coordinate transformation and see the resulting matrix:\n\n\n\n\n\n\n\n\n\nNow for the key insight: MergeTransform is essential for Morton ordering. We need to:\n\nSplit coordinates into individual bits using UnmergeTransform\nReorder and merge bits using MergeTransform to create the Morton index\n\n\n\n\n\n\n\nWhy MergeTransform is Essential for Morton Ordering:\n\n\n\n\n\n\nNow let’s attempt the tensor descriptor implementation:\n\n\n\n\n\n\n\n\n\nLet’s visualize the complete swizzling transformation with tiled matrix layouts:\n\n\n\n\n\n\n\n\n\nLet’s analyze the benefits of Morton ordering for memory access patterns using our working manual implementation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou might wonder why we need this seemingly complex coordinate conversion:\nif len(orig_coord) == 1:\n    # Single offset - convert back to 2D\n    offset = orig_coord[0]\n    orig_y = offset // W\n    orig_x = offset % W\nelse:\n    # Direct 2D coordinates\n    orig_y, orig_x = orig_coord[0], orig_coord[1]\nThe answer lies in how make_naive_tensor_descriptor_packed works:\n\n\n\n\n\n\n\n\n\n\nThis comprehensive example demonstrates both the power and complexity of tensor descriptors for memory transformations:\n\n\n\n\n\n\n\n\n\nMorton ordering is extensively used in GPU computing and high-performance applications:\n1. GPU Texture Caching: Morton-ordered textures improve cache hit rates for 2D access patterns, crucial for graphics rendering and image processing.\n2. Shared Memory Banking: GPU shared memory is organized in banks. Morton ordering reduces bank conflicts when multiple threads access nearby elements.\n3. Matrix Operations: Tiled matrix operations benefit from Morton ordering within tiles, improving memory locality for algorithms like matrix multiplication and convolution.\n4. Image Processing: Block-based algorithms (DCT, wavelet transforms, video codecs) achieve better performance with Morton-ordered data layouts.\nThe mathematical framework provided by tensor descriptors with MergeTransform enables expressing these complex swizzling patterns elegantly and systematically, even when implementation details require careful attention."
  },
  {
    "objectID": "concepts/02_swizzling_example.html#understanding-morton-ordering",
    "href": "concepts/02_swizzling_example.html#understanding-morton-ordering",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Morton ordering interleaves the bits of 2D coordinates to create a 1D ordering that preserves spatial locality. For a 2D coordinate (y, x), we split each coordinate into its binary bits and interleave them:\n\ny = y₁y₀ (2 bits)\nx = x₁x₀ (2 bits)\n\nMorton index = y₁x₁y₀x₀ (4 bits)\n\nThis creates a Z-shaped traversal pattern within each tile, which is exactly what MergeTransform enables us to express mathematically."
  },
  {
    "objectID": "concepts/02_swizzling_example.html#morton-pattern-analysis",
    "href": "concepts/02_swizzling_example.html#morton-pattern-analysis",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Let’s first understand the Morton ordering pattern and why it’s beneficial:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#setup-and-test-data",
    "href": "concepts/02_swizzling_example.html#setup-and-test-data",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Let’s create our 8×8 test texture and visualize the data organization:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#stage-1-tiling-with-unmergetransform",
    "href": "concepts/02_swizzling_example.html#stage-1-tiling-with-unmergetransform",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "First, we split our 8×8 texture into 4×4 tiles using tensor descriptors. This creates a hierarchical structure: (Y_blk, y_in, X_blk, x_in).\n\n\n\n\n\n\nWhy do we need the coordinate conversion code? Let’s examine what the tensor descriptor returns:\n\n\n\n\n\n\nNow let’s verify the Stage 1 coordinate transformation and see the resulting matrix:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#stage-2-morton-ordering-with-mergetransform",
    "href": "concepts/02_swizzling_example.html#stage-2-morton-ordering-with-mergetransform",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Now for the key insight: MergeTransform is essential for Morton ordering. We need to:\n\nSplit coordinates into individual bits using UnmergeTransform\nReorder and merge bits using MergeTransform to create the Morton index\n\n\n\n\n\n\n\nWhy MergeTransform is Essential for Morton Ordering:\n\n\n\n\n\n\nNow let’s attempt the tensor descriptor implementation:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#complete-transformation-visualization",
    "href": "concepts/02_swizzling_example.html#complete-transformation-visualization",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Let’s visualize the complete swizzling transformation with tiled matrix layouts:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#memory-access-pattern-analysis",
    "href": "concepts/02_swizzling_example.html#memory-access-pattern-analysis",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Let’s analyze the benefits of Morton ordering for memory access patterns using our working manual implementation:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#technical-notes",
    "href": "concepts/02_swizzling_example.html#technical-notes",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "You might wonder why we need this seemingly complex coordinate conversion:\nif len(orig_coord) == 1:\n    # Single offset - convert back to 2D\n    offset = orig_coord[0]\n    orig_y = offset // W\n    orig_x = offset % W\nelse:\n    # Direct 2D coordinates\n    orig_y, orig_x = orig_coord[0], orig_coord[1]\nThe answer lies in how make_naive_tensor_descriptor_packed works:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#summary-and-lessons-learned",
    "href": "concepts/02_swizzling_example.html#summary-and-lessons-learned",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "This comprehensive example demonstrates both the power and complexity of tensor descriptors for memory transformations:"
  },
  {
    "objectID": "concepts/02_swizzling_example.html#practical-applications",
    "href": "concepts/02_swizzling_example.html#practical-applications",
    "title": "Memory Swizzling with Morton Ordering",
    "section": "",
    "text": "Morton ordering is extensively used in GPU computing and high-performance applications:\n1. GPU Texture Caching: Morton-ordered textures improve cache hit rates for 2D access patterns, crucial for graphics rendering and image processing.\n2. Shared Memory Banking: GPU shared memory is organized in banks. Morton ordering reduces bank conflicts when multiple threads access nearby elements.\n3. Matrix Operations: Tiled matrix operations benefit from Morton ordering within tiles, improving memory locality for algorithms like matrix multiplication and convolution.\n4. Image Processing: Block-based algorithms (DCT, wavelet transforms, video codecs) achieve better performance with Morton-ordered data layouts.\nThe mathematical framework provided by tensor descriptors with MergeTransform enables expressing these complex swizzling patterns elegantly and systematically, even when implementation details require careful attention."
  },
  {
    "objectID": "concepts/03_sweep_tile.html",
    "href": "concepts/03_sweep_tile.html",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "",
    "text": "Sweep operations are the elegant way to iterate over distributed data. They complete the tile distribution workflow by providing clean, efficient iteration patterns that automatically handle all the complex indexing details.",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#overview",
    "href": "concepts/03_sweep_tile.html#overview",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "",
    "text": "Sweep operations are the elegant way to iterate over distributed data. They complete the tile distribution workflow by providing clean, efficient iteration patterns that automatically handle all the complex indexing details.",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#basic-sweep-mechanism",
    "href": "concepts/03_sweep_tile.html#basic-sweep-mechanism",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Basic Sweep Mechanism",
    "text": "Basic Sweep Mechanism\n\nflowchart LR\n    subgraph \"X-Tile (Reused)\"\n        XT[\"X data loaded onceStays in registers\"]\n    end\n    \n    subgraph \"Y-Sweep\"\n        Y1[\"Y position 0\"]\n        Y2[\"Y position 1\"]\n        Y3[\"Y position 2\"]\n        YN[\"Y position N\"]\n    end\n    \n    subgraph \"Computation\"\n        C[\"Process(X, Y)\"]\n    end\n    \n    XT --&gt; C\n    Y1 --&gt; C\n    Y2 --&gt; C\n    Y3 --&gt; C\n    YN --&gt; C\n    \n    style XT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style C fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n\nThe key insight: Load X data once, then sweep through Y positions while keeping X in fast memory.",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#basic-sweep-operations",
    "href": "concepts/03_sweep_tile.html#basic-sweep-operations",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Basic Sweep Operations",
    "text": "Basic Sweep Operations\nLet’s start with the simplest sweep pattern:",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#the-power-of-sweeptile",
    "href": "concepts/03_sweep_tile.html#the-power-of-sweeptile",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "The Power of SweepTile",
    "text": "The Power of SweepTile\n\n1. Memory Efficiency\n\ngraph TB\n    subgraph \"Traditional Approach\"\n        T1[\"Load X[0]\"] --&gt; P1[\"Process\"]\n        T2[\"Load Y[0]\"] --&gt; P1\n        T3[\"Load X[0]\"] --&gt; P2[\"Process\"]\n        T4[\"Load Y[1]\"] --&gt; P2\n        T5[\"Load X[0]\"] --&gt; P3[\"Process\"]\n        T6[\"Load Y[2]\"] --&gt; P3\n        Note1[\"X loaded 3 times!\"]\n    end\n    \n    subgraph \"Sweep Approach\"\n        S1[\"Load X[0]\"] --&gt; SP[\"Process withY[0], Y[1], Y[2]\"]\n        S2[\"Load Y[0,1,2]\"] --&gt; SP\n        Note2[\"X loaded once!\"]\n    end\n    \n    style Note1 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style Note2 fill:#d1fae5,stroke:#10b981,stroke-width:2px\n\n\n\n2. Code Example",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#sweep-with-computation",
    "href": "concepts/03_sweep_tile.html#sweep-with-computation",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Sweep with Computation",
    "text": "Sweep with Computation\nLet’s use sweep operations for actual computation:",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#c-implementation-pattern",
    "href": "concepts/03_sweep_tile.html#c-implementation-pattern",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "C++ Implementation Pattern",
    "text": "C++ Implementation Pattern\n// Sweep pattern for matrix multiplication\ntemplate&lt;typename ADataType, typename BDataType, typename CDataType&gt;\n__device__ void gemm_sweep_tile(\n    const TileDistribution& dist_a,\n    const TileDistribution& dist_b,\n    TileDistribution& dist_c)\n{\n    // Phase 1: Load A tile into registers (X dimension)\n    auto a_tile = make_distributed_tensor(dist_a);\n    a_tile.load();  // Load once, reuse many times\n    \n    // Phase 2: Create sweep for B (Y dimension)\n    auto b_sweep = make_sweep_tile(dist_b);\n    \n    // Phase 3: Sweep through B positions\n    sweep_tile(b_sweep, [&](auto b_slice) {\n        // b_slice is current Y position data\n        \n        // Compute C = A * B for this Y position\n        auto c_slice = make_distributed_tensor(dist_c);\n        \n        // Actual computation\n        gemm(a_tile, b_slice, c_slice);\n        \n        // Store result\n        c_slice.store();\n    });\n}",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#sweep-patterns",
    "href": "concepts/03_sweep_tile.html#sweep-patterns",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Sweep Patterns",
    "text": "Sweep Patterns\n\nPattern 1: Simple Linear Sweep\n\ngraph LR\n    subgraph \"Linear Sweep\"\n        S0[\"Start\"] --&gt; P0[\"Y[0]\"]\n        P0 --&gt; P1[\"Y[1]\"]\n        P1 --&gt; P2[\"Y[2]\"]\n        P2 --&gt; P3[\"Y[3]\"]\n        P3 --&gt; E[\"End\"]\n    end\n    \n    style S0 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style E fill:#ffebee,stroke:#d32f2f,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#performance-characteristics",
    "href": "concepts/03_sweep_tile.html#performance-characteristics",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Performance Characteristics",
    "text": "Performance Characteristics\n\ngraph TB\n    subgraph \"Sweep Performance Benefits\"\n        B1[\"Zero runtime overheadCompile-time unrolling\"]\n        B2[\"Perfect memory coalescingSequential access patterns\"]\n        B3[\"Automatic vectorizationCompiler optimizations\"]\n        B4[\"Register reuseX data stays in VGPR\"]\n    end\n    \n    subgraph \"Use Cases\"\n        U1[\"Matrix MultiplicationReuse A columns\"]\n        U2[\"ConvolutionReuse filter weights\"]\n        U3[\"ReductionAccumulate over Y\"]\n        U4[\"BroadcastApply X to all Y\"]\n    end\n    \n    B1 --&gt; Performance[\"High Performance\"]\n    B2 --&gt; Performance\n    B3 --&gt; Performance\n    B4 --&gt; Performance\n    \n    Performance --&gt; U1\n    Performance --&gt; U2\n    Performance --&gt; U3\n    Performance --&gt; U4\n    \n    style Performance fill:#d1fae5,stroke:#10b981,stroke-width:3px\n\ndef complete_tile_processing(input_data, window_origin, window_size, operation_name, compute_func):\n    \"\"\"Complete tile processing with sweep operations.\"\"\"\n    print(f\"🚀 {operation_name} Processing\")\n    print(f\"   Input: {input_data.shape} matrix\")\n    print(f\"   Window: {window_size} at {window_origin}\")\n    print()\n    \n    # 1. Setup tensor infrastructure\n    tensor_desc = make_naive_tensor_descriptor_packed(list(input_data.shape))\n    input_view = make_tensor_view(input_data, tensor_desc)\n    \n    # 2. Create distribution\n    encoding = make_tile_distribution_encoding(\n        rs_lengths=[],\n        hs_lengthss=[[window_size[0]], [window_size[1]]],\n        ps_to_rhss_major=[[], []],\n        ps_to_rhss_minor=[[], []],\n        ys_to_rhs_major=[1, 2],\n        ys_to_rhs_minor=[0, 0]\n    )\n    distribution = make_static_tile_distribution(encoding)\n    \n    # 3. Create window and load data - automatically creates distributed tensor!\n    input_window = make_tile_window(input_view, window_size, window_origin, distribution)\n    distributed_input = input_window.load()\n    \n    # 4. Create output window and load for direct processing\n    output_data = input_data.copy()\n    output_view = make_tensor_view(output_data, tensor_desc)\n    output_window = make_tile_window(output_view, window_size, window_origin, distribution)\n    distributed_output = output_window.load()\n    \n    def process_with_sweep(y_indices):\n        \"\"\"Process each element using sweep.\"\"\"\n        input_val = distributed_input.get_element(y_indices)\n        output_val = compute_func(input_val)\n        distributed_output.set_element(y_indices, output_val)\n        print(f\"    Y{y_indices}: {input_val} → {output_val}\")\n    \n    print(\"📊 Processing elements with sweep:\")\n    sweep_tile(distributed_input, process_with_sweep)\n    \n    # 5. Store results back\n    output_window.store(distributed_output)\n    \n    return output_data\n\n# Test the complete workflow\ntest_data = np.array([[1, 2, 3, 4], \n                     [5, 6, 7, 8], \n                     [9, 10, 11, 12], \n                     [13, 14, 15, 16]], dtype=np.float32)\n\nprint(\"Original data:\")\nprint(test_data)\nprint()\n\n# Process a 2x2 window with different operations\nresult1 = complete_tile_processing(test_data, [1, 1], [2, 2], \"Square\", lambda x: x ** 2)\nprint(\"\\nAfter square operation:\")\nprint(result1)\nprint()\n\nresult2 = complete_tile_processing(test_data, [0, 2], [2, 2], \"Multiply by 10\", lambda x: x * 10)\nprint(\"\\nAfter multiply by 10 operation:\")\nprint(result2)",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#advanced-sweep-features",
    "href": "concepts/03_sweep_tile.html#advanced-sweep-features",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Advanced Sweep Features",
    "text": "Advanced Sweep Features\n\n1. Conditional Sweep\n\n\n\n\n\n\n\n\n2. Multi-Dimensional Sweep\n// C++ example: 3D sweep for tensor operations\ntemplate&lt;typename XTensor, typename YTensor&gt;\n__device__ void tensor_3d_sweep(\n    const XTensor& x_tensor,    // 2D slice loaded\n    const YTensor& y_tensor)    // 3D tensor to sweep\n{\n    // Sweep through depth dimension\n    constexpr auto depth = y_tensor.get_shape()[2];\n    \n    static_for&lt;0, depth, 1&gt;{}([&](auto z) {\n        // Get 2D slice at depth z\n        auto y_slice = y_tensor.get_slice(make_tuple(_, _, z));\n        \n        // Process x_tensor with y_slice\n        auto result = tensor_op(x_tensor, y_slice);\n        \n        // Store or accumulate result\n        store_result(result, z);\n    });\n}\n\n\n3. Sweep with Accumulation",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#integration-with-tile-distribution",
    "href": "concepts/03_sweep_tile.html#integration-with-tile-distribution",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Integration with Tile Distribution",
    "text": "Integration with Tile Distribution\n\nflowchart TB\n    subgraph \"Complete Workflow\"\n        TD[\"TileDistributionDefine data layout\"]\n        TW[\"TileWindowCreate view\"]\n        DT[\"DistributedTensorLoad X data\"]\n        ST[\"SweepTileIterate Y positions\"]\n        R[\"ResultsStore outputs\"]\n    end\n    \n    TD --&gt; TW\n    TW --&gt; DT\n    DT --&gt; ST\n    ST --&gt; R\n    \n    style TD fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style ST fill:#fff3e0,stroke:#f57c00,stroke-width:3px\n    style R fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#summary",
    "href": "concepts/03_sweep_tile.html#summary",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Summary",
    "text": "Summary\nSweepTile provides: - Efficiency: Load once, use many times - Simplicity: Clean iteration abstraction - Performance: Zero overhead, perfect patterns - Flexibility: Various sweep patterns for different algorithms\nKey benefits: 1. Memory bandwidth: Optimal reuse of loaded data 2. Register pressure: Keep hot data in fastest memory 3. Code clarity: Express algorithms naturally 4. Compiler optimization: Enable aggressive optimizations\nThe sweep pattern is fundamental to high-performance GPU kernels, turning complex iteration patterns into simple, efficient operations. print(“⚖️ Sweep Pattern Comparison”) print(“=” * 35)",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/03_sweep_tile.html#key-takeaways",
    "href": "concepts/03_sweep_tile.html#key-takeaways",
    "title": "Sweep Tile - Elegant Iteration",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSweep operations complete the tile distribution story with elegant iteration patterns:\n1. Elegant Iteration\n\n✅ Lambda-based processing functions\n✅ Automatic handling of all Y indices\n✅ No manual loops or complex indexing\n\n2. Error-Free Processing\n\n✅ Impossible to miss elements\n✅ No index calculation errors\n✅ Consistent iteration patterns\n\n3. Flexible Patterns\n\n✅ Simple element processing\n✅ Conditional operations\n✅ Accumulation and reduction\n✅ Complex computation workflows\n\n4. Performance Optimization\n\n✅ Compiler-friendly iteration patterns\n✅ Optimal memory access sequences\n✅ Hardware-aware processing\n\n5. Complete Workflow Integration\n\n✅ Seamless integration with TileDistribution\n✅ Perfect pairing with TileWindow\n✅ Enables complete load → sweep → compute → store patterns\n\nSweep operations are the final piece that makes distributed tensor processing both elegant and efficient. With TileDistribution, TileWindow, and Sweep operations, you have the complete toolkit for high-performance GPU computing!",
    "crumbs": [
      "Distribution API",
      "Sweep Tile - Elegant Iteration"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#overview",
    "href": "concepts/05_static_distributed_tensor.html#overview",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Overview",
    "text": "Overview\nNow that you understand how encodings create the transformation machinery, let’s examine the data structures that hold the actual computation data: Static Distributed Tensors.\nThese are the thread-local containers that hold each thread’s portion of the distributed computation. They’re “static” because their layout is determined at compile time for maximum performance.",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#what-is-a-static-distributed-tensor",
    "href": "concepts/05_static_distributed_tensor.html#what-is-a-static-distributed-tensor",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "What is a Static Distributed Tensor?",
    "text": "What is a Static Distributed Tensor?\nBefore diving into the implementation, let’s understand what problem these tensors solve:\nThe Challenge: Each thread needs to store and access its portion of the distributed data efficiently. The access patterns are known at compile time, so we can optimize the layout.\nThe Solution: Static Distributed Tensors are thread-local data containers with compile-time optimized layouts.\n🎯 Key Properties: - Each thread has its own StaticDistributedTensor - Contains only the data that thread needs - Layout optimized for the thread’s access patterns - Provides efficient element access via Y coordinates - Memory is organized according to tile distribution\n🔍 Comparison with Traditional Tensors: - Traditional tensor: Contains all data, shared access - Distributed tensor: Data split across threads - Static distributed tensor: Thread-local portion with compile-time optimized layout",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#tensor-creation-process",
    "href": "concepts/05_static_distributed_tensor.html#tensor-creation-process",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Tensor Creation Process",
    "text": "Tensor Creation Process\nStatic distributed tensors are created from tile distributions and provide the storage for thread-local computations:",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#thread-buffer-organization",
    "href": "concepts/05_static_distributed_tensor.html#thread-buffer-organization",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Thread Buffer Organization",
    "text": "Thread Buffer Organization\nEach thread’s buffer is organized to efficiently store the elements in its tile.\n\ngraph TB\n    subgraph \"Static Distributed Tensor Structure\"\n        subgraph \"Thread 0\"\n            T0B[\"Buffer[4 elements]Contiguous\"]\n            T0Y[\"Y-coordinates[0,0] [0,1][1,0] [1,1]\"]\n            T0M[\"MappingY→BufferLinearization\"]\n        end\n        \n        subgraph \"Thread 1\"\n            T1B[\"Buffer[4 elements]Contiguous\"]\n            T1Y[\"Y-coordinates[0,0] [0,1][1,0] [1,1]\"]\n            T1M[\"MappingY→BufferLinearization\"]\n        end\n        \n        subgraph \"Thread N\"\n            TNB[\"Buffer[4 elements]Contiguous\"]\n            TNY[\"Y-coordinates[0,0] [0,1][1,0] [1,1]\"]\n            TNM[\"MappingY→BufferLinearization\"]\n        end\n    end\n    \n    T0Y --&gt; T0M\n    T0M --&gt; T0B\n    T1Y --&gt; T1M\n    T1M --&gt; T1B\n    TNY --&gt; TNM\n    TNM --&gt; TNB\n    \n    style T0B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style T1B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style TNB fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n📊 Buffer Layout Principles: - Contiguous memory for cache efficiency - Y coordinates provide logical indexing - Buffer positions provide physical indexing - Layout optimized for thread’s access patterns\n📝 Example: [2, 2] Thread Tile\nY coordinate to buffer position mapping: - Y=[0,0] → Buffer position 0 - Y=[0,1] → Buffer position 1 - Y=[1,0] → Buffer position 2 - Y=[1,1] → Buffer position 3\n🔄 Visual Layout:\nLogical (Y coordinates)    Physical (Buffer)\nY[0,0] Y[0,1]              Buffer[0] Buffer[1]\nY[1,0] Y[1,1]           →  Buffer[2] Buffer[3]",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#element-access-patterns",
    "href": "concepts/05_static_distributed_tensor.html#element-access-patterns",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Element Access Patterns",
    "text": "Element Access Patterns\nStatic distributed tensors provide efficient element access using Y coordinates.\n🎯 Access Methods: - get_element(y_indices): Read value at Y coordinate - set_element(y_indices, value): Write value at Y coordinate - Y coordinates are logical (within thread’s tile) - Internally maps to efficient buffer access\n📝 Element Access Example ([2, 2] tile):\nSetting values: - tensor.set_element([0, 0], 11) - tensor.set_element([0, 1], 12) - tensor.set_element([1, 0], 21) - tensor.set_element([1, 1], 22)\nReading values: - tensor.get_element([0, 0]) → 11 - tensor.get_element([0, 1]) → 12 - tensor.get_element([1, 0]) → 21 - tensor.get_element([1, 1]) → 22\n🚀 Performance Benefits: - Y coordinate lookup is O(1) - Buffer access is cache-friendly - No bounds checking needed (static layout) - Compiler can optimize access patterns",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#memory-layout-details",
    "href": "concepts/05_static_distributed_tensor.html#memory-layout-details",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Memory Layout Details",
    "text": "Memory Layout Details\nLet’s examine the internal memory organization in detail.\n\ngraph TB\n    subgraph \"Memory Layout Example: [3,2] Tile\"\n        subgraph \"Logical View (Y-coordinates)\"\n            L00[\"Y[0,0]\"]\n            L01[\"Y[0,1]\"]\n            L10[\"Y[1,0]\"]\n            L11[\"Y[1,1]\"]\n            L20[\"Y[2,0]\"]\n            L21[\"Y[2,1]\"]\n        end\n        \n        subgraph \"Physical Memory (Buffer)\"\n            P0[\"Addr 0Val: Y[0,0]\"]\n            P1[\"Addr 1Val: Y[0,1]\"]\n            P2[\"Addr 2Val: Y[1,0]\"]\n            P3[\"Addr 3Val: Y[1,1]\"]\n            P4[\"Addr 4Val: Y[2,0]\"]\n            P5[\"Addr 5Val: Y[2,1]\"]\n        end\n        \n        subgraph \"Address Calculation\"\n            AC[\"addr = y0 * 2 + y1(row-major)\"]\n        end\n    end\n    \n    L00 --&gt; P0\n    L01 --&gt; P1\n    L10 --&gt; P2\n    L11 --&gt; P3\n    L20 --&gt; P4\n    L21 --&gt; P5\n    \n    L00 --&gt; AC\n    L10 --&gt; AC\n    L20 --&gt; AC\n    AC --&gt; P0\n    AC --&gt; P2\n    AC --&gt; P4\n    \n    style L00 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L01 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L10 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L11 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L20 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L21 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P0 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P1 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P2 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P3 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P4 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style P5 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style AC fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n🗃️ Example: [3, 2] tile (6 elements)\nMemory organization (row-major within tile):\n📊 Physical Memory Layout: - Address 0: Y=[0,0] → Value at position 0 - Address 1: Y=[0,1] → Value at position 1 - Address 2: Y=[1,0] → Value at position 2 - Address 3: Y=[1,1] → Value at position 3 - Address 4: Y=[2,0] → Value at position 4 - Address 5: Y=[2,1] → Value at position 5\n🔄 Address Calculation: For Y[y0, y1] in row-major layout:\naddress = y0 * width + y1\nwhere width = tile_size[1]\n📝 Example Calculations: - Y[0,0] → address 0 - Y[0,1] → address 1 - Y[1,0] → address 2 - Y[1,1] → address 3 - Y[2,0] → address 4 - Y[2,1] → address 5\n💡 Layout Benefits: - Sequential access patterns are cache-friendly - Address calculation is simple and fast - Memory utilization is optimal - Vectorization opportunities are maximized",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#thread-coordination",
    "href": "concepts/05_static_distributed_tensor.html#thread-coordination",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Thread Coordination",
    "text": "Thread Coordination\nStatic distributed tensors enable efficient thread coordination.\n🤝 Coordination Mechanisms: - Each thread has its own tensor instance - Threads coordinate through shared memory - Synchronization points ensure data consistency - Load balancing through work distribution\n📝 Example: [2, 2] threads, [2, 2] tiles\nThread coordination pattern: - Thread 0 (P=[0,0]): Has 4 elements, tensor size [2, 2], coordinates Y[0,0] to Y[1,1] - Thread 1 (P=[0,1]): Has 4 elements, tensor size [2, 2], coordinates Y[0,0] to Y[1,1] - Thread 2 (P=[1,0]): Has 4 elements, tensor size [2, 2], coordinates Y[0,0] to Y[1,1] - Thread 3 (P=[1,1]): Has 4 elements, tensor size [2, 2], coordinates Y[0,0] to Y[1,1]\n🔄 Coordination Benefits: - Each thread works independently on its tensor - No contention for shared data structures - Synchronization only at coordination points - Scales efficiently with thread count",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#practical-usage-patterns",
    "href": "concepts/05_static_distributed_tensor.html#practical-usage-patterns",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Practical Usage Patterns",
    "text": "Practical Usage Patterns\nStatic distributed tensors follow a common usage pattern in practice.\n🎯 Common Usage Pattern: 1. Create tensor from tile distribution 2. Load data into tensor (from global memory) 3. Perform computations on tensor elements 4. Store results back to global memory\n📝 Conceptual Example:\n# Step 1: Create tensor\ntensor = make_static_distributed_tensor(distribution, dtype)\n\n# Step 2: Load data\nfor y in all_y_coordinates:\n    value = load_from_global_memory(p_coord, y_coord)\n    tensor.set_element(y, value)\n\n# Step 3: Compute\nfor y in all_y_coordinates:\n    value = tensor.get_element(y)\n    result = compute_function(value)\n    tensor.set_element(y, result)\n\n# Step 4: Store\nfor y in all_y_coordinates:\n    value = tensor.get_element(y)\n    store_to_global_memory(p_coord, y_coord, value)\n💡 Pattern Benefits: - Clear separation of load/compute/store phases - Optimal memory access patterns - Easy to reason about and debug - Compiler can optimize each phase",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#testing-your-understanding",
    "href": "concepts/05_static_distributed_tensor.html#testing-your-understanding",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Testing Your Understanding",
    "text": "Testing Your Understanding\nLet’s verify your understanding of static distributed tensors:",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/05_static_distributed_tensor.html#key-takeaways",
    "href": "concepts/05_static_distributed_tensor.html#key-takeaways",
    "title": "Static Distributed Tensor - Thread-Local Data Containers",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nStatic distributed tensors are the efficient data containers that make tile distribution practical:\n🎯 Core Concepts:\n\nThread-Local Storage: Each thread has its own tensor\n\n✅ No contention between threads\n✅ Optimal memory access patterns\n✅ Independent computation capability\n✅ Efficient coordination mechanisms\n\nCompile-Time Optimization: Layout determined at compile time\n\n✅ No runtime overhead for layout decisions\n✅ Optimal memory organization\n✅ Maximum compiler optimization opportunities\n✅ Predictable performance characteristics\n\nEfficient Element Access: Y coordinates provide logical indexing\n\n✅ O(1) element access time\n✅ Cache-friendly memory patterns\n✅ No bounds checking overhead\n✅ Vectorization opportunities\n\n\n🔧 Implementation Benefits:\n\nMemory Efficiency: Only stores data the thread needs\nCache Performance: Contiguous memory layout optimized for access patterns\nScalability: Performance scales with thread count\nSimplicity: Clean programming model with logical coordinates\n\n💡 Why This Matters:\n\nPerformance: Optimal memory access patterns for GPU hardware\nProductivity: Easy to reason about and debug\nMaintainability: Clear separation between logical and physical layout\nExtensibility: Same pattern works for any tile distribution strategy\n\nStatic distributed tensors show how CK achieves both programming simplicity and maximum performance. The logical Y coordinate interface hides the complexity of optimal memory layout, giving you the best of both worlds!",
    "crumbs": [
      "Implementation Deep Dive",
      "Static Distributed Tensor - Thread-Local Data Containers"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html",
    "href": "concepts/02_transforms.html",
    "title": "Individual Transforms",
    "section": "",
    "text": "The transformation engine is built from individual transform types that each handle specific coordinate conversions. Understanding these building blocks is essential for mastering the tile distribution system.",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#interactive-exploration",
    "href": "concepts/02_transforms.html#interactive-exploration",
    "title": "Individual Transforms",
    "section": "🎮 Interactive Exploration",
    "text": "🎮 Interactive Exploration\nExplore transformation concepts interactively:\nTensor Transform Visualizer - Explore tensor descriptor transformations with visual graphs and mathematical formulas. See how data layouts change through various transformations.",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#what-are-transforms",
    "href": "concepts/02_transforms.html#what-are-transforms",
    "title": "Individual Transforms",
    "section": "What Are Transforms?",
    "text": "What Are Transforms?\nEach transform type converts coordinates between different spaces:\n\nForward: Upper coordinates → Lower coordinates\n\nBackward: Lower coordinates → Upper coordinates\n\n\nTransform System Architecture\n\ngraph TB\n    subgraph \"Coordinate Spaces\"\n        US[\"Upper SpaceLogical Coordinates\"]\n        LS[\"Lower SpacePhysical Coordinates\"]\n    end\n    \n    subgraph \"Transform Types\"\n        EMB[\"EmbedTransformShape + Strides\"]\n        UNM[\"UnmergeTransformMulti-D → Linear\"]\n        MRG[\"MergeTransformLinear → Multi-D\"]\n        REP[\"ReplicateTransformBroadcast\"]\n        OFF[\"OffsetTransformTranslation\"]\n        PAS[\"PassThroughTransformIdentity\"]\n        PAD[\"PadTransformBoundaries\"]\n    end\n    \n    subgraph \"Operations\"\n        FWD[\"Forwardcalculate_lower_index()\"]\n        BWD[\"Backwardcalculate_upper_index()\"]\n        UPD[\"Updateupdate_lower_index()\"]\n    end\n    \n    US --&gt;|Transform| LS\n    LS --&gt;|Inverse| US\n    \n    EMB --&gt; FWD\n    UNM --&gt; FWD\n    MRG --&gt; FWD\n    REP --&gt; FWD\n    OFF --&gt; FWD\n    PAS --&gt; FWD\n    PAD --&gt; FWD\n    \n    style US fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style LS fill:#fff3e0,stroke:#f57c00,stroke-width:3px\n    style FWD fill:#e8f5e9,stroke:#388e3c,stroke-width:2px",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#embedtransform---view-into-larger-space",
    "href": "concepts/02_transforms.html#embedtransform---view-into-larger-space",
    "title": "Individual Transforms",
    "section": "1. EmbedTransform - View into Larger Space",
    "text": "1. EmbedTransform - View into Larger Space\nThe most fundamental transform - creates a view of shape into a larger buffer.\n\ngraph LR\n    subgraph \"Upper Space\"\n        U[\"Shape: [2, 3]Coords: (0,0) to (1,2)\"]\n    end\n    \n    subgraph \"Lower Space\"\n        L[\"Buffer: 24 elementsStrides: [12, 1]\"]\n    end\n    \n    subgraph \"Mapping\"\n        M[\"(i,j) → i*12 + j*1\"]\n    end\n    \n    U --&gt; M\n    M --&gt; L\n    \n    style U fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create embed transform for 2x3 matrix with strides [12, 1]\nauto transform = make_embed_transform(\n    make_tuple(2, 3),      // upper lengths\n    make_tuple(12, 1)      // strides/coefficients\n);\n\n// Forward transformation: 2D → 1D\nmulti_index&lt;2&gt; upper_coord{1, 2};  // Row 1, Column 2\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 1*12 + 2*1 = 14\n\n// The embed transform inherits from base_transform&lt;1, UpLengths::size()&gt;\n// meaning 1 lower dimension → multiple upper dimensions\n\n// Usage in tensor descriptor\nauto desc = make_naive_tensor_descriptor(\n    make_tuple(number&lt;3&gt;{}, number&lt;4&gt;{}),   // shape\n    make_tuple(number&lt;8&gt;{}, number&lt;1&gt;{})    // strides\n);\n// Internally uses EmbedTransform for strided access",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#unmergetransform---multi-d-to-linear",
    "href": "concepts/02_transforms.html#unmergetransform---multi-d-to-linear",
    "title": "Individual Transforms",
    "section": "2. UnmergeTransform - Multi-D to Linear",
    "text": "2. UnmergeTransform - Multi-D to Linear\nConverts multi-dimensional coordinates to linear (1D) coordinates.\n\ngraph LR\n    subgraph \"Upper (3D)\"\n        U1[\"Shape: [3, 4, 2]Coord: (1, 3, 0)\"]\n    end\n    \n    subgraph \"Lower (1D)\"\n        L1[\"Linear index: 14\"]\n    end\n    \n    subgraph \"Calculation\"\n        C[\"14 = 1×(4×2) + 3×2 + 014 = 8 + 6 + 0\"]\n    end\n    \n    U1 --&gt; C\n    C --&gt; L1\n    \n    style U1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create unmerge transform for 3x4x2 tensor\nauto transform = make_unmerge_transform(\n    make_tuple(3, 4, 2)  // upper lengths\n);\n\n// The unmerge transform inherits from base_transform&lt;1, UpLengths::size()&gt;\n// meaning 1 lower dimension → multiple upper dimensions\n\n// Forward: Multi-D → Linear\nmulti_index&lt;3&gt; upper_coord{1, 3, 0};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 1*(4*2) + 3*2 + 0 = 14\n\n// Backward: Linear → Multi-D (unpacking)\nmulti_index&lt;1&gt; packed_idx{14};\nmulti_index&lt;3&gt; unpacked_coord;\n// This would compute: unpacked_coord = [1, 3, 0]\n\n// Use in tensor descriptor\nauto desc = make_naive_tensor_descriptor_packed(\n    make_tuple(number&lt;3&gt;{}, number&lt;4&gt;{}, number&lt;2&gt;{})\n);\n// UnmergeTransform is used internally for packed layouts",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#mergetransform---linear-to-multi-d",
    "href": "concepts/02_transforms.html#mergetransform---linear-to-multi-d",
    "title": "Individual Transforms",
    "section": "3. MergeTransform - Linear to Multi-D",
    "text": "3. MergeTransform - Linear to Multi-D\nInverse of UnmergeTransform - expands linear coordinates to multi-dimensional.\n\ngraph LR\n    subgraph \"Upper (1D)\"\n        U1[\"Linear index: 13\"]\n    end\n    \n    subgraph \"Lower (2D)\"\n        L1[\"Shape: [4, 5]Coord: (2, 3)\"]\n    end\n    \n    subgraph \"Calculation\"\n        C[\"13 = 2×5 + 3row=13÷5=2, col=13%5=3\"]\n    end\n    \n    U1 --&gt; C\n    C --&gt; L1\n    \n    style U1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create merge transform for 4x5 tensor\nauto transform = make_merge_transform(\n    make_tuple(4, 5)  // lower lengths\n);\n\n// The merge transform inherits from base_transform&lt;LowLengths::size(), 1&gt;\n// meaning multiple lower dimensions → 1 upper dimension\n\n// Forward: Linear → Multi-D (splitting)\nmulti_index&lt;1&gt; upper_coord{13};\nmulti_index&lt;2&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 13 / 5 = 2 (row)\n//         lower_coord[1] = 13 % 5 = 3 (col)\n\n// CK provides two merge implementations:\n// 1. merge_v2_magic_division (default) - uses magic number division\n// 2. merge_v3_division_mod - for power-of-2 dimensions\n\n// Common usage: dimension reduction\nauto desc = transform_tensor_descriptor(\n    input_desc,\n    make_tuple(make_merge_transform(make_tuple(M, N))),\n    make_tuple(sequence&lt;0, 1&gt;{}),  // merge dims 0,1\n    make_tuple(sequence&lt;0&gt;{})      // to single dim 0\n);",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#replicatetransform---broadcasting",
    "href": "concepts/02_transforms.html#replicatetransform---broadcasting",
    "title": "Individual Transforms",
    "section": "4. ReplicateTransform - Broadcasting",
    "text": "4. ReplicateTransform - Broadcasting\nBroadcasts coordinates by ignoring certain dimensions.\n\ngraph TB\n    subgraph \"Broadcasting Example\"\n        U2D[\"Upper: [3, 4](2D matrix)\"]\n        L1D[\"Lower: [4](1D vector)\"]\n        B[\"Broadcast:All rows see same vector\"]\n    end\n    \n    U2D --&gt;|\"Ignore dim 0\"| L1D\n    \n    style U2D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L1D fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create replicate transform for broadcasting\nauto transform = make_replicate_transform(\n    make_tuple(3, 4)  // upper lengths\n);\n\n// The replicate transform inherits from base_transform&lt;0, UpLengths::size()&gt;\n// meaning 0 lower dimensions → multiple upper dimensions\n\n// All upper coordinates map to empty lower space\nmulti_index&lt;2&gt; coord1{0, 0};\nmulti_index&lt;2&gt; coord2{2, 3};\nmulti_index&lt;0&gt; lower1, lower2;\ntransform.calculate_lower_index(lower1, coord1);\ntransform.calculate_lower_index(lower2, coord2);\n// Both lower1 and lower2 are empty (no lower dimensions)\n\n// Common usage: broadcasting in GEMM\nauto broadcast_desc = transform_tensor_descriptor(\n    scalar_desc,  // 0D tensor\n    make_tuple(make_replicate_transform(make_tuple(M, N))),\n    make_tuple(sequence&lt;&gt;{}),     // no lower dims\n    make_tuple(sequence&lt;0, 1&gt;{})  // create 2 upper dims\n);",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#offsettransform---translation",
    "href": "concepts/02_transforms.html#offsettransform---translation",
    "title": "Individual Transforms",
    "section": "5. OffsetTransform - Translation",
    "text": "5. OffsetTransform - Translation\nShifts coordinates by a fixed offset - useful for padding and boundaries.",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#passthroughtransform---identity",
    "href": "concepts/02_transforms.html#passthroughtransform---identity",
    "title": "Individual Transforms",
    "section": "6. PassThroughTransform - Identity",
    "text": "6. PassThroughTransform - Identity\nNo-op transform that passes coordinates unchanged.\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create pass-through transform\nauto transform = make_pass_through_transform(60);  // low_length\n\n// The pass_through transform inherits from base_transform&lt;1, 1&gt;\n// meaning 1 lower dimension → 1 upper dimension (identity)\n\n// Forward transformation: no change\nmulti_index&lt;1&gt; upper_coord{25};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = upper_coord[0] = 25\n\n// Often optimized away at compile time\n// Used as placeholder in transform chains",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#padtransform---smart-boundaries",
    "href": "concepts/02_transforms.html#padtransform---smart-boundaries",
    "title": "Individual Transforms",
    "section": "7. PadTransform - Smart Boundaries",
    "text": "7. PadTransform - Smart Boundaries\nHandles out-of-bounds access with padding values.",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#transform-composition",
    "href": "concepts/02_transforms.html#transform-composition",
    "title": "Individual Transforms",
    "section": "Transform Composition",
    "text": "Transform Composition\nTransforms can be chained to create complex coordinate mappings:\n\ngraph LR\n    subgraph \"Transform Chain Example\"\n        I[\"Input Coord(1, 2)\"]\n        T1[\"EmbedStrides [10, 1]\"]\n        T2[\"Offset+20\"]\n        O[\"Output Index32\"]\n    end\n    \n    I --&gt; T1\n    T1 --&gt;|\"12\"| T2\n    T2 --&gt; O\n    \n    subgraph \"Calculation\"\n        C[\"1×10 + 2×1 = 1212 + 20 = 32\"]\n    end\n    \n    style I fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style O fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style C fill:#f3f4f6,stroke:#6b7280,stroke-width:1px",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#performance-considerations",
    "href": "concepts/02_transforms.html#performance-considerations",
    "title": "Individual Transforms",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\ngraph TB\n    subgraph \"Transform Costs\"\n        Pass[\"PassThroughZero cost\"]\n        Off[\"OffsetAddition only\"]\n        Emb[\"EmbedMultiply + Add\"]\n        Merge[\"Merge/UnmergeMultiple ops\"]\n        Pad[\"PadBounds check\"]\n    end\n    \n    Pass --&gt;|Fastest| Performance\n    Off --&gt; Performance\n    Emb --&gt; Performance\n    Merge --&gt; Performance\n    Pad --&gt;|Slowest| Performance\n    \n    Performance[\"Performance Impact\"]\n    \n    style Pass fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Pad fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n\n\nC++ Transform Chain Example\n// Complex transform composition in CK\n// Transforms are composed through tensor descriptors\n\n// Create base descriptor\nauto base_desc = make_naive_tensor_descriptor(\n    make_tuple(number&lt;3&gt;{}, number&lt;3&gt;{}),  // shape\n    make_tuple(number&lt;3&gt;{}, number&lt;1&gt;{})   // strides\n);\n\n// Apply padding transform\nauto padded_desc = transform_tensor_descriptor(\n    base_desc,\n    make_tuple(make_pad_transform(number&lt;3&gt;{}, number&lt;1&gt;{}, number&lt;1&gt;{}),  // dim 0\n               make_pad_transform(number&lt;3&gt;{}, number&lt;1&gt;{}, number&lt;1&gt;{})), // dim 1\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),      // lower dim mapping\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{})       // upper dim mapping\n);\n\n// CK composes transforms at compile time for zero overhead!\n// The resulting descriptor incorporates all transformations\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create pad transform with left and right padding\nauto transform = make_pad_transform(\n    3,   // low_length (original dimension)\n    1,   // left_pad\n    1    // right_pad\n);\n\n// The pad transform inherits from base_transform&lt;1, 1&gt;\n// Total upper length = low_length + left_pad + right_pad = 5\n\n// Forward transformation: adjust for padding\nmulti_index&lt;1&gt; upper_coord{1};  // First valid element (after left pad)\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = upper_coord[0] - left_pad = 1 - 1 = 0\n\n// Check if coordinate is in valid region\nbool is_valid = transform.is_valid_upper_index_mapped_to_valid_lower_index(upper_coord);\n\n// Variants available:\nauto left_pad_only = make_left_pad_transform(3, 1);   // Only left padding\nauto right_pad_only = make_right_pad_transform(3, 1); // Only right padding",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#additional-transforms-in-composable-kernel",
    "href": "concepts/02_transforms.html#additional-transforms-in-composable-kernel",
    "title": "Individual Transforms",
    "section": "Additional Transforms in Composable Kernel",
    "text": "Additional Transforms in Composable Kernel\n\n8. XorTransform - 2D XOR Mapping\n// From composable_kernel - special 2D coordinate transform\nauto transform = make_xor_transform(make_tuple(4, 8));  // 4x8 dimensions\n\n// The xor_t transform inherits from base_transform&lt;2, 2&gt;\n// Special mapping: lower[1] = upper[1] ^ (upper[0] % lengths[1])\n\nmulti_index&lt;2&gt; upper_coord{2, 5};\nmulti_index&lt;2&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower[0] = 2, lower[1] = 5 ^ (2 % 8) = 5 ^ 2 = 7\n\n// Used for specialized memory access patterns in some algorithms\n\n\n9. SliceTransform - Extract Sub-region\n// From composable_kernel - extract a slice from a dimension\nauto transform = make_slice_transform(\n    10,    // low_length (total dimension)\n    2,     // slice_begin\n    7      // slice_end\n);\n\n// Maps upper range [0, 5) to lower range [2, 7)\nmulti_index&lt;1&gt; upper_coord{3};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 3 + 2 = 5\n\n\n10. ModuloTransform - Cyclic Wrapping\n// From composable_kernel - modulo operation for cyclic access\nauto transform = make_modulo_transform(\n    4,     // modulus\n    16     // up_length\n);\n\n// Maps coordinates cyclically\nmulti_index&lt;1&gt; upper_coord{13};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 13 % 4 = 1",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#summary",
    "href": "concepts/02_transforms.html#summary",
    "title": "Individual Transforms",
    "section": "Summary",
    "text": "Summary\nIndividual transforms provide: - Modularity: Each transform does one thing well - Composability: Chain transforms for complex mappings - Efficiency: Compile-time optimization in C++ - Flexibility: Handle any coordinate conversion need\nUnderstanding these building blocks enables you to: 1. Create custom tensor views 2. Implement efficient data access patterns 3. Handle padding and boundaries correctly 4. Optimize memory layouts for GPU access\nThe C++ implementations in Composable Kernel provide: - Zero-overhead abstractions through templates - Compile-time composition and optimization - Support for complex coordinate transformations - Integration with GPU kernel generation",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#code-validation-strategy",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#code-validation-strategy",
    "title": "",
    "section": "Code Validation Strategy",
    "text": "Code Validation Strategy\nPre-Documentation Creation: All code examples MUST be validated before inclusion in documentation - Step 1: Write standalone Python scripts for each concept/example - Step 2: Test scripts independently with python script_name.py - Step 3: Verify output, fix any errors, ensure clean execution - Step 4: Only after successful validation, extract code snippets for .qmd files - Step 5: Test .qmd code blocks work in isolation (copy-paste test)"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#current-directory-structure",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#current-directory-structure",
    "title": "",
    "section": "Current Directory Structure",
    "text": "Current Directory Structure\ntile_distribution_documentation/validation_scripts/\n├── README.md                    # Testing instructions and overview\n├── requirements.txt             # Python dependencies for all scripts\n├── IMPLEMENTATION_GUIDE.md      # This file - detailed implementation guide\n├── common/                      # Shared utilities and helpers\n│   ├── __init__.py\n│   ├── test_utils.py           # Common testing functions\n│   └── visualization_helpers.py # Shared plotting/display code\n├── code_examples/               # Common utilities for scripts\n│   └── common_utils.py         # Shared helper functions\n├── part1_foundation/           # ✅ Memory → Tensors (Complete)\n│   ├── buffer_view_basics.py   # Raw memory buffer examples\n│   ├── tensor_view_basics.py   # Multi-dimensional tensor views\n│   └── validate_foundation.py  # Validation tests for part 1\n├── part2_transforms/           # ✅ Coordinate Transformation Engine (Complete)\n│   ├── coordinate_transforms.py     # Individual transforms (Merge, Unmerge, Replicate)\n│   ├── test_part2.py              # Validation tests for part 2\n│   └── validate_coordinate_transforms.py # Coordinate transform validation\n├── part3_distribution_api/     # ✅ High-Level Distribution APIs (Complete)\n│   ├── tile_distribution_basics.py # Basic distribution concepts\n│   ├── tile_window_basics.py       # make_tile_window examples\n│   ├── sweep_operations.py         # sweep_tile usage patterns\n│   └── validate_distribution_api.py # Validation tests for part 3\n├── part4_coordinate_systems/   # ✅ P, Y, X, R, D Spaces (Complete)\n│   └── coordinate_systems_basics.py # Complete coordinate system demonstration\n├── part5_internals/            # ✅ Distribution Encoding Internals (Complete)\n│   ├── encoding_internals.py       # TileDistributionEncoding deep dive\n│   └── static_distributed_tensor.py # StaticDistributedTensor implementation\n├── part6_thread_mapping/       # ✅ Hardware Thread Mapping (Complete)\n│   └── thread_mapping.py           # Thread cooperation and access patterns\n└── part7_advanced_topics/      # 🚧 Performance & Optimization (To be created)\n    ├── performance_optimization.py  # Memory coalescing, efficiency techniques\n    ├── debugging_techniques.py     # Access pattern visualization, profiling\n    └── custom_patterns.py          # Extension points, custom implementations"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#script-status-overview",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#script-status-overview",
    "title": "",
    "section": "Script Status Overview",
    "text": "Script Status Overview\n\n✅ Completed Parts\n\nPart 1: Foundation concepts (buffer views, tensor views)\nPart 2: Coordinate transformation engine (transforms, adaptors)\nPart 3: Distribution API (tile distribution, tile window, sweep operations)\nPart 4: Coordinate systems (P, Y, X, R, D spaces)\nPart 5: Internal implementation (encoding, static distributed tensor)\nPart 6: Thread mapping (hardware connection, cooperation patterns)\n\n\n\n🚧 Remaining Work\n\nPart 7: Advanced topics (performance optimization, debugging, custom patterns)"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#documentation-creation-strategy",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#documentation-creation-strategy",
    "title": "",
    "section": "Documentation Creation Strategy",
    "text": "Documentation Creation Strategy\n\nPhase 1: Complete Validation Scripts\n\nCreate Part 7 Scripts: Performance optimization, debugging techniques, custom patterns\nTest All Scripts: Ensure every script runs successfully\nUpdate Documentation: Fix any issues found during validation\n\n\n\nPhase 2: Create QMD Files\n\nExtract Working Code: Take validated examples from scripts\nCreate Concept Files: Build complete .qmd files with:\n\nConcept introduction\nWorking code examples (from validation scripts)\nInteractive elements\nPractical applications\n\n\n\n\nPhase 3: Integration and Testing\n\nTest QMD Files: Ensure all code blocks work in Quarto\nInteractive Integration: Link to Streamlit apps and visualizations\nFinal Validation: End-to-end testing of documentation"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#script-design-standards",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#script-design-standards",
    "title": "",
    "section": "Script Design Standards",
    "text": "Script Design Standards\n\nMinimalistic Script Design\n\nOne concept per script - No information overload\nClear progression: Headers → Brief intro → Code → Key insight → Summary\nCode-first approach: Show working example, then explain what it does\nEducational output: Print statements that teach, not just debug\nValidation focus - Scripts test what docs will show\n\n\n\nConversational Script Style\n\nDirect address: “You’ll see that…” “Let’s try this…” “Here’s what happens when you…”\nCollaborative tone: “We’re going to build…” “Now we can…” “This lets us…”\nPractical focus: “This is useful because…” “You’d use this when…” “In practice, you’ll find…”\nLearning journey: “First, let’s understand…” “Now that you know X, we can tackle Y…”\nEncouraging: “Don’t worry if this seems complex…” “You’re doing great!” “This will click soon…”\nReal-world context: “GPU programmers often need…” “In a real kernel, you’d…” “This pattern shows up in…”\n\n\n\nScript Structure Template\n#!/usr/bin/env python3\n\"\"\"\nScript Title - Brief Description\n\nShows how to [main concept]. This script demonstrates:\n1. Basic concept introduction\n2. Working examples\n3. Real-world applications\n4. Common pitfalls and solutions\n\"\"\"\n\nimport sys\nimport os\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'code_examples'))\n\nfrom common_utils import *\n# Import specific modules for this concept\n\ndef demonstrate_basic_concept():\n    \"\"\"Show the fundamental concept with simple examples.\"\"\"\n    print_step(1, \"Basic concept demonstration\")\n    # Implementation here\n    return result\n\ndef demonstrate_advanced_usage():\n    \"\"\"Show more complex usage patterns.\"\"\"\n    print_step(2, \"Advanced usage patterns\")\n    # Implementation here\n    return result\n\ndef test_concept_operations():\n    \"\"\"Test that the concept operations work correctly.\"\"\"\n    print_step(3, \"Testing operations\")\n    # Test implementations here\n    return all_tests_passed\n\ndef main():\n    \"\"\"Main function to run all demonstrations.\"\"\"\n    if not check_imports():\n        return False\n\n    print_section(\"Script Title\")\n\n    # Run demonstrations\n    result1 = demonstrate_basic_concept()\n    result2 = demonstrate_advanced_usage()\n\n    # Run tests\n    all_tests_passed = test_concept_operations()\n\n    print_section(\"Summary\")\n    print(f\"✅ Demonstrations completed\")\n    print(f\"✅ All tests passed: {all_tests_passed}\")\n\n    return all_tests_passed\n\nif __name__ == \"__main__\":\n    success = run_script_safely(main, \"Script Title\")\n    sys.exit(0 if success else 1)"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#documentation-file-creation",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#documentation-file-creation",
    "title": "",
    "section": "Documentation File Creation",
    "text": "Documentation File Creation\n\nQMD File Structure Template\n---\ntitle: \"Part X: Concept Title\"\n---\n\n# Part X: Concept Title\n\n## Overview\n\nBrief introduction to the concept and its importance in the tile distribution system.\n\n**Learning Objectives:**\n- Understand [concept 1]\n- Master [concept 2]\n- See [concept 3] in practice\n\n## Key Concepts\n\n### Concept 1\nExplanation with working examples.\n\n```python\n# Working code example from validation scripts\n\n\nConcept 2\nExplanation with practical applications.\n# More working code examples"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#interactive-exploration",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#interactive-exploration",
    "title": "",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\n\nLink to relevant Streamlit app\nLink to specific visualization"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#practical-applications",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#practical-applications",
    "title": "",
    "section": "Practical Applications",
    "text": "Practical Applications\nReal-world usage patterns and performance considerations."
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#summary",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\nKey takeaways and connections to other concepts."
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#next-steps",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#next-steps",
    "title": "",
    "section": "Next Steps",
    "text": "Next Steps\nLink to the next part in the learning journey.\n\n## Validation Workflow\n\n```bash\n# Step 1: Test individual scripts\ncd tile_distribution_documentation/validation_scripts/part1_foundation/\npython buffer_view_basics.py\npython tensor_view_basics.py\n\n# Step 2: Run part-specific tests\npython validate_foundation.py\n\n# Step 3: Test all parts\ncd ../..\npython -m pytest validation_scripts/  # If we add pytest support\n\n# Step 4: Extract working code snippets for .qmd files\n# Only after all tests pass successfully"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#code-quality-standards",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#code-quality-standards",
    "title": "",
    "section": "Code Quality Standards",
    "text": "Code Quality Standards\n\nStandalone Execution: Each script runs without external dependencies\nClear Output: Print statements showing what each example demonstrates\nError Handling: Graceful failure with helpful error messages\nDocumentation: Inline comments explaining key concepts\nMinimal Dependencies: Only use libraries available in the main project"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#interactive-integration-strategy",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#interactive-integration-strategy",
    "title": "",
    "section": "Interactive Integration Strategy",
    "text": "Interactive Integration Strategy\n\nStreamlit Apps\n\napp.py: Tile Distribution Visualizer\n\nLink from Part 0 (motivation) and Part 3 (distribution API)\nPre-configured examples for each concept\n\ntensor_transform_app.py: Transformation Pipeline Explorer\n\nLink from Part 2 (transformation engine)\nVisual demonstration of transform chains\n\nthread_visualization_app.py: Thread Access Pattern Analyzer\n\nLink from Part 6 (thread mapping)\nThread-by-thread access pattern visualization\n\n\n\n\nManim Animations\n\nanimation/: Tile distribution encoding graph visualization\n\nEmbed in Part 5 (internals) for encoding explanation\nVisual representation of R, H, P, Y dimensions"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#success-criteria",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#success-criteria",
    "title": "",
    "section": "Success Criteria",
    "text": "Success Criteria\n\nScript Validation Success\n\nAll scripts run without errors\nClear educational output\nComprehensive concept coverage\nReal-world examples included\n\n\n\nDocumentation Creation Success\n\nComplete standalone documentation (no external dependencies)\nAll code examples work in QMD files\nInteractive elements properly integrated\nClear learning progression from Part 0 to Part 7\n\n\n\nQuality Assurance\n\nConcepts explained from first principles\nWorking examples for all APIs\nPerformance considerations included\nDebugging techniques demonstrated"
  },
  {
    "objectID": "validation_scripts/IMPLEMENTATION_GUIDE.html#current-priority",
    "href": "validation_scripts/IMPLEMENTATION_GUIDE.html#current-priority",
    "title": "",
    "section": "Current Priority",
    "text": "Current Priority\nImmediate: Create Part 7 advanced topics scripts Next: Begin QMD file creation using validated examples Future: Interactive integration and final testing\nThis guide ensures all documentation is built on a foundation of validated, working code examples."
  },
  {
    "objectID": "concepts/05_encoding_internals.html#overview",
    "href": "concepts/05_encoding_internals.html#overview",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Overview",
    "text": "Overview\nThe tile distribution encoding system represents the core mathematical framework that transforms high-level tensor distribution specifications into concrete, optimized GPU kernel implementations. This sophisticated compile-time machinery bridges the gap between abstract mathematical descriptions and executable coordinate transformations, enabling the Composable Kernel framework to generate highly efficient code for complex tensor operations.\nAt its heart, the encoding system defines how multi-dimensional tensor data is distributed across GPU processing elements through a hierarchical decomposition scheme. By specifying relationships between different coordinate spaces - replication (R), hierarchical (H), partition (P), and yield (Y) dimensions - the encoding provides a complete blueprint for data layout and access patterns that can be resolved entirely at compile time.",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#encoding-structure",
    "href": "concepts/05_encoding_internals.html#encoding-structure",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Encoding Structure",
    "text": "Encoding Structure\n\ngraph TB\n    subgraph \"Encoding Components\"\n        RS[\"R-space LengthsReplication dimensions\"]\n        HS[\"H-space LengthsHierarchical decomposition[[2,2],[2,2]]\"]\n        P2RH[\"P→RH MappingsThread to hierarchyMajor/Minor\"]\n        Y2RH[\"Y→RH MappingsElement to hierarchyMajor/Minor\"]\n    end\n    \n    subgraph \"Generated Components\"\n        ADAPTOR[\"ps_ys_to_xs_adaptorCoordinate transformer\"]\n        DESC[\"ys_to_d_descriptorMemory linearizer\"]\n        ENC[\"EncodingOriginal specification\"]\n    end\n    \n    subgraph \"Transformation Chain\"\n        T1[\"ReplicateTransform\"]\n        T2[\"UnmergeTransform\"]\n        T3[\"MergeTransform\"]\n    end\n    \n    RS --&gt; T1\n    HS --&gt; T2\n    P2RH --&gt; ADAPTOR\n    Y2RH --&gt; ADAPTOR\n    \n    T1 --&gt; T2\n    T2 --&gt; T3\n    T3 --&gt; ADAPTOR\n    \n    HS --&gt; DESC\n    Y2RH --&gt; DESC\n    \n    style RS fill:#fce4ec,stroke:#c2185b,stroke-width:2px\n    style HS fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style ADAPTOR fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style DESC fill:#fff3e0,stroke:#f57c00,stroke-width:3px",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#what-is-tile-distribution-encoding",
    "href": "concepts/05_encoding_internals.html#what-is-tile-distribution-encoding",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "What is Tile Distribution Encoding?",
    "text": "What is Tile Distribution Encoding?\nThe tile_distribution_encoding struct serves as a compile-time blueprint for defining complex tensor data layouts in GPU programming. It addresses a fundamental challenge in high-performance computing: how to map abstract tensor operations to the hierarchical memory and execution architecture of modern GPUs while maintaining both performance and programmability.\nThe encoding system operates through a multi-stage transformation pipeline that converts high-level specifications into concrete coordinate mappings. This pipeline consists of three primary components:\nThe ps_ys_to_xs_adaptor performs the crucial transformation from processing element coordinates (P) and logical access pattern coordinates (Y) to physical tensor coordinates (X). This adaptor encodes the complete mapping logic through a chain of coordinate transformations including replicate, unmerge, and merge operations.\nThe ys_to_d_descriptor manages the linearization of the multi-dimensional Y coordinate space into a one-dimensional data space suitable for register allocation. This component ensures efficient register utilization by mapping logical access patterns to physical storage locations.\nThe transformation chains are automatically constructed from the encoding parameters through template metaprogramming, ensuring zero runtime overhead. Each transformation in the chain corresponds to a specific coordinate space manipulation, collectively implementing the complete distribution strategy.\nThe C++ implementation leverages advanced template metaprogramming techniques:\n// From ck_tile/core/tensor/tile_distribution_encoding.hpp\ntemplate &lt;typename RsLengths_,    // Replication dimension lengths\n          typename HsLengthss_,   // Hierarchical dimension lengths\n          typename Ps2RHssMajor_, // P to RH mapping (major)\n          typename Ps2RHssMinor_, // P to RH mapping (minor)\n          typename Ys2RHsMajor_,  // Y to RH mapping (major)\n          typename Ys2RHsMinor_&gt;  // Y to RH mapping (minor)\nstruct tile_distribution_encoding\n{\n    // All computations resolved at compile time\n    static constexpr index_t NDimX = HsLengthss::size();\n    static constexpr index_t NDimP = Ps2RHssMajor::size();\n    static constexpr index_t NDimY = Ys2RHsMajor::size();\n    static constexpr index_t NDimR = RsLengths::size();\n    \n    // Nested detail struct performs complex compile-time calculations\n    struct detail\n    {\n        // Precomputed mappings and transformations\n        static constexpr auto get_h_dim_lengths_prefix_sum();\n        static constexpr auto get_uniformed_idx_y_to_h();\n        // ... extensive compile-time computation ...\n    };\n};",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#encoding-structure-deep-dive",
    "href": "concepts/05_encoding_internals.html#encoding-structure-deep-dive",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Encoding Structure Deep Dive",
    "text": "Encoding Structure Deep Dive\nThe tile distribution encoding employs a sophisticated type system that captures the complete specification of tensor distribution patterns at compile time. Understanding this structure is essential for leveraging the full power of the CK framework’s optimization capabilities.\nThe encoding revolves around several interconnected dimension types that collectively define the distribution strategy:",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#c-template-implementation",
    "href": "concepts/05_encoding_internals.html#c-template-implementation",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "C++ Template Implementation",
    "text": "C++ Template Implementation\nThe encoding system is implemented as a C++ template struct that leverages compile-time computation for maximum performance:\n// From include/ck_tile/core/tensor/tile_distribution_encoding.hpp\n\ntemplate &lt;typename RsLengths_,      // Replication dimensions\n          typename HsLengthss_,     // Hierarchical dimensions (tuple of tuples)\n          typename Ps2RHssMajor_,   // P→RH major mapping\n          typename Ps2RHssMinor_,   // P→RH minor mapping\n          typename Ys2RHsMajor_,    // Y→RH major mapping\n          typename Ys2RhsMinor_,    // Y→RH minor mapping\n          typename RHs2Xs_&gt;         // RH→X final mapping\nstruct tile_distribution_encoding\n{\n    using rs_lengths_type = RsLengths_;\n    using hs_lengthss_type = HsLengthss_;\n    \n    // Static member functions for compile-time access\n    CK_TILE_HOST_DEVICE static constexpr auto get_rs_lengths()\n    {\n        return rs_lengths_type{};\n    }\n    \n    CK_TILE_HOST_DEVICE static constexpr auto get_hs_lengthss() \n    {\n        return hs_lengthss_type{};\n    }\n    \n    // Compute total number of dimensions\n    CK_TILE_HOST_DEVICE static constexpr index_t get_num_of_dimension_p()\n    {\n        return size(Ps2RHssMajor_{});\n    }\n    \n    CK_TILE_HOST_DEVICE static constexpr index_t get_num_of_dimension_y()\n    {\n        return size(Ys2RHsMajor_{});\n    }\n};\n\nKey C++ Features Used\n\nTemplate Metaprogramming: All parameters are types, not values, enabling compile-time optimization\nConstexpr Functions: Everything is computed at compile time\nType Aliases: Clean access to template parameters\nStatic Member Functions: No runtime overhead",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#parameter-breakdown",
    "href": "concepts/05_encoding_internals.html#parameter-breakdown",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Parameter Breakdown",
    "text": "Parameter Breakdown\nEach template parameter in the encoding system serves a specific purpose in defining the overall distribution strategy. These parameters work together to create a complete specification that can be transformed into efficient GPU code.\n\nR-Dimensions: Replication Specification\nThe RsLengths parameter defines dimensions that are replicated across processing units, enabling data sharing patterns essential for many tensor operations. In the transformation pipeline, these dimensions generate coord_transform_enum::replicate operations that broadcast data across multiple processing elements.\nReplication serves several critical purposes in GPU kernel optimization. It enables efficient data reuse in operations like matrix multiplication where the same input data is needed by multiple output computations. It also facilitates reduction operations where multiple threads collaborate to compute a single result. The replication pattern directly impacts memory access efficiency and register utilization.\nFor example, in a GEMM operation: - Matrix A might have RsLengths = sequence&lt;NWarp&gt;, indicating replication across warps computing different N-dimension tiles - Matrix B might have RsLengths = sequence&lt;MWarp&gt;, indicating replication across warps computing different M-dimension tiles - The output matrix C typically has no replication at the outer level, as each element is uniquely owned\n\n\nH-Dimensions: Hierarchical Decomposition\nThe HsLengthss parameter represents the hierarchical decomposition of tensor dimensions, encoded as a tuple of sequences. Each sequence defines how a logical tensor dimension (X-dimension) is broken down into finer-grained components that map to the GPU’s execution hierarchy.\nThis hierarchical decomposition is fundamental to achieving high performance on GPUs. It enables: - Memory coalescing: By aligning the decomposition with warp and thread organization - Register blocking: By defining tile sizes that fit in the register file - Shared memory utilization: By creating tiles that exploit data reuse\nThe decomposition typically follows a pattern like sequence&lt;RepeatCount, WarpCount, ThreadCount, VectorSize&gt;, where: - RepeatCount: Number of iterations each thread performs - WarpCount: Number of warps assigned to this dimension - ThreadCount: Number of threads per warp assigned to this dimension\n- VectorSize: Number of elements accessed in a single vector operation\nIn the transformation pipeline, each H-dimension group generates coord_transform_enum::unmerge operations that break down abstract dimensions into their hierarchical components:\n// Example from block GEMM implementation\nconstexpr auto hs_lengthss = tuple&lt;\n    sequence&lt;MIterPerWarp, MWarp&gt;,  // M-dimension decomposition\n    sequence&lt;KIterPerWarp&gt;          // K-dimension decomposition\n&gt;;\n\n\nP-Dimensions: Partition Mapping\nThe Ps2RHssMajor and Ps2RHssMinor parameters define how partition dimensions map to the underlying RH-dimensions. Partition dimensions represent the fundamental unit of work assignment in the GPU’s execution model, typically corresponding to hardware constructs like warps and threads.\nThe mapping mechanism uses a major/minor indexing scheme where: - Major index identifies which RH-dimension group (0 for R-dimensions, 1 to NDimX for H-dimensions) - Minor index identifies the specific component within that group\nThis mapping enables flexible work distribution strategies. For instance, in a typical block-level distribution: - NDimP = 1 might map to {MWarp, NWarp} for distributing work across warp groups - NDimP = 2 at the combined level typically maps to {warp_id, lane_id} for thread-level distribution\nThe P-dimension mapping directly influences memory access patterns and computational efficiency:\n// P-dimension retrieval in tile_distribution\nCK_TILE_HOST_DEVICE static auto _get_partition_index()\n{\n    if constexpr(NDimP == 1)\n        return array&lt;index_t, 1&gt;{get_lane_id()};\n    else if constexpr(NDimP == 2)\n        return array&lt;index_t, 2&gt;{get_warp_id(), get_lane_id()};\n}\n\n\nY-Dimensions: Logical View Mapping\nThe Ys2RHsMajor and Ys2RHsMinor parameters define the logical view of the tile data - how users perceive and access the distributed tensor. Y-dimensions represent the iteration space for accessing tile elements, abstracting away the complexity of the underlying distribution.\nThe Y-to-RH mapping serves multiple purposes: - Interface definition: Provides a clean API for accessing distributed data - Access pattern encoding: Defines the order in which elements are processed - Vectorization support: Enables efficient vector operations by grouping contiguous elements\nIn practice, Y-dimensions often correspond to the logical dimensions of the operation being performed. For a matrix multiplication tile: - Y0 might represent the M-dimension iteration space - Y1 might represent the K-dimension iteration space\nThe mapping ensures that logical access patterns translate to efficient physical memory operations.\n\n[[4, 4], [4, 4]] → 4x4 tiles (16 elements per thread)\n[[2, 4], [4, 2]] → 2x4 and 4x2 tiles (8 elements per thread)\n\n\n\nP→RH Mappings\n🔹 ps_to_rhss_major/minor (P→RH Mappings)\nMaps partition coordinates to RH space: - Controls which H dimensions each P dimension affects - major/minor specify different levels of the mapping\n📝 Conceptual Example: ps_to_rhss_major=[[1], [2]] means: - P dimension 0 maps to H dimension 1 - P dimension 1 maps to H dimension 2\nThis determines how thread coordinates affect tile placement.\n\n\nY→RH Mappings\n🔹 ys_to_rhs_major/minor (Y→RH Mappings)\nMaps Y coordinates to RH space: - Determines how logical Y coordinates map to hierarchical structure - Controls the internal organization of each thread’s tile\n📝 Example Mapping: ys_to_rhs_major=[1, 1, 2, 2] means: - Y[0] maps to H1 - Y[1] maps to H1\n- Y[2] maps to H2 - Y[3] maps to H2\nThis creates the internal structure of thread tiles.",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#from-encoding-to-tile-distribution",
    "href": "concepts/05_encoding_internals.html#from-encoding-to-tile-distribution",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "From Encoding to Tile Distribution",
    "text": "From Encoding to Tile Distribution\nThe magic happens when make_static_tile_distribution() transforms the mathematical encoding into runtime components:",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#the-py-x-transformation-chain",
    "href": "concepts/05_encoding_internals.html#the-py-x-transformation-chain",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "The P+Y → X Transformation Chain",
    "text": "The P+Y → X Transformation Chain\nThe heart of the system is the adaptor that implements the P+Y → X transformation.\n\nflowchart LR\n    subgraph \"Input Coordinates\"\n        P[\"P-coordinates[warp_id, lane_id]\"]\n        Y[\"Y-coordinates[y0, y1, y2, y3]\"]\n    end\n    \n    subgraph \"Transformation Pipeline\"\n        C1[\"Combine P+Y\"]\n        T1[\"ReplicateTransform(if R-dims exist)\"]\n        T2[\"UnmergeTransform(break into H-dims)\"]\n        T3[\"MergeTransform(combine to X-dims)\"]\n    end\n    \n    subgraph \"Output\"\n        X[\"X-coordinates[x0, x1]Tensor position\"]\n    end\n    \n    P --&gt; C1\n    Y --&gt; C1\n    C1 --&gt; T1\n    T1 --&gt; T2\n    T2 --&gt; T3\n    T3 --&gt; X\n    \n    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\nC++ Transformation Chain Implementation\nThe transformation chain is built using C++ template metaprogramming to create a compile-time pipeline:\n// Building the transformation chain in make_static_tile_distribution\ntemplate &lt;typename Encoding&gt;\nCK_TILE_HOST_DEVICE auto make_ps_ys_to_xs_adaptor(const Encoding& encoding)\n{\n    // Step 1: Create individual transforms\n    constexpr auto replicate_transform = make_replicate_transform(\n        encoding.get_rs_lengths());\n    \n    constexpr auto unmerge_transform = make_unmerge_transform(\n        encoding.get_hs_lengthss());\n    \n    constexpr auto merge_transform = make_merge_transform(\n        encoding.get_rhs_to_xs_mapping());\n    \n    // Step 2: Chain transforms together\n    constexpr auto transform_chain = chain_transforms(\n        replicate_transform,\n        unmerge_transform, \n        merge_transform);\n    \n    // Step 3: Create adaptor with the chain\n    return make_tile_adaptor(\n        transform_chain,\n        encoding.get_lower_dimension_hidden_idss());\n}\n\n\nHow Transforms Work in C++\nEach transform is a compile-time object that knows how to convert coordinates:\n// Example: Replicate transform implementation\ntemplate &lt;typename Lengths&gt;\nstruct replicate_transform\n{\n    static constexpr index_t num_of_upper_dimension = size(Lengths{});\n    static constexpr index_t num_of_lower_dimension = 2 * num_of_upper_dimension;\n    \n    template &lt;typename UpperIndex&gt;\n    CK_TILE_HOST_DEVICE constexpr auto \n    calculate_lower_index(const UpperIndex& idx_upper) const\n    {\n        // Replicate each coordinate: [a,b] -&gt; [a,b,0,0]\n        auto idx_lower = make_zero_multi_index&lt;num_of_lower_dimension&gt;();\n        \n        static_for&lt;0, num_of_upper_dimension, 1&gt;{}([&](auto i) {\n            idx_lower(i) = idx_upper[i];\n            idx_lower(i + num_of_upper_dimension) = 0;\n        });\n        \n        return idx_lower;\n    }\n};\n🔗 The P+Y → X Transformation Chain\nThe ps_ys_to_xs_adaptor implements a chain of transformations: 1. Start with P coordinates (which thread) 2. Add Y coordinates (which element in thread’s tile) 3. Apply replication transforms (R-space) 4. Apply hierarchical transforms (H-space) 5. Merge into final X coordinates\n💡 Why This Chain Works: - Each transform handles one aspect of the mapping - Transforms are composable and efficient - The chain is built automatically from encoding - Same pattern works for any distribution strategy\n📝 Conceptual Example: - Input: P=[1,0] + Y=[0,1] → Combined=[1,0,0,1] - Transform 1: Handle replication (none in this case) - Transform 2: Handle hierarchical structure - Transform 3: Merge to final coordinates - Output: X=[0,3] (final tensor position)",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#the-y-to-d-linearization",
    "href": "concepts/05_encoding_internals.html#the-y-to-d-linearization",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "The Y to D Linearization",
    "text": "The Y to D Linearization\nThe descriptor handles the linearization of Y coordinates to memory addresses.\n\nC++ Implementation of Y→D Descriptor\nThe Y→D descriptor is responsible for converting multi-dimensional Y coordinates into linear memory offsets:\n// Y to D descriptor implementation\ntemplate &lt;typename YLengths, typename YStrides&gt;\nstruct ys_to_d_descriptor\n{\n    static constexpr index_t num_of_dimension = size(YLengths{});\n    \n    // Calculate linear offset from Y coordinates\n    template &lt;typename YIndex&gt;\n    CK_TILE_HOST_DEVICE constexpr index_t \n    calculate_offset(const YIndex& idx_y) const\n    {\n        index_t offset = 0;\n        \n        static_for&lt;0, num_of_dimension, 1&gt;{}([&](auto i) {\n            offset += idx_y[i] * YStrides{}[i];\n        });\n        \n        return offset;\n    }\n    \n    // Get element space size (total elements per thread)\n    CK_TILE_HOST_DEVICE static constexpr index_t \n    get_element_space_size()\n    {\n        return reduce_on_sequence(\n            YLengths{}, \n            multiplies{}, \n            number&lt;1&gt;{});\n    }\n};\n\n// Usage in distributed tensor\ntemplate &lt;typename TileDistribution&gt;\nstruct static_distributed_tensor\n{\n    using ys_to_d_descriptor = typename TileDistribution::ys_to_d_descriptor;\n    \n    // Thread-local storage\n    static constexpr index_t thread_buffer_size = \n        ys_to_d_descriptor::get_element_space_size();\n    \n    DataType thread_buffer_[thread_buffer_size];\n    \n    // Access element at Y coordinate\n    template &lt;typename YIndex&gt;\n    CK_TILE_HOST_DEVICE DataType& at(const YIndex& idx_y)\n    {\n        const index_t offset = ys_to_d_descriptor{}.calculate_offset(idx_y);\n        return thread_buffer_[offset];\n    }\n};\n\n\nMemory Layout Optimization\nThe descriptor enables efficient register allocation:\n// Optimized layout for vector operations\ntemplate &lt;index_t M, index_t N, index_t VectorSize&gt;\nstruct make_ys_to_d_descriptor_for_gemm\n{\n    // Layout: [M/VectorSize][N][VectorSize]\n    // This ensures vector loads are contiguous in memory\n    using type = tile_descriptor&lt;\n        sequence&lt;M/VectorSize, N, VectorSize&gt;,\n        sequence&lt;N * VectorSize, VectorSize, 1&gt;&gt;;\n};\n\n\nY to D Linearization Details\nThe ys_to_d_descriptor handles memory layout within each thread: 1. Start with Y coordinates [y0, y1, y2, y3] 2. Apply thread’s local layout (usually row-major) 3. Compute linear offset within thread’s buffer 4. Result: D coordinate (memory address)\n📝 Example with [2, 2] tile: - Y=[0,0] → D=0 - Y=[0,1] → D=1 - Y=[1,0] → D=2 - Y=[1,1] → D=3\n💡 Why Separate from Adaptor: - Adaptor handles inter-thread coordination (P+Y → X) - Descriptor handles intra-thread layout (Y → D) - This separation enables different memory layouts - Each thread can have its own descriptor",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#practical-examples",
    "href": "concepts/05_encoding_internals.html#practical-examples",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Practical Examples",
    "text": "Practical Examples\nDifferent encodings create different behaviors:\n🎯 Example 1: Simple 2x2 Distribution\nsimple_encoding = TileDistributionEncoding(\n    rs_lengths=[],\n    hs_lengthss=[[2], [2]],\n    ps_to_rhss_major=[[], []],\n    ps_to_rhss_minor=[[], []],\n    ys_to_rhs_major=[1, 2],\n    ys_to_rhs_minor=[0, 0]\n)\n\nNo replication\nSimple hierarchical structure\nDirect P→H mapping\nGood for basic matrix operations\n\n🎯 Example 2: With Replication\nreplicated_encoding = TileDistributionEncoding(\n    rs_lengths=[2],  # 2-way replication\n    hs_lengthss=[[2], [2]],\n    ps_to_rhss_major=[[], []],\n    ps_to_rhss_minor=[[], []],\n    ys_to_rhs_major=[1, 2],\n    ys_to_rhs_minor=[0, 0]\n)\n\n2-way replication for data sharing\nSame hierarchical structure\nGood for broadcast operations\nEnables thread cooperation",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#testing-your-understanding",
    "href": "concepts/05_encoding_internals.html#testing-your-understanding",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Testing Your Understanding",
    "text": "Testing Your Understanding\nLet’s verify your understanding of encoding internals:",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/05_encoding_internals.html#key-takeaways",
    "href": "concepts/05_encoding_internals.html#key-takeaways",
    "title": "Encoding Internals - The Internal Machinery",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThe tile distribution encoding system represents a sophisticated solution to the challenge of mapping high-level tensor operations to GPU hardware. Understanding its internals reveals several key architectural principles:\n\nMathematical Foundation\nThe encoding structure provides a complete mathematical specification of thread organization and data distribution:\n\nDimensional Hierarchy: The system defines four types of dimensions (R, H, P, Y) that collectively describe the complete distribution strategy\nCompile-Time Resolution: All relationships and transformations are resolved at compile time, ensuring zero runtime overhead\nComposable Transformations: Individual coordinate transformations (replicate, unmerge, merge) compose to create complex mappings\nHardware Alignment: The hierarchical decomposition naturally aligns with GPU hardware organization\n\n\n\nAutomatic Code Generation\nThe transformation from mathematical specification to executable code happens through a sophisticated pipeline:\n\nEncoding Analysis: The make_adaptor_encoding_for_tile_distribution function analyzes the encoding parameters\nTransform Chain Construction: Individual transformations are instantiated and connected based on the specification\nOptimization Application: Compile-time optimizations eliminate overhead and generate efficient code\nComponent Integration: The generated adaptors and descriptors integrate seamlessly with the broader CK framework\n\n\n\nComponent Architecture\nThe clean separation of concerns enables both flexibility and performance:\n\nThe ps_ys_to_xs_adaptor handles the complex mapping from logical to physical coordinates\nThe ys_to_d_descriptor manages efficient linearization for register allocation\nEach component has a well-defined interface enabling independent optimization\nComponents compose naturally to create complete distribution systems\n\n\n\nPerformance Implications\nEvery aspect of the encoding system is designed with GPU performance in mind:\n\nMemory Coalescing: Hierarchical decomposition ensures adjacent threads access adjacent memory\nRegister Efficiency: Y-to-D linearization optimizes register allocation and minimizes spills\nBank Conflict Avoidance: Careful dimension ordering prevents shared memory bank conflicts\nVectorization Support: The encoding naturally supports vector operations for maximum throughput\n\n\n\nPractical Applications\nThe encoding system’s flexibility enables efficient implementation of diverse operations:\n\nMatrix Multiplication: Hierarchical tiling with appropriate replication patterns\nConvolution: Spatial tiling with proper boundary handling\nReduction Operations: Collaborative patterns through R-dimension specification\nComplex Fusion: Multiple operations can share distribution patterns for efficiency\n\nThe encoding internals demonstrate how the Composable Kernel framework achieves both mathematical elegance and practical performance. By leveraging compile-time computation and sophisticated type system design, the same mathematical framework that provides clarity and composability also generates code that rivals hand-optimized implementations. This synergy between abstraction and performance represents the core strength of the CK approach to GPU kernel development.",
    "crumbs": [
      "Implementation Deep Dive",
      "Encoding Internals - The Internal Machinery"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html",
    "href": "concepts/01_tensor_view.html",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "",
    "text": "TensorView adds multi-dimensional structure to raw memory. While BufferView provides linear access, TensorView enables coordinate-based access to matrices, tensors, and higher-dimensional data structures.\n\n\n\ngraph TB\n    subgraph \"Memory Foundation\"\n        Memory[\"Flat Memory Array0 1 2 3 4 5 6 7 8 9 10 11\"]\n    end\n    \n    subgraph \"Access Layer\"\n        BufferView[\"BufferViewLinear Memory Access\"]\n        Descriptor[\"TensorDescriptorShape & Stride Info\"]\n    end\n    \n    subgraph \"Tensor Layer\"\n        TensorView[\"TensorViewMulti-dimensional Access\"]\n    end\n    \n    subgraph \"Logical View\"\n        Matrix[\"2D Matrix View[3×4][[0,1,2,3][4,5,6,7][8,9,10,11]]\"]\n    end\n    \n    Memory --&gt; BufferView\n    Memory --&gt; Descriptor\n    BufferView --&gt; TensorView\n    Descriptor --&gt; TensorView\n    TensorView --&gt; Matrix\n    \n    style Memory fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style BufferView fill:#dbeafe,stroke:#3b82f6,stroke-width:2px\n    style Descriptor fill:#fed7aa,stroke:#f59e0b,stroke-width:2px\n    style TensorView fill:#fce7f3,stroke:#ec4899,stroke-width:2px\n    style Matrix fill:#e9d5ff,stroke:#9333ea,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#overview",
    "href": "concepts/01_tensor_view.html#overview",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "",
    "text": "TensorView adds multi-dimensional structure to raw memory. While BufferView provides linear access, TensorView enables coordinate-based access to matrices, tensors, and higher-dimensional data structures.\n\n\n\ngraph TB\n    subgraph \"Memory Foundation\"\n        Memory[\"Flat Memory Array0 1 2 3 4 5 6 7 8 9 10 11\"]\n    end\n    \n    subgraph \"Access Layer\"\n        BufferView[\"BufferViewLinear Memory Access\"]\n        Descriptor[\"TensorDescriptorShape & Stride Info\"]\n    end\n    \n    subgraph \"Tensor Layer\"\n        TensorView[\"TensorViewMulti-dimensional Access\"]\n    end\n    \n    subgraph \"Logical View\"\n        Matrix[\"2D Matrix View[3×4][[0,1,2,3][4,5,6,7][8,9,10,11]]\"]\n    end\n    \n    Memory --&gt; BufferView\n    Memory --&gt; Descriptor\n    BufferView --&gt; TensorView\n    Descriptor --&gt; TensorView\n    TensorView --&gt; Matrix\n    \n    style Memory fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style BufferView fill:#dbeafe,stroke:#3b82f6,stroke-width:2px\n    style Descriptor fill:#fed7aa,stroke:#f59e0b,stroke-width:2px\n    style TensorView fill:#fce7f3,stroke:#ec4899,stroke-width:2px\n    style Matrix fill:#e9d5ff,stroke:#9333ea,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#creation-and-basic-usage",
    "href": "concepts/01_tensor_view.html#creation-and-basic-usage",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Creation and Basic Usage",
    "text": "Creation and Basic Usage\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/tensor/tensor_view.hpp\n#include &lt;ck_tile/core/tensor/tensor_view.hpp&gt;\n\n__device__ void example_tensor_creation()\n{\n    // Create a 3x4 matrix in global memory\n    float data[12] = {0,1,2,3,4,5,6,7,8,9,10,11};\n    \n    // Method 1: Create buffer and descriptor separately\n    auto buffer = make_buffer_view&lt;address_space_enum::global&gt;(data, 12);\n    auto desc = make_tensor_descriptor(make_tuple(3, 4), make_tuple(4, 1));\n    \n    // Create tensor view\n    auto tensor = make_tensor_view&lt;address_space_enum::global&gt;(buffer, desc);\n    \n    // Method 2: Use convenience function\n    auto tensor2 = make_naive_tensor_view&lt;address_space_enum::global&gt;(\n        data,           // pointer\n        make_tuple(3, 4),  // shape\n        make_tuple(4, 1)   // strides\n    );\n    \n    // Access element at (1, 2)\n    float value = tensor.get_element(make_tuple(1, 2));  // Returns 6\n    \n    // Update element\n    tensor.set_element(make_tuple(2, 1), 99.0f);\n}",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#flow-layouts-from-coordinates-to-memory",
    "href": "concepts/01_tensor_view.html#flow-layouts-from-coordinates-to-memory",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Flow & Layouts: From Coordinates to Memory",
    "text": "Flow & Layouts: From Coordinates to Memory\n\nflowchart LR\n    subgraph \"User Input\"\n        Coord[\"Coordinate(1, 2)\"]\n    end\n    \n    subgraph \"TensorView Processing\"\n        Shape[\"Shape Checkrow &lt; 3?col &lt; 4?\"]\n        Stride[\"Apply Stridesoffset = 1×4 + 2×1\"]\n        Buffer[\"BufferView Accessbuffer[6]\"]\n    end\n    \n    subgraph \"Result\"\n        Value[\"Value: 6\"]\n    end\n    \n    Coord --&gt; Shape\n    Shape --&gt;|Valid| Stride\n    Stride --&gt; Buffer\n    Buffer --&gt; Value\n    \n    style Coord fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style Shape fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style Stride fill:#dcfce7,stroke:#10b981,stroke-width:2px\n    style Buffer fill:#dbeafe,stroke:#3b82f6,stroke-width:2px\n    style Value fill:#d1fae5,stroke:#10b981,stroke-width:2px\n\n\ngraph TB\n    subgraph \"Row-Major Layout (C-style)\"\n        RM[\"Memory: [0,1,2,3,4,5,6,7,8,9,10,11]Shape: (3,4)Strides: (4,1)\"]\n        RMMatrix[\"[[0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11]]\"]\n        RM --&gt; RMMatrix\n    end\n    \n    subgraph \"Column-Major Layout (Fortran-style)\"\n        CM[\"Memory: [0,3,6,9,1,4,7,10,2,5,8,11]Shape: (3,4)Strides: (1,3)\"]\n        CMMatrix[\"[[0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11]]\"]\n        CM --&gt; CMMatrix\n    end\n    \n    subgraph \"Custom Stride (Transposed View)\"\n        TV[\"Memory: [0,1,2,3,4,5,6,7,8,9,10,11]Shape: (4,3)Strides: (1,4)\"]\n        TVMatrix[\"[[0, 4, 8] [1, 5, 9] [2, 6, 10] [3, 7, 11]]\"]\n        TV --&gt; TVMatrix\n    end\n    \n    style RM fill:#e0f2fe,stroke:#0284c7,stroke-width:2px\n    style CM fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style TV fill:#f3e8ff,stroke:#9333ea,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#coordinate-mapping",
    "href": "concepts/01_tensor_view.html#coordinate-mapping",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Coordinate Mapping",
    "text": "Coordinate Mapping\n\ngraph LR\n    subgraph \"2D Coordinate\"\n        C2D[\"(row=2, col=1)\"]\n    end\n    \n    subgraph \"Offset Calculation\"\n        Calc[\"offset = row × row_stride + col × col_strideoffset = 2 × 4 + 1 × 1 = 9\"]\n    end\n    \n    subgraph \"1D Memory\"\n        Mem[\"[0,1,2,3,4,5,6,7,8,9,10,11]\"]\n    end\n    \n    C2D --&gt; Calc\n    Calc --&gt; Mem\n    \n    style C2D fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style Calc fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style Mem fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#advanced-operations",
    "href": "concepts/01_tensor_view.html#advanced-operations",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Advanced Operations",
    "text": "Advanced Operations\n\nSlicing and Subviews\n\n\n\n\n\n\n\n\nBroadcasting and Padding",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#tensorview-vs-bufferview",
    "href": "concepts/01_tensor_view.html#tensorview-vs-bufferview",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "TensorView vs BufferView",
    "text": "TensorView vs BufferView\n\ngraph TB\n    subgraph \"BufferView\"\n        BV1[\"Linear indexing only\"]\n        BV2[\"buffer[5]\"]\n        BV3[\"No shape information\"]\n        BV4[\"Direct memory access\"]\n    end\n    \n    subgraph \"TensorView\"\n        TV1[\"Multi-dimensional indexing\"]\n        TV2[\"tensor[1, 2]\"]\n        TV3[\"Shape-aware operations\"]\n        TV4[\"Coordinate transformations\"]\n    end\n    \n    subgraph \"Use Cases\"\n        UC1[\"BufferView: Low-level memory ops\"]\n        UC2[\"TensorView: Matrix/tensor algorithms\"]\n    end\n    \n    BV1 --&gt; UC1\n    TV1 --&gt; UC2\n    \n    style BV1 fill:#dbeafe,stroke:#3b82f6,stroke-width:2px\n    style TV1 fill:#fce7f3,stroke:#ec4899,stroke-width:2px\n\n\nC++ Advanced Features\n__device__ void advanced_tensor_operations()\n{\n    // Create a 4x6 matrix\n    float data[24];\n    auto tensor = make_naive_tensor_view&lt;address_space_enum::global&gt;(\n        data, make_tuple(4, 6), make_tuple(6, 1)\n    );\n    \n    // Slicing and windowing require tile_window\n    // Create a tile window for block access\n    auto window = make_tile_window(\n        tensor,\n        make_tuple(2, 3),    // window shape\n        make_tuple(1, 2)     // window origin\n    );\n    \n    // Vectorized access using coordinates\n    auto coord = make_tensor_coordinate(\n        tensor.get_tensor_descriptor(),\n        make_tuple(1, 0)\n    );\n    // Load 4 consecutive elements as float4\n    using float4 = ck_tile::vector_type&lt;float, 4&gt;::type;\n    auto vec4 = tensor.get_vectorized_elements&lt;float4&gt;(coord, 0);\n}",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#performance-considerations",
    "href": "concepts/01_tensor_view.html#performance-considerations",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\ngraph LR\n    subgraph \"Memory Access Patterns\"\n        Seq[\"Sequential Access(Good cache usage)\"]\n        Stride[\"Strided Access(May cause cache misses)\"]\n        Random[\"Random Access(Poor cache usage)\"]\n    end\n    \n    subgraph \"Optimization Strategies\"\n        Opt1[\"Use row-major for row iteration\"]\n        Opt2[\"Use col-major for column iteration\"]\n        Opt3[\"Minimize stride between accesses\"]\n        Opt4[\"Vectorize when possible\"]\n    end\n    \n    Seq --&gt; Opt1\n    Stride --&gt; Opt2\n    Stride --&gt; Opt3\n    Random --&gt; Opt4\n    \n    style Seq fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Stride fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style Random fill:#fee2e2,stroke:#ef4444,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#summary",
    "href": "concepts/01_tensor_view.html#summary",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Summary",
    "text": "Summary\nTensorView provides:\n\nMulti-dimensional indexing: Natural coordinate-based access\nFlexible memory layouts: Row-major, column-major, custom strides\nZero-copy views: Transpose, slice, reshape without copying data\nType safety: Dimensions encoded in type (C++)\nIntegration: Works seamlessly with BufferView and transforms\n\nThe abstraction enables writing dimension-agnostic algorithms while maintaining high performance through compile-time optimizations.",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html",
    "href": "concepts/02_adaptors.html",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "",
    "text": "While individual transforms are powerful, TensorAdaptors let us chain multiple transforms together to create complex coordinate transformations. Think of adaptors as transformation pipelines that can reshape, reorder, and restructure tensors in sophisticated ways.\nTensorAdaptors are the bridge between individual transforms and the high-level tensor operations you’ll use in real applications.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#overview",
    "href": "concepts/02_adaptors.html#overview",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "",
    "text": "While individual transforms are powerful, TensorAdaptors let us chain multiple transforms together to create complex coordinate transformations. Think of adaptors as transformation pipelines that can reshape, reorder, and restructure tensors in sophisticated ways.\nTensorAdaptors are the bridge between individual transforms and the high-level tensor operations you’ll use in real applications.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#tensoradaptor-basics",
    "href": "concepts/02_adaptors.html#tensoradaptor-basics",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "TensorAdaptor Basics",
    "text": "TensorAdaptor Basics\nLet’s start by understanding what a TensorAdaptor is and how it works:\n\ngraph TB\n    subgraph \"Adaptor Composition\"\n        subgraph \"Single Transform\"\n            T1[\"Transform(e.g., Transpose)\"]\n            I1[\"Input Coords[0,1,2]\"]\n            O1[\"Output Coords[2,0,1]\"]\n        end\n        \n        subgraph \"Chained Transforms\"\n            T2A[\"Transform A(e.g., Merge)\"]\n            T2B[\"Transform B(e.g., Pad)\"]\n            I2[\"Input2D\"]\n            M2[\"Intermediate1D\"]\n            O2[\"Output1D Padded\"]\n        end\n        \n        subgraph \"Complex Pipeline\"\n            T3A[\"Transpose\"]\n            T3B[\"Unmerge\"]\n            T3C[\"Pad\"]\n            I3[\"Input\"]\n            O3[\"Output\"]\n        end\n    end\n    \n    I1 --&gt; T1\n    T1 --&gt; O1\n    \n    I2 --&gt; T2A\n    T2A --&gt; M2\n    M2 --&gt; T2B\n    T2B --&gt; O2\n    \n    I3 --&gt; T3A\n    T3A --&gt; T3B\n    T3B --&gt; T3C\n    T3C --&gt; O3\n    \n    style T1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style T2A fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style T2B fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style T3A fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style T3B fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style T3C fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\n\n\n\n\n\nThe most important method of a TensorAdaptor is calculate_bottom_index, which calculates the lower index from the upper index. It achives this by applying the transforms in reverse order and calling calculate_lower_index on each transform.\nLet’s go over some of the utility functions for creating tensor adaptors and see how they work in real life. We start with one of the simplest one.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#transpose-adaptor-dimension-reordering",
    "href": "concepts/02_adaptors.html#transpose-adaptor-dimension-reordering",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Transpose Adaptor: Dimension Reordering",
    "text": "Transpose Adaptor: Dimension Reordering\nThe transpose adaptor reorders tensor dimensions according to a permutation pattern.\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel - create transpose adaptor\n// Transpose adaptor example: [0, 1, 2] → [2, 0, 1]\nauto transpose_adaptor = make_identity_tensor_adaptor&lt;3&gt;();  // Start with identity\n\n// Apply transpose using transform_tensor_adaptor\n// In CK, transpose is typically done through tensor descriptor transformations\nauto transposed_desc = transform_tensor_descriptor(\n    original_desc,\n    make_tuple(make_pass_through_transform(original_desc.get_length(2)),\n               make_pass_through_transform(original_desc.get_length(0)),\n               make_pass_through_transform(original_desc.get_length(1))),\n    make_tuple(sequence&lt;2&gt;{}, sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),  // old dims\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}, sequence&lt;2&gt;{})   // new dims\n);\n\n// Alternative: Direct coordinate transformation\nmulti_index&lt;3&gt; top_coord{0, 1, 2};\n// After transpose [2, 0, 1]: coord becomes [2, 0, 1]",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#single-stage-adaptors-custom-transform-chains",
    "href": "concepts/02_adaptors.html#single-stage-adaptors-custom-transform-chains",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Single-Stage Adaptors: Custom Transform Chains",
    "text": "Single-Stage Adaptors: Custom Transform Chains\nYou can create custom adaptors by specifying exactly which transforms to use and how they connect, the API for that is called make_single_stage_tensor_adaptor:\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel - create single-stage tensor adaptor\n// Note: In CK, adaptors are typically created through tensor descriptors\n\n// Create a descriptor that merges 2x3 dimensions into single dimension\nauto base_desc = make_naive_tensor_descriptor_packed(make_tuple(2, 3));\n\n// Apply merge transform\nauto merged_desc = transform_tensor_descriptor(\n    base_desc,\n    make_tuple(make_merge_transform(make_tuple(2, 3))),\n    make_tuple(sequence&lt;0, 1&gt;{}),  // merge dims 0,1\n    make_tuple(sequence&lt;0&gt;{})      // to single dim 0\n);\n\n// The adaptor is embedded in the descriptor\n// To use it:\nmulti_index&lt;1&gt; top_coord{5};  // 1D coordinate\n// This internally calculates: row = 5/3 = 1, col = 5%3 = 2\nNow that we saw how we can create an adaptor, let’s see how we can combine a few of them together.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#chaining-adaptors-building-complex-transformations",
    "href": "concepts/02_adaptors.html#chaining-adaptors-building-complex-transformations",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Chaining Adaptors: Building Complex Transformations",
    "text": "Chaining Adaptors: Building Complex Transformations\nThe real power comes from chaining multiple adaptors together to create sophisticated transformations. Below we try some trivial example of combining merge and unmerge just to show how these transformations combine.\n\ngraph LR\n    subgraph \"Adaptor Chaining Flow\"\n        subgraph \"Adaptor 1\"\n            A1I[\"Bottom Dims[0,1]\"]\n            A1T[\"Transform:Merge[2,3]\"]\n            A1O[\"Top Dims[0]\"]\n        end\n        \n        subgraph \"Adaptor 2\"\n            A2I[\"Bottom Dims[0]\"]\n            A2T[\"Transform:Unmerge[2,3]\"]\n            A2O[\"Top Dims[0,1]\"]\n        end\n        \n        subgraph \"Chained Result\"\n            CI[\"Input 2DBottom[0,1]\"]\n            CO[\"Output 2DTop[0,1]\"]\n        end\n    end\n    \n    A1I --&gt; A1T\n    A1T --&gt; A1O\n    A1O --&gt; A2I\n    A2I --&gt; A2T\n    A2T --&gt; A2O\n    \n    CI --&gt; A1I\n    A2O --&gt; CO\n    \n    style A1T fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style A2T fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style CI fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style CO fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel - chaining adaptors through descriptors\n\n// Start with a 2D descriptor\nauto desc1 = make_naive_tensor_descriptor_packed(make_tuple(2, 3));\n\n// First transformation: merge 2D to 1D\nauto merged_desc = transform_tensor_descriptor(\n    desc1,\n    make_tuple(make_merge_transform(make_tuple(2, 3))),\n    make_tuple(sequence&lt;0, 1&gt;{}),  // merge dims 0,1\n    make_tuple(sequence&lt;0&gt;{})      // to dim 0\n);\n\n// Second transformation: unmerge 1D back to 2D\nauto final_desc = transform_tensor_descriptor(\n    merged_desc,\n    make_tuple(make_unmerge_transform(make_tuple(2, 3))),\n    make_tuple(sequence&lt;0&gt;{}),     // from dim 0\n    make_tuple(sequence&lt;0, 1&gt;{})   // to dims 0,1\n);\n\n// The chained transformation is embedded in final_desc\n// Result should be identity transformation",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#transform-addition-extending-existing-adaptors",
    "href": "concepts/02_adaptors.html#transform-addition-extending-existing-adaptors",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Transform Addition: Extending Existing Adaptors",
    "text": "Transform Addition: Extending Existing Adaptors\nYou can add new transforms to existing adaptors using transform_tensor_adaptor. Important: The new_upper_dimension_new_top_idss parameter controls the final output dimensions of the adaptor.\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel - transform tensor adaptor pattern\n\n// Start with transposed descriptor\nauto base_desc = make_naive_tensor_descriptor(\n    make_tuple(3, 4),\n    make_tuple(1, 3)   // transposed strides\n);\n\n// Add padding to both dimensions\nauto padded_desc = transform_tensor_descriptor(\n    base_desc,\n    make_tuple(make_pad_transform(3, 1, 1),   // pad dim 0: 3 → 5\n               make_pad_transform(4, 0, 0)),   // keep dim 1: 4 → 4\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),  // input dims\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{})   // output dims (keep 2D)\n);\n\n// Access pattern\nmulti_index&lt;2&gt; padded_coord{1, 2};  // In padded space\n// Internally calculates: unpadded = [1-1, 2] = [0, 2]\n// Then applies transpose strides",
<<<<<<< HEAD
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#advanced-c-patterns",
    "href": "concepts/02_adaptors.html#advanced-c-patterns",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Advanced C++ Patterns",
    "text": "Advanced C++ Patterns\n\nComplex Nested Transforms in C++\n// From composable_kernel - complex nested transform patterns\n\n// Example: 4D tensor with complex transformations\n// Shape: [A, B, C, D] with various transforms\n\n// 1. Create base descriptor\nauto base_desc = make_naive_tensor_descriptor_packed(\n    make_tuple(A, B, C, D)\n);\n\n// 2. Apply multiple transformations\n// First: merge first 3 dimensions\nauto step1_desc = transform_tensor_descriptor(\n    base_desc,\n    make_tuple(make_merge_transform(make_tuple(A, B, C)),\n               make_pass_through_transform(D)),\n    make_tuple(sequence&lt;0, 1, 2&gt;{}, sequence&lt;3&gt;{}),  // input mapping\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{})         // output: 2D\n);\n\n// 3. Then unmerge back but with different grouping\nauto step2_desc = transform_tensor_descriptor(\n    step1_desc,\n    make_tuple(make_unmerge_transform(make_tuple(A*B, C)),\n               make_pass_through_transform(D)),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),        // from 2D\n    make_tuple(sequence&lt;0, 1&gt;{}, sequence&lt;2&gt;{})      // to 3D\n);\n\n// The adaptor chain is embedded in the descriptors\n// CK optimizes these at compile time\n\n\nGPU Memory Layout Example\n// From composable_kernel - typical GPU block descriptor pattern\n\n// Create descriptor for thread block tile: 64x64\n// With 8x8 vector loads per thread\nconstexpr auto BlockM = 64;\nconstexpr auto BlockN = 64;\nconstexpr auto VectorM = 8;\nconstexpr auto VectorN = 8;\n\n// Thread arrangement: 8x8 threads\nconstexpr auto ThreadM = BlockM / VectorM;  // 8\nconstexpr auto ThreadN = BlockN / VectorN;  // 8\n\n// Create block descriptor with proper layout\nauto block_desc = transform_tensor_descriptor(\n    make_naive_tensor_descriptor_packed(\n        make_tuple(number&lt;BlockM&gt;{}, number&lt;BlockN&gt;{})\n    ),\n    make_tuple(\n        make_unmerge_transform(make_tuple(\n            number&lt;ThreadM&gt;{}, number&lt;VectorM&gt;{}\n        )),\n        make_unmerge_transform(make_tuple(\n            number&lt;ThreadN&gt;{}, number&lt;VectorN&gt;{}\n        ))\n    ),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),           // from 2D\n    make_tuple(sequence&lt;0, 2&gt;{}, sequence&lt;1, 3&gt;{})     // to 4D: [TM,TN,VM,VN]\n);\n\n// This creates the layout:\n// - Dimension 0,1: Thread indices\n// - Dimension 2,3: Vector indices within thread\n// Enables coalesced memory access on GPU",
=======
>>>>>>> df61344 (pushes the renderes after rebase)
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#summary",
    "href": "concepts/02_adaptors.html#summary",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Summary",
    "text": "Summary\nTensorAdaptors are the coordination layer that makes complex tensor operations possible:\n\nIdentity Adaptor: Starting point for building transformations\nTranspose Adaptor: Dimension reordering with permutation patterns\nSingle-Stage Adaptors: Custom transform chains with precise control\nChained Adaptors: Complex multi-stage transformation pipelines\nTransform Addition: Extending existing adaptors with new transforms\nAdvanced Examples: Complex nested transforms with flattening behavior\nGPU Block Descriptors: Real-world GPU memory layout patterns\nC++ Equivalents: True working equivalent of complex nested C++ transforms\n\nKey concepts: - Bottom/Top Dimensions: Input and output coordinate spaces - Hidden Dimensions: Internal coordinate mappings between transforms - Transform Chains: Sequential application of multiple transforms - Coordinate Transformation: Bidirectional mapping between coordinate spaces - Nested Transforms: Complex multi-level transformation hierarchies\n\nBreakthrough Discovery\nWe successfully created the true C++ equivalent of complex nested transforms:\n# C++ nested transform equivalent\ncpp_equivalent = make_single_stage_tensor_adaptor(\n    transforms=[\n        UnmergeTransform([A, B, C]),  # Converts 3D (A,B,C) to 1D linear  \n        PassThroughTransform(D)       # Passes through D dimension\n    ],\n    lower_dimension_old_top_idss=[[0], [1]],          # Transform inputs\n    upper_dimension_new_top_idss=[[0, 1, 2], [3]]     # Transform outputs\n)\nKey insights: - Transform direction: Names refer to lower→higher, but calculate_lower_index() goes higher→lower - UnmergeTransform: Converts multi-D to linear when used with calculate_lower_index() - Parameter mapping: Controls the coordinate flow between dimensions - Mathematical equivalence: Exact same results as C++ nested structure\nTensorAdaptors bridge the gap between low-level transforms and high-level tensor operations, providing the flexibility to create sophisticated data layouts and access patterns that are essential for efficient GPU computing.\n\n\nKey C++ Patterns in Composable Kernel\n\nDescriptor-Based Adaptors: In CK, adaptors are typically embedded within tensor descriptors rather than created separately\nCompile-Time Optimization: All transformations are resolved at compile time for zero overhead\nType Safety: Template metaprogramming ensures coordinate transformations are type-safe\nGPU Optimization: Transform chains are designed for efficient GPU memory access patterns\n\n\n\nCommon C++ Transform Chains\n// Padding for convolution\nauto padded = transform_tensor_descriptor(\n    input, \n    make_tuple(make_pad_transform(H, pad_h, pad_h),\n               make_pad_transform(W, pad_w, pad_w)),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{})\n);\n\n// Dimension merging for GEMM\nauto merged = transform_tensor_descriptor(\n    input,\n    make_tuple(make_merge_transform(make_tuple(M, K))),\n    make_tuple(sequence&lt;0, 1&gt;{}),\n    make_tuple(sequence&lt;0&gt;{})\n);\n\n// Broadcasting for elementwise ops\nauto broadcast = transform_tensor_descriptor(\n    scalar,\n    make_tuple(make_replicate_transform(make_tuple(M, N))),\n    make_tuple(sequence&lt;&gt;{}),\n    make_tuple(sequence&lt;0, 1&gt;{})\n);\nNext, we’ll see how TensorAdaptors are combined with element space information to create complete TensorDescriptors.",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html",
    "href": "concepts/06_thread_mapping.html",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "",
    "text": "The final piece of the puzzle: how threads get their unique IDs and how that maps to specific data, connecting our mathematical abstractions to physical hardware.\nUp to this point, we’ve learned about encodings, transformations, and distributed tensors. But there’s one crucial question remaining: How do actual GPU threads know which data to process?\nThis is where thread mapping comes in - the bridge between our mathematical abstractions and the physical hardware that executes our code.",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#interactive-exploration",
    "href": "concepts/06_thread_mapping.html#interactive-exploration",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "🎮 Interactive Exploration",
    "text": "🎮 Interactive Exploration\nExplore thread mapping concepts interactively:\n🧵 Thread Visualization App - Visualize GPU thread coordinate mapping and access patterns. Understand how individual threads access distributed tensor data.",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#thread-identification-and-partition-indices",
    "href": "concepts/06_thread_mapping.html#thread-identification-and-partition-indices",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Thread Identification and Partition Indices",
    "text": "Thread Identification and Partition Indices\nBefore threads can process data, they need to know who they are and what work they’re responsible for.\n\nHardware Thread Identification\nIn GPU hardware, threads are organized hierarchically:\n// CUDA/HIP thread identification\n__device__ void get_thread_coordinates()\n{\n    // Grid-level coordinates (which block)\n    int block_x = blockIdx.x;\n    int block_y = blockIdx.y;\n    int block_z = blockIdx.z;\n    \n    // Block-level coordinates (which thread in block)\n    int thread_x = threadIdx.x;\n    int thread_y = threadIdx.y;\n    int thread_z = threadIdx.z;\n    \n    // Warp identification\n    int warp_id = threadIdx.x / 32;  // 32 threads per warp\n    int lane_id = threadIdx.x % 32;  // Position within warp\n    \n    // Global thread ID calculation\n    int global_thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n}\n\n\nC++ Thread Mapping in CK\nComposable Kernel abstracts thread identification into partition indices:\n// From tile_partition.hpp\ntemplate &lt;typename ThreadLayout&gt;\nstruct tile_partition\n{\n    CK_TILE_DEVICE static constexpr index_t get_thread_idx()\n    {\n        return threadIdx.x;\n    }\n    \n    CK_TILE_DEVICE static constexpr index_t get_block_idx()\n    {\n        return blockIdx.x;\n    }\n    \n    // Convert to multi-dimensional partition index\n    template &lt;index_t NumDim&gt;\n    CK_TILE_DEVICE static constexpr auto get_partition_index()\n    {\n        constexpr auto thread_layout = ThreadLayout{};\n        \n        // Convert linear thread ID to multi-dimensional index\n        return thread_layout.template get_index&lt;NumDim&gt;(get_thread_idx());\n    }\n};\n\ngraph TB\n    subgraph \"GPU Device\"\n        subgraph \"Thread Block\"\n            subgraph \"Warp 0\"\n                T0[\"Thread 0lane_id=0\"]\n                T1[\"Thread 1lane_id=1\"]\n                T2[\"...\"]\n                T31[\"Thread 31lane_id=31\"]\n            end\n            \n            subgraph \"Warp 1\"\n                T32[\"Thread 32lane_id=0\"]\n                T33[\"Thread 33lane_id=1\"]\n                T34[\"...\"]\n                T63[\"Thread 63lane_id=31\"]\n            end\n            \n            W2[\"Warp 2\"]\n            W3[\"...\"]\n            W7[\"Warp 7\"]\n        end\n    end\n    \n    subgraph \"Thread Identification\"\n        TID[\"Thread ID = blockIdx.x * blockDim.x + threadIdx.x\"]\n        WID[\"Warp ID = threadIdx.x / 32\"]\n        LID[\"Lane ID = threadIdx.x % 32\"]\n    end\n    \n    subgraph \"P-space Mapping\"\n        P[\"P-coordinatesNDimP=1: [thread_id]NDimP=2: [warp_id, lane_id]\"]\n    end\n    \n    T0 --&gt; TID\n    TID --&gt; WID\n    TID --&gt; LID\n    WID --&gt; P\n    LID --&gt; P\n    \n    style T0 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style T32 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style P fill:#fff3e0,stroke:#f57c00,stroke-width:3px\n\n\n\n\n\n\n\n\n\nThread Hierarchy Structure\nThe hardware organizes threads in a specific hierarchy:\n🔹 Block Level: Groups of warps working together - {warp_per_block_m}×{warp_per_block_n} warps per block - Shared memory and synchronization scope - Block-level coordination possible\n🔹 Warp Level: Groups of threads executing in lockstep - {thread_per_warp_m}×{thread_per_warp_n} threads per warp - SIMD execution (all threads execute same instruction) - Warp-level primitives (shuffle, vote, etc.)\n🔹 Thread Level: Individual execution units - {vector_m}×{vector_n} elements per thread - Independent register space - Vector operations on multiple elements\n\n\nThread ID Mapping\nEach thread gets a unique ID that maps to its position in the hierarchy:",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#thread-to-data-mapping",
    "href": "concepts/06_thread_mapping.html#thread-to-data-mapping",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Thread-to-Data Mapping",
    "text": "Thread-to-Data Mapping\nOnce threads know their IDs, they need to map those IDs to specific data elements.\n\ngraph TB\n    subgraph \"Thread to Data Mapping\"\n        subgraph \"Thread Grid\"\n            T00[\"Thread[0,0]Warp 0\"]\n            T01[\"Thread[0,1]Warp 0\"]\n            T10[\"Thread[1,0]Warp 1\"]\n            T11[\"Thread[1,1]Warp 1\"]\n        end\n        \n        subgraph \"Data Tiles\"\n            D00[\"Data[0:4, 0:4]16 elements\"]\n            D01[\"Data[0:4, 4:8]16 elements\"]\n            D10[\"Data[4:8, 0:4]16 elements\"]\n            D11[\"Data[4:8, 4:8]16 elements\"]\n        end\n        \n        subgraph \"Memory Access\"\n            MA[\"Coalesced AccessAdjacent threads → Adjacent memory\"]\n        end\n    end\n    \n    T00 --&gt; D00\n    T01 --&gt; D01\n    T10 --&gt; D10\n    T11 --&gt; D11\n    \n    D00 --&gt; MA\n    D01 --&gt; MA\n    \n    style T00 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style D00 fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style MA fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\nData Distribution Pattern\nThe RMSNorm operation distributes tensor data across threads in a structured pattern:\n\n\n\n\n\n\n\n\nThread Work Assignment\nEach thread is assigned a specific rectangular region of the tensor:",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#thread-cooperation-patterns",
    "href": "concepts/06_thread_mapping.html#thread-cooperation-patterns",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Thread Cooperation Patterns",
    "text": "Thread Cooperation Patterns\nThreads don’t work in isolation - they cooperate at different levels to achieve optimal performance.\n\nWarp-Level Cooperation\nThreads within a warp execute in lockstep (SIMD):\n🤝 Warp-Level Cooperation - Warps per block: {warp_per_block_m}×{warp_per_block_n} - Threads per warp: {thread_per_warp_m}×{thread_per_warp_n} - Cooperation pattern: Threads within a warp process adjacent data - Synchronization: Warp-level SIMD execution\n\n\nBlock-Level Cooperation\nThreads within a block can share data and synchronize:\n🏗️ Block-Level Cooperation - Shared memory: All threads in block can access shared memory - Synchronization: __syncthreads() barriers available - Data sharing: Threads can exchange intermediate results - Collective operations: Reduction, broadcast across block\n\n\nVector-Level Processing\nEach thread processes multiple elements efficiently:\n⚡ Vector-Level Processing - Elements per thread: {vector_m}×{vector_n} elements - Memory coalescing: Adjacent threads access adjacent memory - Vectorization: Hardware can combine multiple operations - Register efficiency: Multiple elements in registers",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#memory-access-patterns",
    "href": "concepts/06_thread_mapping.html#memory-access-patterns",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Memory Access Patterns",
    "text": "Memory Access Patterns\nThe thread mapping directly affects memory access efficiency.\n\nC++ Implementation of Memory Access\nHere’s how CK implements efficient memory access patterns:\n// Coalesced memory access pattern\ntemplate &lt;typename DataType, index_t VectorSize&gt;\n__device__ void coalesced_load(const DataType* __restrict__ src,\n                               DataType* __restrict__ dst,\n                               index_t tid)\n{\n    // Each thread loads VectorSize elements\n    // Adjacent threads access adjacent memory\n    constexpr index_t stride = blockDim.x;\n    \n    // Vectorized load for efficiency\n    using vector_t = vector_type_t&lt;DataType, VectorSize&gt;;\n    \n    // Calculate aligned address\n    const vector_t* src_vec = reinterpret_cast&lt;const vector_t*&gt;(\n        src + tid * VectorSize);\n    \n    // Single vectorized load instruction\n    vector_t data = *src_vec;\n    \n    // Store to registers\n    reinterpret_cast&lt;vector_t*&gt;(dst)[0] = data;\n}\n\n// CK's distributed tensor load implementation\ntemplate &lt;typename DistributedTensor&gt;\n__device__ void load_tile_window(DistributedTensor& dist_tensor,\n                                const auto& tile_window)\n{\n    // Get thread's partition index\n    constexpr auto partition = tile_partition::get_partition_index();\n    \n    // Each thread loads its assigned data\n    tile_window.load(dist_tensor, partition);\n    \n    // Hardware automatically coalesces adjacent thread accesses\n}\n\n\nMemory Access Optimization Techniques\nCK uses several techniques to optimize memory access:\n// 1. Vector loads for maximum bandwidth\ntemplate &lt;index_t N&gt;\nusing vector_load_t = conditional_t&lt;N == 1, float,\n                     conditional_t&lt;N == 2, float2,\n                     conditional_t&lt;N == 4, float4,\n                                           float&gt;&gt;&gt;;\n\n// 2. Swizzling to avoid bank conflicts\ntemplate &lt;index_t BankSize = 32&gt;\n__device__ index_t swizzle_offset(index_t tid, index_t offset)\n{\n    // Rotate access pattern to avoid conflicts\n    return (offset + (tid / BankSize)) % BankSize;\n}\n\n// 3. Prefetching for latency hiding\n__device__ void prefetch_next_tile(const float* src, index_t offset)\n{\n    // Prefetch to L2 cache\n    __builtin_prefetch(src + offset, 0, 3);\n}\n\n\nCoalesced Memory Access\n\n\n\n\n\n\n\n\nMemory Efficiency Benefits\nThe structured thread mapping provides several memory efficiency benefits:\n🎯 Memory Coalescing Benefits: - Adjacent access: Threads in same warp access adjacent memory locations - Cache efficiency: Related data loaded together into cache lines - Bandwidth utilization: Maximum memory bandwidth achieved - Reduced latency: Fewer memory transactions needed\n⚡ Performance Characteristics: - Predictable patterns: Access patterns known at compile time - Vectorization: Hardware can optimize vector operations - Reduced overhead: No complex address calculations at runtime - Scalability: Pattern scales efficiently with thread count",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#practical-thread-mapping-example",
    "href": "concepts/06_thread_mapping.html#practical-thread-mapping-example",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Practical Thread Mapping Example",
    "text": "Practical Thread Mapping Example\n\nComplete C++ Kernel Example\nHere’s a complete example showing how thread mapping works in a real CK kernel:\n// RMSNorm kernel using CK's thread mapping\ntemplate &lt;typename DataType,\n          typename ComputeType,\n          index_t BlockSize,\n          index_t VectorSize&gt;\n__global__ void rmsnorm_kernel(const DataType* __restrict__ x,\n                              DataType* __restrict__ y,\n                              const DataType* __restrict__ weight,\n                              ComputeType epsilon,\n                              index_t hidden_size)\n{\n    // 1. Thread identification\n    const index_t tid = threadIdx.x;\n    const index_t bid = blockIdx.x;\n    \n    // 2. Create tile distribution encoding\n    // This would be defined based on your specific RMSNorm pattern\n    using Encoding = tile_distribution_encoding&lt;\n        sequence&lt;&gt;,                          // No replication\n        tuple&lt;sequence&lt;4, 2&gt;, sequence&lt;4, 2&gt;&gt;, // H dimensions\n        tuple&lt;sequence&lt;1&gt;, sequence&lt;2&gt;&gt;,     // P to RH major\n        tuple&lt;sequence&lt;0&gt;, sequence&lt;0&gt;&gt;,     // P to RH minor\n        sequence&lt;1, 2&gt;,                      // Y to RH major\n        sequence&lt;0, 0&gt;                       // Y to RH minor\n    &gt;;\n    constexpr auto tile_dist = make_static_tile_distribution(Encoding{});\n    \n    // 3. Get thread's partition index from distribution\n    const auto partition_idx = tile_dist._get_partition_index();\n    \n    // 4. Shared memory for reduction\n    __shared__ ComputeType shared_sum[BlockSize];\n    \n    // 5. Create tensor view and tile window\n    auto x_view = make_naive_tensor_view&lt;address_space_enum::global&gt;(\n        x + bid * hidden_size,\n        make_tuple(hidden_size),\n        make_tuple(number&lt;1&gt;{})\n    );\n    \n    auto x_window = make_tile_window(\n        x_view,\n        make_tuple(hidden_size),\n        make_tuple(number&lt;0&gt;{}),\n        tile_dist);\n    \n    // 6. Each thread processes its assigned elements\n    ComputeType thread_sum = 0;\n    static_for&lt;0, VectorSize, 1&gt;{}([&](auto i) {\n        // Access pattern would depend on your tile window setup\n        // This is conceptual - actual implementation varies\n        thread_sum += val * val;\n    });\n    \n    // 7. Warp-level reduction\n    thread_sum = warp_reduce_sum&lt;WarpSize&gt;(thread_sum);\n    \n    // 8. Block-level reduction\n    if (tid % WarpSize == 0) {\n        shared_sum[tid / WarpSize] = thread_sum;\n    }\n    __syncthreads();\n    \n    // 9. Final reduction by first warp\n    if (tid &lt; BlockSize / WarpSize) {\n        thread_sum = shared_sum[tid];\n        thread_sum = warp_reduce_sum&lt;BlockSize / WarpSize&gt;(thread_sum);\n    }\n    \n    // 10. Compute RMS and normalize\n    if (tid == 0) {\n        shared_sum[0] = rsqrt(thread_sum / hidden_size + epsilon);\n    }\n    __syncthreads();\n    \n    const ComputeType rms_recip = shared_sum[0];\n    \n    // 11. Write normalized output\n    auto y_window = make_tile_window(\n        make_tensor_view&lt;address_space_enum::global&gt;(y + bid * hidden_size),\n        tile_dist);\n    \n    static_for&lt;0, VectorSize, 1&gt;{}([&](auto i) {\n        auto idx = tile_dist.get_tensor_coordinate(partition_idx, i);\n        ComputeType val = static_cast&lt;ComputeType&gt;(x_window.get(idx));\n        ComputeType w = static_cast&lt;ComputeType&gt;(weight[idx[1]]);\n        y_window.set(idx, static_cast&lt;DataType&gt;(val * rms_recip * w));\n    });\n}\n\n\nKey Thread Mapping Concepts in Action\n\nThread-to-Data Assignment: Each thread gets a unique partition_idx\nVectorized Access: Each thread processes VectorSize elements\nWarp Cooperation: Threads within a warp perform reductions\nBlock Synchronization: All threads synchronize for final result\nCoalesced Memory: Adjacent threads access adjacent memory",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#practical-thread-mapping-example-1",
    "href": "concepts/06_thread_mapping.html#practical-thread-mapping-example-1",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Practical Thread Mapping Example",
    "text": "Practical Thread Mapping Example\nLet’s see how thread mapping works in practice with a complete example:",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#testing-your-understanding",
    "href": "concepts/06_thread_mapping.html#testing-your-understanding",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Testing Your Understanding",
    "text": "Testing Your Understanding\nLet’s verify your understanding of thread mapping concepts:",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/06_thread_mapping.html#key-takeaways",
    "href": "concepts/06_thread_mapping.html#key-takeaways",
    "title": "Thread Mapping - Connecting to Hardware",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nThread mapping is the crucial bridge between mathematical abstractions and physical hardware execution:\n🎯 Thread Identification:\n\nHierarchical Organization: Threads organized in blocks → warps → threads → vectors\n\n✅ Each level has specific cooperation capabilities\n✅ Hardware provides efficient primitives at each level\n✅ Thread IDs map directly to data regions\n✅ Predictable and efficient execution patterns\n\nData Assignment: Each thread gets a specific rectangular region\n\n✅ Work distributed evenly across threads\n✅ Memory access patterns optimized for coalescing\n✅ Vector operations maximize throughput\n✅ Scalable across different hardware configurations\n\nCooperation Patterns: Threads cooperate at multiple levels\n\n✅ Warp-level SIMD execution for efficiency\n✅ Block-level shared memory and synchronization\n✅ Vector-level processing for maximum throughput\n✅ Hierarchical coordination for complex operations\n\n\n🚀 Performance Benefits:\n\nMemory Coalescing: Adjacent threads access adjacent memory for optimal bandwidth\nCache Efficiency: Related data loaded together, reducing memory latency\nVectorization: Hardware can optimize multiple operations per thread\nPredictable Patterns: Compile-time optimization of access patterns\n\n💡 Why This Matters:\nThread mapping connects all the previous concepts (encodings, transformations, distributions) to actual hardware execution. It’s the final piece that makes tile distribution practical for real-world GPU programming.\nThe RMSNorm example shows how a real operation uses these concepts to achieve optimal performance on modern GPU hardware. Every thread knows exactly what data to process, how to access it efficiently, and how to cooperate with other threads - all determined by the mathematical encoding we started with!\nThis completes the journey from basic memory concepts to hardware-optimized execution. You now understand the complete tile distribution system from mathematical foundations to practical implementation.",
    "crumbs": [
      "Thread Mapping",
      "Thread Mapping - Connecting to Hardware"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html",
    "href": "concepts/02_convolution_example.html",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "This chapter demonstrates a practical application of tensor descriptors by implementing convolution operations. We’ll progress from a naive implementation to an optimized approach using tensor descriptors, showing how they enable efficient memory access patterns for GPU acceleration. First we show how we can achieve the results using numpy implementation.\nThe convolution operation is fundamental in deep learning, and understanding its implementation details reveals how high-performance libraries achieve their efficiency. We’ll explore:\n\nNaive Implementation: Direct nested loops for reference\nWindow Extraction: Using NumPy’s as_strided for overlapping windows\nTensor Descriptor Windows: Achieving the same with tensor descriptors\nIm2col Transformation: Converting convolution to matrix multiplication\nMulti-channel Extension: Handling realistic deep learning scenarios\n\n\ngraph TB\n    subgraph \"Convolution Process\"\n        I[\"Input Image6×6\"]\n        K[\"Kernel3×3\"]\n        SW[\"Sliding WindowExtract 3×3 patches\"]\n        DP[\"Dot ProductElement-wise multiply & sum\"]\n        O[\"Output4×4\"]\n    end\n    \n    subgraph \"Im2col Optimization\"\n        W[\"Windows Matrix16×9(all patches)\"]\n        KF[\"Kernel Flattened9×1\"]\n        MM[\"Matrix MultiplyW @ K\"]\n        OF[\"Output Flattened16×1\"]\n    end\n    \n    I --&gt; SW\n    K --&gt; DP\n    SW --&gt; DP\n    DP --&gt; O\n    \n    SW --&gt; W\n    K --&gt; KF\n    W --&gt; MM\n    KF --&gt; MM\n    MM --&gt; OF\n    OF --&gt; O\n    \n    style I fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style O fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style MM fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into convolution, let’s understand how as_strided works with a simple example. We’ll start by tiling our matrix into non-overlapping blocks.\n\n\n\n\n\n\n\n\nThe key insight is understanding strides - how many bytes to skip to move to the next element in each dimension:\n\nOriginal matrix: shape=(6, 6), strides=(48, 8) (8 bytes per int64, 6 elements per row)\nTiled view: shape=(3, 3, 2, 2), strides=(96, 16, 48, 8)\n\nTo move to next tile row: skip 2 matrix rows = 48 × 2 = 96 bytes\nTo move to next tile col: skip 2 matrix cols = 8 × 2 = 16 bytes\n\nWithin tile: use original strides (48, 8)\n\n\n\n\n\n\nNow let’s see how to create overlapping windows - the foundation of convolution:\n\n\n\n\n\n\n\n\n\nNon-overlapping tiles: We skip by tile_size in strides: (stride[0] * tile_size, stride[1] * tile_size, ...)\nOverlapping windows: We skip by 1 in strides: (stride[0] * 1, stride[1] * 1, ...)\n\nThis creates sliding windows that overlap, which is exactly what we need for convolution!\n\n\n\n\nLet’s start with the most straightforward implementation using nested loops:\n\n\n\n\n\n\nThis implementation directly follows the mathematical definition of convolution. For each output position, we extract the corresponding window from the input image and compute the element-wise product with the kernel.\n\n\n\nNow let’s apply what we learned about overlapping windows to convolution. We need to extract all 3×3 windows for convolution:\n\n\n\n\n\n\nPerfect! We now have a 4D tensor [4, 4, 3, 3] containing all 16 convolution windows. Each [i, j, :, :] slice contains the window at output position (i, j).\n\n\n\nNow let’s achieve the same result using tensor descriptors, we have seen a similar results before when we learnt about the EmbedTransform that is behind make_naive_tensor_descriptor.\n\n\n\n\n\n\nThe key insight is the stride pattern [W, 1, W, 1]:\n\nMoving one step in out_h direction requires jumping W elements (one row)\nMoving one step in out_w direction requires jumping 1 element (one column)\n\nMoving one step in kernel_h direction requires jumping W elements (one row)\nMoving one step in kernel_w direction requires jumping 1 element (one column)\n\n\n\nWe can see that we get the same results as we do in numpy approach.\n\n\n\n\n\n\n\n\n\n\nThe next step is converting our 4D windows to a 2D matrix format suitable for matrix multiplication. This is called the “im2col” (image to column) transformation. This can be done by using reshape operator of numpy.\n\n\n\n\n\n\nEach row of the im2col matrix contains a flattened convolution window. This transformation allows us to compute all convolutions simultaneously using a single matrix multiplication.\n\n\n\nSince we already extracted windows using tensor descriptors, we can simply reshape them just like the NumPy version, let’s see how we can do it using the transformation pipelines.\n\n\nSince we already extracted the 4D windows using a tensor descriptor, the simplest way to get the im2col matrix is to just reshape the result, similar to how we handled the NumPy array.\n\n\n\n\n\n\nPerfect! This is exactly the same as the NumPy approach - just reshape the 4D windows into a 2D matrix.\n\n\n\nHowever, tensor descriptors can also create the im2col layout directly without intermediate 4D windows. This is useful for GPU implementations:\n\n\n\n\n\n\n\n\n\n\nSimple reshape: Easy to understand, perfect for CPU implementations\nDirect tensor descriptor: Enables efficient GPU kernel generation where the hardware can directly compute memory addresses for the im2col layout without materializing intermediate 4D arrays\n\nThe advanced direct tensor descriptor approach uses two MergeTransform operations: 1. Merge spatial dimensions: [out_h, out_w] → num_windows 2. Merge kernel dimensions: [K, K] → patch_size\nThis transforms the 4D tensor [out_h, out_w, K, K] directly into a 2D matrix [num_windows, patch_size] without materializing the intermediate 4D array.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith our im2col matrix ready, we can now perform convolution using simple matrix multiplication:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReal-world deep learning scenarios involve multiple input and output channels. Let’s extend our approach:\n\n\n\n\n\n\nFor multi-channel convolution:\n\nInput: [H, W, C_in]\nFilters: [K, K, C_in, C_out]\nIm2col matrix: [num_windows, K×K×C_in]\nReshaped filters: [K×K×C_in, C_out]\nOutput: [out_h, out_w, C_out]\n\nThe same im2col principle applies, but now each window includes all input channels, and we can compute all output channels simultaneously.\n\n\n\nWe’ve demonstrated the complete evolution from naive convolution to optimized tensor descriptor-based implementation:\n\nNaive approach: Direct mathematical implementation with nested loops\nWindow extraction: Using as_strided to create efficient memory views\nTensor descriptors: Achieving the same with structured transformations\nIm2col transformation: Converting convolution to matrix multiplication\nMulti-channel extension: Scaling to realistic deep learning scenarios\n\n\n\n\nMemory efficiency: Tensor descriptors avoid data duplication by creating views\nParallelization: Im2col enables massive parallelization through matrix multiplication\nGeneralization: The tensor descriptor approach extends naturally to complex memory patterns\nGPU acceleration: These transformations form the foundation for efficient GPU kernels\n\nThe tensor descriptor system provides a unified framework for describing these transformations, making it possible to generate efficient code for various hardware architectures automatically. It is also important to note that the tensor descriptor machinary is implmented in compile time C++ code, therefore very efficient. This python implementation is just a simulator to demonstrate the concept.\n\n\n\n\n\n\nMethod\nMemory Usage\nParallelization\nGPU Suitability\n\n\n\n\nNaive loops\nLow\nPoor\nPoor\n\n\nAs_strided\nMedium\nGood\nLimited\n\n\nTensor descriptors\nMedium\nExcellent\nExcellent\n\n\nIm2col\nHigh\nExcellent\nExcellent\n\n\n\nTensor descriptors strike the optimal balance: they provide the parallelization benefits of im2col while maintaining the memory efficiency of strided operations, making them ideal for high-performance GPU implementations.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#understanding-as_strided-simple-tiling-first",
    "href": "concepts/02_convolution_example.html#understanding-as_strided-simple-tiling-first",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Before diving into convolution, let’s understand how as_strided works with a simple example. We’ll start by tiling our matrix into non-overlapping blocks.\n\n\n\n\n\n\n\n\nThe key insight is understanding strides - how many bytes to skip to move to the next element in each dimension:\n\nOriginal matrix: shape=(6, 6), strides=(48, 8) (8 bytes per int64, 6 elements per row)\nTiled view: shape=(3, 3, 2, 2), strides=(96, 16, 48, 8)\n\nTo move to next tile row: skip 2 matrix rows = 48 × 2 = 96 bytes\nTo move to next tile col: skip 2 matrix cols = 8 × 2 = 16 bytes\n\nWithin tile: use original strides (48, 8)",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#overlapping-windows-with-as_strided",
    "href": "concepts/02_convolution_example.html#overlapping-windows-with-as_strided",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Now let’s see how to create overlapping windows - the foundation of convolution:\n\n\n\n\n\n\n\n\n\nNon-overlapping tiles: We skip by tile_size in strides: (stride[0] * tile_size, stride[1] * tile_size, ...)\nOverlapping windows: We skip by 1 in strides: (stride[0] * 1, stride[1] * 1, ...)\n\nThis creates sliding windows that overlap, which is exactly what we need for convolution!",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#naive-convolution-reference",
    "href": "concepts/02_convolution_example.html#naive-convolution-reference",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Let’s start with the most straightforward implementation using nested loops:\n\n\n\n\n\n\nThis implementation directly follows the mathematical definition of convolution. For each output position, we extract the corresponding window from the input image and compute the element-wise product with the kernel.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#window-extraction-with-numpy-as_strided",
    "href": "concepts/02_convolution_example.html#window-extraction-with-numpy-as_strided",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Now let’s apply what we learned about overlapping windows to convolution. We need to extract all 3×3 windows for convolution:\n\n\n\n\n\n\nPerfect! We now have a 4D tensor [4, 4, 3, 3] containing all 16 convolution windows. Each [i, j, :, :] slice contains the window at output position (i, j).",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#window-extraction-with-tensor-descriptors",
    "href": "concepts/02_convolution_example.html#window-extraction-with-tensor-descriptors",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Now let’s achieve the same result using tensor descriptors, we have seen a similar results before when we learnt about the EmbedTransform that is behind make_naive_tensor_descriptor.\n\n\n\n\n\n\nThe key insight is the stride pattern [W, 1, W, 1]:\n\nMoving one step in out_h direction requires jumping W elements (one row)\nMoving one step in out_w direction requires jumping 1 element (one column)\n\nMoving one step in kernel_h direction requires jumping W elements (one row)\nMoving one step in kernel_w direction requires jumping 1 element (one column)\n\n\n\nWe can see that we get the same results as we do in numpy approach.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#im2col-transformation-with-numpy",
    "href": "concepts/02_convolution_example.html#im2col-transformation-with-numpy",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "The next step is converting our 4D windows to a 2D matrix format suitable for matrix multiplication. This is called the “im2col” (image to column) transformation. This can be done by using reshape operator of numpy.\n\n\n\n\n\n\nEach row of the im2col matrix contains a flattened convolution window. This transformation allows us to compute all convolutions simultaneously using a single matrix multiplication.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#im2col-with-tensor-descriptors",
    "href": "concepts/02_convolution_example.html#im2col-with-tensor-descriptors",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Since we already extracted windows using tensor descriptors, we can simply reshape them just like the NumPy version, let’s see how we can do it using the transformation pipelines.\n\n\nSince we already extracted the 4D windows using a tensor descriptor, the simplest way to get the im2col matrix is to just reshape the result, similar to how we handled the NumPy array.\n\n\n\n\n\n\nPerfect! This is exactly the same as the NumPy approach - just reshape the 4D windows into a 2D matrix.\n\n\n\nHowever, tensor descriptors can also create the im2col layout directly without intermediate 4D windows. This is useful for GPU implementations:\n\n\n\n\n\n\n\n\n\n\nSimple reshape: Easy to understand, perfect for CPU implementations\nDirect tensor descriptor: Enables efficient GPU kernel generation where the hardware can directly compute memory addresses for the im2col layout without materializing intermediate 4D arrays\n\nThe advanced direct tensor descriptor approach uses two MergeTransform operations: 1. Merge spatial dimensions: [out_h, out_w] → num_windows 2. Merge kernel dimensions: [K, K] → patch_size\nThis transforms the 4D tensor [out_h, out_w, K, K] directly into a 2D matrix [num_windows, patch_size] without materializing the intermediate 4D array.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#convolution-via-matrix-multiplication",
    "href": "concepts/02_convolution_example.html#convolution-via-matrix-multiplication",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "With our im2col matrix ready, we can now perform convolution using simple matrix multiplication:",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#multi-channel-convolution",
    "href": "concepts/02_convolution_example.html#multi-channel-convolution",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "Real-world deep learning scenarios involve multiple input and output channels. Let’s extend our approach:\n\n\n\n\n\n\nFor multi-channel convolution:\n\nInput: [H, W, C_in]\nFilters: [K, K, C_in, C_out]\nIm2col matrix: [num_windows, K×K×C_in]\nReshaped filters: [K×K×C_in, C_out]\nOutput: [out_h, out_w, C_out]\n\nThe same im2col principle applies, but now each window includes all input channels, and we can compute all output channels simultaneously.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_convolution_example.html#summary",
    "href": "concepts/02_convolution_example.html#summary",
    "title": "Convolution Implementation with Tensor Descriptors",
    "section": "",
    "text": "We’ve demonstrated the complete evolution from naive convolution to optimized tensor descriptor-based implementation:\n\nNaive approach: Direct mathematical implementation with nested loops\nWindow extraction: Using as_strided to create efficient memory views\nTensor descriptors: Achieving the same with structured transformations\nIm2col transformation: Converting convolution to matrix multiplication\nMulti-channel extension: Scaling to realistic deep learning scenarios\n\n\n\n\nMemory efficiency: Tensor descriptors avoid data duplication by creating views\nParallelization: Im2col enables massive parallelization through matrix multiplication\nGeneralization: The tensor descriptor approach extends naturally to complex memory patterns\nGPU acceleration: These transformations form the foundation for efficient GPU kernels\n\nThe tensor descriptor system provides a unified framework for describing these transformations, making it possible to generate efficient code for various hardware architectures automatically. It is also important to note that the tensor descriptor machinary is implmented in compile time C++ code, therefore very efficient. This python implementation is just a simulator to demonstrate the concept.\n\n\n\n\n\n\nMethod\nMemory Usage\nParallelization\nGPU Suitability\n\n\n\n\nNaive loops\nLow\nPoor\nPoor\n\n\nAs_strided\nMedium\nGood\nLimited\n\n\nTensor descriptors\nMedium\nExcellent\nExcellent\n\n\nIm2col\nHigh\nExcellent\nExcellent\n\n\n\nTensor descriptors strike the optimal balance: they provide the parallelization benefits of im2col while maintaining the memory efficiency of strided operations, making them ideal for high-performance GPU implementations.",
    "crumbs": [
      "Transformation Engine",
      "Convolution Implementation with Tensor Descriptors"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html",
    "href": "concepts/02_tensor_coordinates.html",
    "title": "Basic Coordinates",
    "section": "",
    "text": "Before diving into transforms and adaptors, we need to understand the basic coordinate system. MultiIndex is the fundamental building block used throughout the pytensor system.",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#what-is-multiindex",
    "href": "concepts/02_tensor_coordinates.html#what-is-multiindex",
    "title": "Basic Coordinates",
    "section": "What is MultiIndex?",
    "text": "What is MultiIndex?\nMultiIndex represents a position in N-dimensional space - think of it as GPS coordinates for tensors:\n\nSimple: Just stores a list of integers\nFundamental: Used by all transforms, adaptors, and descriptors\nFlexible: Supports copying, comparison, and modification\n\n\nMultiIndex Architecture\n\ngraph TB\n    subgraph \"MultiIndex Structure\"\n        MI[\"MultiIndexContainer for N integers\"]\n        D0[\"Dimension 0X coordinate\"]\n        D1[\"Dimension 1Y coordinate\"]\n        D2[\"Dimension 2Z coordinate\"]\n        DN[\"Dimension N-1...\"]\n    end\n\n    subgraph \"Usage Context\"\n        T[\"TransformsInput/Output\"]\n        A[\"AdaptorsPosition Tracking\"]\n        TV[\"TensorsElement Access\"]\n    end\n\n    subgraph \"Operations\"\n        Create[\"Create: MultiIndex(size, values)\"]\n        Access[\"Access: coord[i]\"]\n        Modify[\"Modify: coord[i] = value\"]\n        Convert[\"Convert: to_list()\"]\n    end\n\n    MI --&gt; D0\n    MI --&gt; D1\n    MI --&gt; D2\n    MI --&gt; DN\n\n    T --&gt; MI\n    A --&gt; MI\n    TV --&gt; MI\n\n    style MI fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px\n    style D0 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style D1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style D2 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style DN fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style T fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style A fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style TV fill:#ffebee,stroke:#d32f2f,stroke-width:2px",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#creating-and-using-multiindex",
    "href": "concepts/02_tensor_coordinates.html#creating-and-using-multiindex",
    "title": "Basic Coordinates",
    "section": "Creating and Using MultiIndex",
    "text": "Creating and Using MultiIndex\n\n\n\n\n\n\n\nAccessing and Modifying Coordinates",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#coordinate-operations",
    "href": "concepts/02_tensor_coordinates.html#coordinate-operations",
    "title": "Basic Coordinates",
    "section": "Coordinate Operations",
    "text": "Coordinate Operations\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/container/multi_index.hpp\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n\n__device__ void example_multiindex_usage()\n{\n    // Create 3D coordinate with runtime values\n    auto coord = make_multi_index(1, 2, 3);\n\n    // Access dimensions (runtime)\n    auto x = coord[0];  // Returns 1\n    auto y = coord[1];  // Returns 2\n    auto z = coord[2];  // Returns 3\n\n    // For compile-time coordinates, use number&lt;&gt;\n    auto coord_static = make_multi_index(number&lt;1&gt;{}, number&lt;2&gt;{}, number&lt;3&gt;{});\n\n    // Create from tuple\n    auto shape = make_tuple(128, 256, 64);\n    auto coord2 = to_multi_index(shape);\n\n    // Modify coordinate\n    auto new_coord = coord;\n    new_coord.set&lt;0&gt;(number&lt;5&gt;{});  // Set X to 5\n\n    // Use in tensor access\n    auto tensor = make_naive_tensor_view&lt;address_space_enum::global&gt;(\n        data_ptr, shape, strides\n    );\n    // Create tensor coordinate for access\n    auto tensor_coord = make_tensor_coordinate(\n        tensor.get_tensor_descriptor(), coord\n    );\n    float value = tensor.get_vectorized_elements&lt;float&gt;(tensor_coord, 0);\n}",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#multiindex-in-transforms",
    "href": "concepts/02_tensor_coordinates.html#multiindex-in-transforms",
    "title": "Basic Coordinates",
    "section": "MultiIndex in Transforms",
    "text": "MultiIndex in Transforms\nMultiIndex serves as the common currency between different coordinate spaces:\n[1, 2, 3]\"] --&gt; MI[\"MultiIndexStorage\"]\n        MI --&gt; TR[\"TransformProcessing\"]\n        TR --&gt; MO[\"MultiIndexOutput\"]\n        MO --&gt; TA[\"Tensor Accesselement(coord)\"]\n    end\n\n    subgraph \"Example: 3D Tensor Access\"\n        T3D[\"3D Tensorshape=[4,5,6]\"] --&gt; COORD[\"MultiIndex(3, [1,2,3])\"]\n        COORD --&gt; ELEM[\"Element atposition [1,2,3]\"]\n    end\n\n    style UI fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style MI fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style MO fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style COORD fill:#fff3e0,stroke:#f57c00,stroke-width:2px",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#working-with-different-tensor-ranks",
    "href": "concepts/02_tensor_coordinates.html#working-with-different-tensor-ranks",
    "title": "Basic Coordinates",
    "section": "Working with Different Tensor Ranks",
    "text": "Working with Different Tensor Ranks",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#multiindex-patterns",
    "href": "concepts/02_tensor_coordinates.html#multiindex-patterns",
    "title": "Basic Coordinates",
    "section": "MultiIndex Patterns",
    "text": "MultiIndex Patterns\n\nPattern 1: Iteration\n\n\n\n\n\n\n\n\nPattern 2: Boundary Checking\n\n\n\n\n\n\n\n\nPattern 3: Transform Chaining",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/02_tensor_coordinates.html#summary",
    "href": "concepts/02_tensor_coordinates.html#summary",
    "title": "Basic Coordinates",
    "section": "Summary",
    "text": "Summary\nMultiIndex is the foundation of the coordinate system:\n\nSimple abstraction: Just a container for N integers\nUniversal usage: Every transform and adaptor uses MultiIndex\nType-safe in C++: Compile-time size and bounds checking\nFlexible in Python: Dynamic size for experimentation\n\nUnderstanding MultiIndex is crucial before moving to transforms and adaptors, as they all operate on these coordinate containers.",
    "crumbs": [
      "Transformation Engine",
      "Basic Coordinates"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html",
    "href": "concepts/00_introduction_motivation.html",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "",
    "text": "Before diving into any code, let’s establish the fundamental problem tile distribution solves and why it’s essential for GPU programming. Understanding the “why” will make all the subsequent concepts much clearer.\nLearning Objectives:\n\nUnderstand the GPU memory coalescing challenge\nSee how tile distribution enables efficient thread cooperation\nGet intuition for coordinate mapping concepts\nAppreciate the performance benefits of structured data access",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#overview",
    "href": "concepts/00_introduction_motivation.html#overview",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "",
    "text": "Before diving into any code, let’s establish the fundamental problem tile distribution solves and why it’s essential for GPU programming. Understanding the “why” will make all the subsequent concepts much clearer.\nLearning Objectives:\n\nUnderstand the GPU memory coalescing challenge\nSee how tile distribution enables efficient thread cooperation\nGet intuition for coordinate mapping concepts\nAppreciate the performance benefits of structured data access",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#the-gpu-memory-problem",
    "href": "concepts/00_introduction_motivation.html#the-gpu-memory-problem",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "The GPU Memory Problem",
    "text": "The GPU Memory Problem\n\n    graph TB\n    subgraph \"Random Access Pattern (Inefficient)\"\n        subgraph \"Threads\"\n            T0_R[\"Thread 0\"]\n            T1_R[\"Thread 1\"] \n            T2_R[\"Thread 2\"]\n            T3_R[\"Thread 3\"]\n        end\n        \n        subgraph \"Memory\"\n            M0[\"Mem[0]\"]\n            M7[\"Mem[7]\"]\n            M15[\"Mem[15]\"]\n            M23[\"Mem[23]\"]\n            M31[\"Mem[31]\"]\n            M39[\"Mem[39]\"]\n            M47[\"Mem[47]\"]\n            M55[\"Mem[55]\"]\n        end\n        \n        T0_R -.-&gt; M23\n        T1_R -.-&gt; M7\n        T2_R -.-&gt; M47\n        T3_R -.-&gt; M15\n    end\n    \n    subgraph \"Tile Distribution Pattern (Efficient)\"\n        subgraph \"Threads_TD\"\n            T0_TD[\"Thread 0\"]\n            T1_TD[\"Thread 1\"]\n            T2_TD[\"Thread 2\"]\n            T3_TD[\"Thread 3\"]\n        end\n        \n        subgraph \"Memory_TD\"\n            M0_TD[\"Mem[0]\"]\n            M1_TD[\"Mem[1]\"]\n            M2_TD[\"Mem[2]\"]\n            M3_TD[\"Mem[3]\"]\n            M4_TD[\"Mem[4]\"]\n            M5_TD[\"Mem[5]\"]\n            M6_TD[\"Mem[6]\"]\n            M7_TD[\"Mem[7]\"]\n        end\n        \n        T0_TD --&gt; M0_TD\n        T0_TD --&gt; M1_TD\n        T1_TD --&gt; M2_TD\n        T1_TD --&gt; M3_TD\n        T2_TD --&gt; M4_TD\n        T2_TD --&gt; M5_TD\n        T3_TD --&gt; M6_TD\n        T3_TD --&gt; M7_TD\n    end\n    \n    style T0_R fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style T1_R fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style T2_R fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style T3_R fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    \n    style T0_TD fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style T1_TD fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style T2_TD fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style T3_TD fill:#d1fae5,stroke:#10b981,stroke-width:2px\n\n\nWhy Random Memory Access is Slow\nModern GPUs have high computational power, but they are fundamentally limited by memory bandwidth. When thousands of threads try to access memory randomly, several problems occur:\n\nMemory Coalescing: GPU memory controllers work most efficiently when adjacent threads access adjacent memory locations. Random access patterns prevent this optimization.\nCache Efficiency: Random access patterns don’t benefit from cache locality, leading to frequent cache misses.\nThread Divergence: When threads in a warp access memory in unpredictable patterns, the hardware can’t optimize the memory requests.\n\n\n\nThe Thread Cooperation Challenge\nConsider a simple matrix multiplication where 256 threads need to cooperate:\n# Inefficient: Random access pattern\ndef naive_matrix_multiply():\n    thread_id = get_thread_id()\n    # Each thread randomly accesses matrix elements\n    # No coordination between threads\n    # Poor memory coalescing\n    \n    # Get this thread's output position\n    row = thread_id // MATRIX_WIDTH\n    col = thread_id % MATRIX_WIDTH\n    \n    # Each thread computes one element of C = A * B\n    result = 0.0\n    for k in range(MATRIX_WIDTH):\n        # Random access pattern - threads in a warp access non-contiguous memory\n        # Thread 0: A[0,0], A[0,1], A[0,2]...\n        # Thread 1: A[1,0], A[1,1], A[1,2]...\n        # These are far apart in memory!\n        a_element = global_memory_A[row * MATRIX_WIDTH + k]\n        \n        # Even worse for B - accessing column-wise causes strided access\n        # Thread 0: B[0,0], B[1,0], B[2,0]...\n        # Thread 1: B[0,1], B[1,1], B[2,1]...\n        # Massive stride between accesses!\n        b_element = global_memory_B[k * MATRIX_WIDTH + col]\n        \n        result += a_element * b_element\n    \n    # Write result - adjacent threads write to adjacent locations (at least this is good)\n    global_memory_C[row * MATRIX_WIDTH + col] = result\nProblems with this approach:\n\nUnpredictable Memory Access: Threads access memory randomly\nNo Cooperation: Threads don’t coordinate their memory accesses\nPoor Cache Utilization: No locality of reference\nInefficient Bandwidth Usage: Memory controllers can’t optimize",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#the-tile-distribution-solution",
    "href": "concepts/00_introduction_motivation.html#the-tile-distribution-solution",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "The Tile Distribution Solution",
    "text": "The Tile Distribution Solution\n\nStructured Mapping from Logical to Physical Coordinates\nTile distribution solves these problems by providing a structured mapping from logical coordinates (what data does each thread need?) to physical coordinates (where is that data in memory?).\n# Efficient: Tile-based distribution\ndef tile_distributed_matrix_multiply():\n    # 1. Each thread gets a unique tile of data\n    tile_distribution = make_static_tile_distribution(encoding)\n    \n    # 2. Threads cooperate to access memory efficiently\n    tile_window = make_tile_window(tensor_view, window_lengths, origin, tile_distribution)\n    \n    # 3. Memory accesses are coalesced and predictable\n    loaded_tensor = tile_window.load()\n    \n    # 4. Process tile data efficiently\n    def process_element(y_indices):\n        value = loaded_tensor.get_element(y_indices)\n        # ... efficient computation\n    \n    sweep_tile(loaded_tensor, process_element)\n\n\nKey Benefits\n\nPredictable Memory Access Patterns: Threads access memory in structured, predictable ways\nEfficient Thread Cooperation: Threads coordinate their memory accesses for optimal coalescing\nCache-Friendly Access: Spatial and temporal locality improve cache utilization\nScalable Performance: Patterns work across different GPU architectures and problem sizes",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#the-coordinate-mapping-insight",
    "href": "concepts/00_introduction_motivation.html#the-coordinate-mapping-insight",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "The Coordinate Mapping Insight",
    "text": "The Coordinate Mapping Insight\nThe key insight is that tile distribution provides a mathematical framework for mapping between different coordinate spaces:\n\ngraph LR\n    subgraph \"Coordinate Spaces\"\n        P[\"P-spaceThread Position(thread_x, thread_y,warp_id, block_id)\"]\n        Y[\"Y-spaceLocal Data(y0, y1, y2, y3)\"]\n        X[\"X-spaceGlobal Position(x0, x1)\"]\n        D[\"D-spaceMemory Address(linearized)\"]\n    end\n    \n    subgraph \"Transformations\"\n        T1[\"P + Y → XThread data mapping\"]\n        T2[\"X → DMemory linearization\"]\n    end\n    \n    P --&gt; T1\n    Y --&gt; T1\n    T1 --&gt; X\n    X --&gt; T2\n    T2 --&gt; D\n    \n    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style T1 fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style T2 fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n\n\nP-space: Where is each thread? (thread_x, thread_y, warp_id, block_id)\nY-space: What data does each thread need? (y0, y1, y2, y3)\nX-space: Where is that data physically located? (x0, x1)\nD-space: What’s the actual memory address? (linearized coordinates)\n\nThe magic happens in the transformations: P + Y → X → D",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#whats-coming-next",
    "href": "concepts/00_introduction_motivation.html#whats-coming-next",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "What’s Coming Next",
    "text": "What’s Coming Next\nNow that you understand why tile distribution matters, we’ll build up the complete system:\n\nFoundation: Start with raw memory and build up to structured tensors\nTransformation Engine: Learn the coordinate transformation engine\nDistribution API: Master the high-level tile distribution APIs\nCoordinate Systems: Understand the complete coordinate system\nImplementation: Dive into the internal implementation\nThread Mapping: See how it all connects to hardware threads\nAdvanced Topics: Learn advanced optimization techniques",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#interactive-learning-tools",
    "href": "concepts/00_introduction_motivation.html#interactive-learning-tools",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "🎮 Interactive Learning Tools",
    "text": "🎮 Interactive Learning Tools\nEnhance your learning with interactive applications:\n\n📊 Tile Distribution Visualizer - See memory access pattern comparisons, thread cooperation visualization, and performance impact demonstrations\n🔄 Tensor Transform Visualizer - Explore coordinate transformations with visual graphs\n\n🧵 Thread Visualization App - Visualize how threads map to data elements",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#summary",
    "href": "concepts/00_introduction_motivation.html#summary",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "Summary",
    "text": "Summary\nTile distribution isn’t just a technical detail—it’s the foundation that makes GPU computing efficient. By providing structured, predictable mappings between logical and physical coordinates, it enables:\n\nEfficient Memory Access: Coalesced, cache-friendly patterns\nThread Cooperation: Coordinated work distribution\nScalable Performance: Patterns that work across different hardware\nPredictable Optimization: Mathematical framework for performance tuning\n\nReady to see how it all works? Let’s start building from the foundation: From Raw Memory to Structured Tensors.",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/00_introduction_motivation.html#next-steps",
    "href": "concepts/00_introduction_motivation.html#next-steps",
    "title": "Introduction and Motivation - Why Tile Distribution Matters",
    "section": "Next Steps",
    "text": "Next Steps\nContinue to Buffer Views to start building your understanding from the ground up.",
    "crumbs": [
      "Foundation",
      "Introduction and Motivation - Why Tile Distribution Matters"
    ]
  },
  {
    "objectID": "concepts/terminology.html",
    "href": "concepts/terminology.html",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "",
    "text": "This page provides a comprehensive reference for all terminology used in the Composable Kernel (CK) tile distribution system. Understanding these terms is essential for working with CK and reading the documentation effectively."
  },
  {
    "objectID": "concepts/terminology.html#overview",
    "href": "concepts/terminology.html#overview",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "",
    "text": "This page provides a comprehensive reference for all terminology used in the Composable Kernel (CK) tile distribution system. Understanding these terms is essential for working with CK and reading the documentation effectively."
  },
  {
    "objectID": "concepts/terminology.html#core-concepts",
    "href": "concepts/terminology.html#core-concepts",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nTile\nA contiguous block of data that is processed as a unit by a group of threads. Tiles are the fundamental unit of work distribution in CK, designed to maximize memory bandwidth utilization and computational efficiency.\nC++ Usage: using TileShape = sequence&lt;256, 256&gt;;\n\n\nDistribution\nThe pattern by which data is assigned to processing elements (threads, warps, blocks). A distribution defines the mapping from logical coordinates to physical resources.\nC++ Type: tile_distribution&lt;...&gt;\n\n\nEncoding\nA compile-time specification that describes how tensor data should be distributed across GPU processing elements. The encoding captures the complete distribution strategy.\nC++ Type: tile_distribution_encoding&lt;...&gt;"
  },
  {
    "objectID": "concepts/terminology.html#coordinate-spaces",
    "href": "concepts/terminology.html#coordinate-spaces",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Coordinate Spaces",
    "text": "Coordinate Spaces\n\nP-Space (Partition Space)\nThe coordinate space representing processing elements in the GPU hierarchy.\n\nDimensions: Typically 1 or 2 (e.g., [lane_id] or [warp_id, lane_id])\nPurpose: Identifies which thread/warp is executing\nValues: Hardware thread indices\n\nC++ Example:\n// Get current thread's P coordinates\nauto p_idx = Distribution::_get_partition_index();\n\n\nY-Space (Yield Space)\nThe coordinate space representing the logical access pattern within each tile.\n\nDimensions: Variable, typically 2-4\nPurpose: Defines iteration pattern within thread’s assigned work\nValues: Indices within the tile\n\nC++ Example:\n// Iterate over Y-space\nsweep_tile(tensor, [](auto y_idx) { /*...*/ });\n\n\nX-Space (Physical Tensor Space)\nThe coordinate space representing actual positions in the tensor.\n\nDimensions: Matches tensor dimensions (e.g., 2 for matrices)\nPurpose: Maps to global memory addresses\nValues: Physical tensor coordinates\n\nC++ Example:\n// Calculate X coordinates from P+Y\nauto x_idx = distribution.calculate_index(p_idx);\n\n\nR-Space (Replication Space)\nThe coordinate space representing data replication patterns.\n\nDimensions: Variable, often 0-2\nPurpose: Enables data sharing across threads\nValues: Replication indices\n\nC++ Example:\n// R-dimensions in encoding\nusing Encoding = tile_distribution_encoding&lt;\n    sequence&lt;2&gt;,  // rs_lengths: 2-way replication\n    /*...*/\n&gt;;\n\n\nD-Space (Data Space)\nThe linearized coordinate space for thread-local storage.\n\nDimensions: 1 (linear index)\nPurpose: Maps Y-coordinates to register indices\nValues: Linear memory offsets\n\nC++ Example:\n// Y-to-D descriptor linearizes storage\nauto d_idx = ys_to_d_descriptor.calculate_offset(y_idx);"
  },
  {
    "objectID": "concepts/terminology.html#dimension-types",
    "href": "concepts/terminology.html#dimension-types",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Dimension Types",
    "text": "Dimension Types\n\nH-Dimensions (Hierarchical Dimensions)\nThe hierarchical decomposition of tensor dimensions, capturing how work is divided across the GPU hierarchy.\nStructure: Each H-dimension group is a sequence of factors - Example: sequence&lt;4, 2, 8, 4&gt; means: - Repeat: 4 iterations per thread - Warps: 2 warps per block - Threads: 8 threads per warp\n- Vector: 4 elements per operation\nC++ Example:\nusing HsLengthss = tuple&lt;\n    sequence&lt;4, 2, 8, 4&gt;,  // H0: M dimension\n    sequence&lt;4, 2, 8, 4&gt;   // H1: N dimension\n&gt;;\n\n\nRH-Dimensions (R + H Dimensions Combined)\nThe combined space of R and H dimensions, used internally for coordinate transformations.\n\nMajor: Identifies which dimension group (0 for R, 1+ for H)\nMinor: Identifies position within the group"
  },
  {
    "objectID": "concepts/terminology.html#transformations",
    "href": "concepts/terminology.html#transformations",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Transformations",
    "text": "Transformations\n\nAdaptor\nA chain of coordinate transformations that maps between coordinate spaces.\nTypes: - ps_ys_to_xs_adaptor: Maps (P,Y) → X coordinates - ys_to_d_adaptor: Maps Y → D coordinates\nC++ Type: tensor_adaptor&lt;...&gt;\n\n\nDescriptor\nA complete specification of tensor layout including transformations and memory layout.\nC++ Type: tensor_descriptor&lt;...&gt;"
  },
  {
    "objectID": "concepts/terminology.html#operations",
    "href": "concepts/terminology.html#operations",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Operations",
    "text": "Operations\n\nLoad Tile\nOperation that transfers data from global memory to thread-local registers according to the distribution pattern.\nC++ Function: tile_window.load()\n\n\nStore Tile\nOperation that transfers data from thread-local registers back to global memory.\nC++ Function: tile_window.store(tile)\n\n\nSweep Tile\nOperation that iterates over all elements in a distributed tensor, applying a user-defined function.\nC++ Function: sweep_tile(tensor, lambda)\n\n\nShuffle Tile\nOperation that exchanges data between threads within a warp.\nC++ Function: shuffle_tile(tensor, shuffle_pattern)"
  },
  {
    "objectID": "concepts/terminology.html#memory-concepts",
    "href": "concepts/terminology.html#memory-concepts",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Memory Concepts",
    "text": "Memory Concepts\n\nCoalescing\nThe property where adjacent threads access adjacent memory locations, maximizing memory bandwidth utilization.\n\n\nBank Conflict\nA performance degradation that occurs when multiple threads in a warp access different addresses in the same memory bank.\n\n\nVectorization\nThe technique of loading/storing multiple elements in a single memory transaction.\nC++ Example:\n// Vector load of 4 elements\nusing float4 = vector_type&lt;float, 4&gt;::type;\nfloat4 data = tensor_view.template get_vectorized_elements&lt;4&gt;(x_idx);"
  },
  {
    "objectID": "concepts/terminology.html#distribution-components",
    "href": "concepts/terminology.html#distribution-components",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Distribution Components",
    "text": "Distribution Components\n\nWindow\nA view into a subset of a tensor that respects the distribution pattern.\nC++ Type: tile_window&lt;...&gt;\n\n\nStatic Distributed Tensor\nA thread-local tensor stored in registers, distributed according to a tile distribution.\nC++ Type: static_distributed_tensor&lt;...&gt;\n\n\nSpans\nIteration ranges over distributed dimensions, used by sweep operations.\nC++ Type: tile_distributed_span&lt;...&gt;"
  },
  {
    "objectID": "concepts/terminology.html#gpu-hardware-terms",
    "href": "concepts/terminology.html#gpu-hardware-terms",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "GPU Hardware Terms",
    "text": "GPU Hardware Terms\n\nWarp\nA group of threads (32 on AMD GPUs) that execute in lockstep.\n\n\nLane\nAn individual thread within a warp (0-31).\n\n\nBlock\nA group of warps that can cooperate through shared memory.\n\n\nGrid\nThe complete set of blocks launched for a kernel."
  },
  {
    "objectID": "concepts/terminology.html#template-parameters",
    "href": "concepts/terminology.html#template-parameters",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Template Parameters",
    "text": "Template Parameters\n\nsequence&lt;…&gt;\nA compile-time integer sequence used to specify dimensions and lengths.\nExample: sequence&lt;256, 256&gt; for a 256×256 tile\n\n\ntuple&lt;…&gt;\nA heterogeneous collection of types, often used for grouping sequences.\nExample: tuple&lt;sequence&lt;4,4&gt;, sequence&lt;4,4&gt;&gt;\n\n\nnumber\nA compile-time integer constant.\nExample: number&lt;16&gt; represents the value 16"
  },
  {
    "objectID": "concepts/terminology.html#optimization-terms",
    "href": "concepts/terminology.html#optimization-terms",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Optimization Terms",
    "text": "Optimization Terms\n\nRegister Spilling\nWhen a kernel uses more registers than available, causing data to spill to slower memory.\n\n\nOccupancy\nThe ratio of active warps to maximum possible warps on a GPU multiprocessor.\n\n\nMemory Bandwidth Utilization\nThe percentage of theoretical memory bandwidth achieved by a kernel.\n\n\nInstruction-Level Parallelism (ILP)\nThe ability to execute multiple independent instructions simultaneously."
  },
  {
    "objectID": "concepts/terminology.html#common-patterns",
    "href": "concepts/terminology.html#common-patterns",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Common Patterns",
    "text": "Common Patterns\n\nGEMM (General Matrix Multiplication)\nA fundamental operation where C = αA×B + βC.\n\n\nReduction\nAn operation that combines multiple values into a single result (e.g., sum, max).\n\n\nBroadcast\nAn operation that replicates a value across multiple processing elements.\n\n\nTranspose\nAn operation that swaps dimensions of a tensor."
  },
  {
    "objectID": "concepts/terminology.html#performance-metrics",
    "href": "concepts/terminology.html#performance-metrics",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Performance Metrics",
    "text": "Performance Metrics\n\nFLOPS (Floating-Point Operations Per Second)\nMeasure of computational throughput.\n\n\nBandwidth\nRate of data transfer, typically measured in GB/s.\n\n\nLatency\nTime delay between issuing an operation and its completion.\n\n\nThroughput\nRate of operation completion, often measured in operations per second."
  },
  {
    "objectID": "concepts/terminology.html#usage-examples",
    "href": "concepts/terminology.html#usage-examples",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Usage Examples",
    "text": "Usage Examples\n\nCreating a Distribution\n// Define encoding\nusing MyEncoding = tile_distribution_encoding&lt;\n    sequence&lt;&gt;,                        // No replication\n    tuple&lt;sequence&lt;4,2,8,4&gt;,          // M dimension\n          sequence&lt;4,2,8,4&gt;&gt;,         // N dimension\n    tuple&lt;sequence&lt;1,2&gt;, sequence&lt;1,2&gt;&gt;, // P mappings\n    tuple&lt;sequence&lt;1,1&gt;, sequence&lt;2,2&gt;&gt;, // P minor\n    sequence&lt;1,1,2,2&gt;,                   // Y major\n    sequence&lt;0,3,0,3&gt;                    // Y minor\n&gt;;\n\n// Create distribution\nauto distribution = make_static_tile_distribution(MyEncoding{});\n\n\nUsing Tile Window\n// Create window\nauto window = make_tile_window(\n    tensor_view,\n    TileShape{},\n    origin,\n    distribution\n);\n\n// Load-compute-store pattern\nauto tile = window.load();\nsweep_tile(tile, compute_func);\nwindow.store(tile);"
  },
  {
    "objectID": "concepts/terminology.html#related-documentation",
    "href": "concepts/terminology.html#related-documentation",
    "title": "Terminology Reference - Key Concepts and Definitions",
    "section": "Related Documentation",
    "text": "Related Documentation\n\nCoordinate Systems - Detailed explanation of coordinate spaces\nTile Distribution - Core distribution concepts\nEncoding Internals - How encodings work internally\nThread Mapping - Hardware thread organization"
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#overview",
    "href": "concepts/04_coordinate_systems.html#overview",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Overview",
    "text": "Overview\nNow that you understand the APIs and basic transformations, it’s time to learn the mathematical foundation that makes it all work: the coordinate system.\nTile distribution uses five interconnected coordinate spaces to map from thread identification all the way to memory addresses. Understanding these coordinate spaces is the key to mastering tile distribution.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#the-five-coordinate-spaces",
    "href": "concepts/04_coordinate_systems.html#the-five-coordinate-spaces",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "The Five Coordinate Spaces",
    "text": "The Five Coordinate Spaces\nBefore diving into code, let’s understand what problem these coordinate spaces solve:\nThe Challenge: You have an 8×8 matrix and 4 GPU threads. Each thread needs to know:\n\nWhich thread am I? (Thread identification)\n\nWhat work should I do? (Work assignment)\nWhere is my data in the tensor? (Physical location)\nHow do I share data with other threads? (Cooperation)\nWhat’s the memory address? (Hardware access)\n\nThe Solution: Five coordinate spaces that transform from logical to physical:",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#the-five-coordinate-spaces-1",
    "href": "concepts/04_coordinate_systems.html#the-five-coordinate-spaces-1",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "The Five Coordinate Spaces",
    "text": "The Five Coordinate Spaces\n\ngraph TB\n    subgraph \"Coordinate Spaces Overview\"\n        P[\"P-spaceThread IdentificationWhich thread am I?\"]\n        Y[\"Y-spaceLogical TileWhich element in my tile?\"]\n        X[\"X-spacePhysical TensorWhere in the tensor?\"]\n        R[\"R-spaceReplicationData sharing pattern\"]\n        D[\"D-spaceLinear StorageMemory address\"]\n    end\n    \n    subgraph \"Transformations\"\n        T1[\"P + Y → XThread + Element → Position\"]\n        T2[\"X → DPosition → Address\"]\n    end\n    \n    P --&gt; T1\n    Y --&gt; T1\n    T1 --&gt; X\n    X --&gt; T2\n    T2 --&gt; D\n    \n    R -.-&gt; P\n    R -.-&gt; Y\n    \n    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n    style R fill:#fce4ec,stroke:#c2185b,stroke-width:2px\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n\n\nP-space (Partition)\nThread identification - Coordinates: thread_x, thread_y, warp_id, block_id - Purpose: Identifies which thread is doing the work - Source: GPU hardware intrinsics\n\n\nY-space (Logical Tile)\nElement within thread’s work - Coordinates: y0, y1, y2, y3 (logical coordinates) - Purpose: Specifies which element within the thread’s tile - Usage: Iteration over assigned data\n\n\nX-space (Physical Tensor)\nActual tensor coordinates - Coordinates: x0, x1 (physical matrix coordinates) - Purpose: Maps to actual position in the tensor - Result: Global memory addresses\n\n\nR-space (Replication)\nData sharing across threads - Coordinates: r0, r1 (replication coordinates) - Purpose: Enables shared data across multiple threads - Usage: Reduction operations, broadcasting\n\n\nD-space (Linearized Storage)\nMemory layout - Coordinates: d (single linear index) - Purpose: Maps to actual memory address - Usage: Register allocation, memory access ## The Five Coordinate Spaces\n\n\n\n\n\n\n\n\n\n\n\n\n\nP-space (Partition)\nThread identification - Coordinates: thread_x, thread_y, warp_id, block_id - Purpose: Identifies which thread is doing the work - Source: GPU hardware intrinsics\n\n\nY-space (Logical Tile)\nElement within thread’s work - Coordinates: y0, y1, y2, y3 (logical coordinates) - Purpose: Specifies which element within the thread’s tile - Usage: Iteration over assigned data\n\n\nX-space (Physical Tensor)\nActual tensor coordinates - Coordinates: x0, x1 (physical matrix coordinates) - Purpose: Maps to actual position in the tensor - Result: Global memory addresses\n\n\nR-space (Replication)\nData sharing across threads - Coordinates: r0, r1 (replication coordinates) - Purpose: Enables shared data across multiple threads - Usage: Reduction operations, broadcasting\n\n\nD-space (Linearized Storage)\nMemory layout - Coordinates: d (single linear index) - Purpose: Maps to actual memory address - Usage: Register allocation, memory access",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#p-space-partition-coordinates",
    "href": "concepts/04_coordinate_systems.html#p-space-partition-coordinates",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "P-space: Partition Coordinates",
    "text": "P-space: Partition Coordinates\nP-space identifies which thread is doing the work. Each thread gets a unique P coordinate.\n\nP-space Architecture\n\ngraph TB\n    subgraph \"GPU Thread Hierarchy\"\n        subgraph \"Block\"\n            subgraph \"Warp 0\"\n                T0[\"Thread 0P=[0,0]\"]\n                T1[\"Thread 1P=[0,1]\"]\n                T2[\"Thread 2P=[0,2]\"]\n                T31[\"...\"]\n                T3[\"Thread 31P=[0,31]\"]\n            end\n            subgraph \"Warp 1\"\n                T32[\"Thread 32P=[1,0]\"]\n                T33[\"Thread 33P=[1,1]\"]\n                T34[\"...\"]\n                T63[\"Thread 63P=[1,31]\"]\n            end\n            W2[\"Warp 2...\"]\n            W7[\"Warp 7\"]\n        end\n    end\n    \n    subgraph \"P-space Mapping\"\n        PM[\"P-coordinates = [warp_id, lane_id]orP-coordinates = [block_x, block_y, thread_x, thread_y]\"]\n    end\n    \n    T0 --&gt; PM\n    T32 --&gt; PM\n    \n    style T0 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style T32 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n\n\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/container/multi_index.hpp\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n#include &lt;ck_tile/core/numeric/tuple.hpp&gt;\n#include &lt;ck_tile/core/utility/thread_id.hpp&gt;\n\n// P-space coordinate calculation in production kernels\ntemplate &lt;typename TileDistribution&gt;\n__device__ void example_p_space_calculation()\n{\n    // Method 1: Get P-coordinates from hardware thread IDs\n    // This is how real CK kernels determine thread position\n    const index_t thread_id = get_thread_local_1d_id();  // Hardware thread ID\n    const index_t warp_id = get_warp_local_1d_id();      // Warp within block\n    const index_t lane_id = get_lane_id();               // Thread within warp\n    \n    // Convert to multi-dimensional P-coordinates\n    // For 2D P-space: [warp_id, lane_id]\n    auto p_coord_2d = make_multi_index(warp_id, lane_id);\n    \n    // For 3D P-space: [block_x, warp_id, lane_id]\n    const index_t block_x = blockIdx.x;\n    auto p_coord_3d = make_multi_index(block_x, warp_id, lane_id);\n    \n    // Method 2: Using tile distribution to get structured P-coordinates\n    // This is the preferred method in CK\n    constexpr auto tile_distribution = TileDistribution{};\n    \n    // Get P-coordinates directly from tile distribution\n    const auto p_coord = tile_distribution.calculate_p_coord();\n    // p_coord is a multi_index with compile-time known size\n    \n    // Access individual P dimensions\n    constexpr index_t p_dim0 = p_coord.at(number&lt;0&gt;{});  // First P dimension\n    constexpr index_t p_dim1 = p_coord.at(number&lt;1&gt;{});  // Second P dimension\n    \n    // P-coordinates are used for:\n    // 1. Work distribution - which data this thread processes\n    // 2. Memory coalescing - ensuring optimal access patterns\n    // 3. Thread cooperation - coordinating shared memory usage\n    \n    // Example: Matrix multiplication P-space\n    // P = [warp_m, warp_n, thread_m, thread_n]\n    // Each dimension controls different aspects of parallelization\n    \n    static_assert(tile_distribution.ndim_p == 2);  // 2D P-space\n}\n\n// Compile-time P-space structure definition\ntemplate &lt;index_t... PSizes&gt;\nstruct p_space_definition\n{\n    static constexpr auto p_lengths = make_tuple(number&lt;PSizes&gt;{}...);\n    static constexpr index_t ndim_p = sizeof...(PSizes);\n    \n    // Total number of threads\n    static constexpr index_t total_threads = (PSizes * ...);\n    \n    // Convert linear thread ID to P-coordinates\n    __device__ static constexpr auto thread_id_to_p_coord(index_t tid)\n    {\n        // Compile-time unrolling of coordinate calculation\n        return unravel_index&lt;PSizes...&gt;(tid);\n    }\n};\n\n// Example usage\nusing matmul_p_space = p_space_definition&lt;4, 2, 8&gt;;  // 4 warps, 2 groups, 8 threads\nstatic_assert(matmul_p_space::total_threads == 64);\nstatic_assert(matmul_p_space::ndim_p == 3);\n\n\nPython vs C++ P-Space Differences\n\n1. Thread ID Source\n\nC++: Hardware intrinsics provide real thread IDs\n// Actual GPU hardware values\nconst index_t thread_x = threadIdx.x;     // 0-31 within warp\nconst index_t warp_id = threadIdx.x / 32; // Warp number\nconst index_t block_id = blockIdx.x;      // Block number\n\nDirect access to GPU hardware thread hierarchy\nZero-overhead coordinate calculation\nCompile-time thread organization validation\n\nPython: Simulated thread grid\n# Simulation for learning\nfor thread_x in range(thread_grid[0]):\n    for thread_y in range(thread_grid[1]):\n        p_coord = [thread_x, thread_y]\n\nEducational simulation of thread behavior\nHelps understand thread-to-work mapping\nRuntime coordinate generation\n\n\n\n\n2. Coordinate Structure\n\nC++: Template-encoded dimensions\n// P-space structure encoded in types\nusing p_space = multi_index&lt;4, 2, 8&gt;;  // Compile-time sizes\nstatic_assert(p_space::size() == 3);   // 3D P-space\n\n// Coordinates computed at compile time when possible\nconstexpr auto p_coord = make_multi_index(\n    number&lt;2&gt;{}, number&lt;1&gt;{}, number&lt;5&gt;{}\n);\nPython: Runtime lists\n# Runtime coordinate representation\np_coord = [thread_x, thread_y]  # Dynamic size and values\n\n\n\n3. Performance Characteristics\n\nC++: Zero-overhead abstractions\n// This entire calculation often compiles to single instruction\nconst auto p_coord = tile_distribution.calculate_p_coord();\nconst auto work_offset = p_coord.at(number&lt;0&gt;{}) * work_per_thread;\n\nCompile-time coordinate resolution\nHardware thread IDs mapped directly to memory addresses\nNo runtime overhead for coordinate calculations\n\nPython: Educational clarity\n# Focus on understanding concepts, not performance\nthread_id = thread_x * thread_grid[1] + thread_y\n\nRuntime calculations help visualize the mapping\nClear step-by-step coordinate generation\nExcellent for learning thread organization patterns\n\n\nKey Insight: P-space gives each thread a unique identity. In real GPU kernels, these come from hardware intrinsics like threadIdx.x, blockIdx.y, etc.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#y-space-logical-tile-coordinates",
    "href": "concepts/04_coordinate_systems.html#y-space-logical-tile-coordinates",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Y-space: Logical Tile Coordinates",
    "text": "Y-space: Logical Tile Coordinates\nY-space defines what work each thread does. Each thread processes a “tile” of elements.\n\nY-space Work Assignment\n\ngraph TB\n    subgraph \"Thread's Tile (2x2 elements)\"\n        Y00[\"Y=[0,0]Element 0\"]\n        Y01[\"Y=[0,1]Element 1\"]\n        Y10[\"Y=[1,0]Element 2\"]\n        Y11[\"Y=[1,1]Element 3\"]\n    end\n    \n    subgraph \"Y-space Structure\"\n        YS[\"Each thread processesthe same Y-space patternbut at different X locations\"]\n    end\n    \n    subgraph \"Example: 4 Threads\"\n        T0[\"Thread 0P=[0,0]\"]\n        T1[\"Thread 1P=[0,1]\"]\n        T2[\"Thread 2P=[1,0]\"]\n        T3[\"Thread 3P=[1,1]\"]\n    end\n    \n    Y00 --&gt; YS\n    Y01 --&gt; YS\n    Y10 --&gt; YS\n    Y11 --&gt; YS\n    \n    T0 --&gt; YS\n    T1 --&gt; YS\n    T2 --&gt; YS\n    T3 --&gt; YS\n    \n    style Y00 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style Y01 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style Y10 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style Y11 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/container/multi_index.hpp\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n#include &lt;ck_tile/core/numeric/tuple.hpp&gt;\n#include &lt;ck_tile/ops/fused_multihead_attention/kernel/impl/fa_fwd_kernel_impl.hpp&gt;\n\n// Y-space coordinate generation in production kernels\ntemplate &lt;typename TileDistribution&gt;\n__device__ void example_y_space_iteration()\n{\n    constexpr auto tile_distribution = TileDistribution{};\n    \n    // Get Y-space dimensions at compile time\n    constexpr auto y_lengths = tile_distribution.get_y_lengths();\n    constexpr index_t y_dim = y_lengths.size();\n    \n    // Method 1: Manual Y-space iteration (educational)\n    // This shows what sweep_tile() does internally\n    constexpr index_t y0_max = y_lengths.at(number&lt;0&gt;{});\n    constexpr index_t y1_max = y_lengths.at(number&lt;1&gt;{});\n    \n    // Completely unrolled at compile time\n    static_for&lt;0, y0_max, 1&gt;{}([&](auto y0) {\n        static_for&lt;0, y1_max, 1&gt;{}([&](auto y1) {\n            // Y-coordinates as compile-time constants\n            constexpr auto y_coord = make_multi_index(y0, y1);\n            \n            // This is one element in the thread's tile\n            // All Y-space iteration happens at compile time\n            // No runtime loops, no branch overhead\n            \n            // Convert Y to actual tensor position using P+Y→X transform\n            const auto x_coord = tile_distribution.calculate_index(\n                p_coord, y_coord  // Combine thread position with element offset\n            );\n            \n            // Process this element\n            // tensor_view(x_coord) = computation(...);\n        });\n    });\n    \n    // Method 2: Using sweep_tile (recommended)\n    // This is the idiomatic CK way to iterate Y-space\n    auto distributed_tensor = make_distributed_tensor(\n        tensor_view, tile_distribution\n    );\n    \n    sweep_tile(distributed_tensor, [&](auto y_coord) {\n        // y_coord is compile-time multi_index\n        // This lambda is called once for each Y-space position\n        // All calls are inlined and unrolled at compile time\n        \n        auto value = distributed_tensor(y_coord);\n        // Process value...\n        distributed_tensor(y_coord) = new_value;\n    });\n    \n    // Method 3: Hierarchical Y-space (advanced)\n    // For complex kernels like FlashAttention\n    constexpr auto y_hierarchical = make_tuple(\n        number&lt;4&gt;{},   // Repeat dimension\n        number&lt;2&gt;{},   // Warp dimension  \n        number&lt;8&gt;{},   // Thread dimension\n        number&lt;4&gt;{}    // Vector dimension\n    );\n    \n    // 4D Y-space: [repeat, warp, thread, vector]\n    // Each dimension serves different purpose:\n    // - Repeat: Algorithm repetition (attention heads)\n    // - Warp: Inter-warp cooperation\n    // - Thread: Per-thread work items\n    // - Vector: SIMD vectorization\n}\n\n// Compile-time Y-space structure\ntemplate &lt;index_t... YSizes&gt;\nstruct y_space_definition\n{\n    static constexpr auto y_lengths = make_tuple(number&lt;YSizes&gt;{}...);\n    static constexpr index_t ndim_y = sizeof...(YSizes);\n    \n    // Total elements per thread\n    static constexpr index_t elements_per_thread = (YSizes * ...);\n    \n    // Y-space iterator\n    template &lt;typename Func&gt;\n    __device__ static constexpr void for_each_y(Func&& func)\n    {\n        // Completely unrolled nested loops\n        unroll_y_space&lt;YSizes...&gt;(func);\n    }\n};\n\n// Example: 2x2 tile per thread\nusing simple_tile_y = y_space_definition&lt;2, 2&gt;;\nstatic_assert(simple_tile_y::elements_per_thread == 4);\nstatic_assert(simple_tile_y::ndim_y == 2);\n\n// Example: Complex 4D hierarchical structure\nusing attention_y = y_space_definition&lt;4, 2, 8, 4&gt;;\nstatic_assert(attention_y::elements_per_thread == 256);\nstatic_assert(attention_y::ndim_y == 4);\n\n\nPython vs C++ Y-Space Differences\n\n1. Iteration Pattern\n\nC++: Compile-time loop unrolling\n// No runtime loops - everything unrolled at compile time\nstatic_for&lt;0, 2, 1&gt;{}([&](auto y0) {\n    static_for&lt;0, 2, 1&gt;{}([&](auto y1) {\n        // This becomes 4 separate code blocks, no branching\n        constexpr auto y_coord = make_multi_index(y0, y1);\n        process_element(y_coord);\n    });\n});\n\nZero runtime overhead for Y-space iteration\nAll loops unrolled into straight-line code\nPerfect instruction-level parallelism\n\nPython: Runtime loops for understanding\n# Runtime loops help visualize the iteration pattern\nfor y0 in range(2):\n    for y1 in range(2):\n        y_coord = [y0, y1]\n        # Shows the logical structure clearly\n\nClear visualization of iteration patterns\nEasy to understand nested loop structure\nGood for learning Y-space organization\n\n\n\n\n2. Coordinate Representation\n\nC++: Compile-time multi_index\n// Y-coordinates known at compile time\nconstexpr auto y_coord = make_multi_index(number&lt;0&gt;{}, number&lt;1&gt;{});\nstatic_assert(y_coord.size() == 2);\n\n// Can be used in template parameters\nauto element = tensor_view.template get&lt;y_coord.at(number&lt;0&gt;{}), \n                                      y_coord.at(number&lt;1&gt;{})&gt;();\nPython: Runtime lists\n# Runtime coordinate representation\ny_coord = [y0, y1]  # Flexible but with runtime overhead\n\n\n\n3. Memory Access Pattern\n\nC++: Optimal register usage\n// Each Y-space element typically maps to a register\nsweep_tile(distributed_tensor, [&](auto y_coord) {\n    // Compiler assigns each element to optimal register\n    // No memory loads/stores for temporary values\n    auto reg_value = distributed_tensor(y_coord);\n});\n\nY-space elements stored in registers\nZero memory bandwidth for intermediate values\nPerfect cache locality through compile-time optimization\n\nPython: Educational memory model\n# Simulates the logical organization\n# Shows how threads organize their work\nelement_id = y0 * tile_size[1] + y1\n\nHelps understand data organization principles\nShows relationship between coordinates and memory layout\nGood for visualizing thread work distribution\n\n\nKey Insight: Y-space defines the structure of work within each thread. Every thread has the same Y-space structure, but processes different data.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#x-space-physical-tensor-coordinates",
    "href": "concepts/04_coordinate_systems.html#x-space-physical-tensor-coordinates",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "X-space: Physical Tensor Coordinates",
    "text": "X-space: Physical Tensor Coordinates\nX-space gives the actual position in the tensor. This is where the data lives.\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/tensor/tensor_descriptor.hpp\n#include &lt;ck_tile/core/tensor/tensor_descriptor.hpp&gt;\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n#include &lt;ck_tile/core/numeric/tuple.hpp&gt;\n\n// X-space coordinate handling in production kernels\ntemplate &lt;typename TensorDescriptor&gt;\n__device__ void example_x_space_operations()\n{\n    constexpr auto tensor_desc = TensorDescriptor{};\n    \n    // X-space properties known at compile time\n    constexpr auto x_lengths = tensor_desc.get_lengths();  // Tensor dimensions\n    constexpr auto x_strides = tensor_desc.get_strides();  // Memory layout\n    constexpr index_t ndim_x = x_lengths.size();\n    \n    // Method 1: Direct X-coordinate specification\n    // This is how users think about tensor access\n    constexpr auto x_coord = make_multi_index(number&lt;3&gt;{}, number&lt;4&gt;{});\n    \n    // Convert X-coordinates to linear memory offset\n    constexpr auto linear_offset = tensor_desc.calculate_offset(x_coord);\n    // offset = x_coord[0] * x_strides[0] + x_coord[1] * x_strides[1]\n    //        = 3 * stride[0] + 4 * stride[1]\n    \n    // Method 2: X-coordinates from P+Y transformation\n    // This is how tile distribution generates X-coordinates\n    template &lt;typename TileDistribution&gt;\n    auto calculate_x_from_py(const auto& p_coord, const auto& y_coord)\n    {\n        constexpr auto tile_dist = TileDistribution{};\n        \n        // The core transformation: P + Y → X\n        // This maps thread position + element offset to tensor position\n        const auto x_coord = tile_dist.calculate_index(p_coord, y_coord);\n        \n        return x_coord;\n    }\n    \n    // Method 3: Bounds checking for X-coordinates\n    // Essential for correct memory access\n    template &lt;typename XCoord&gt;\n    __device__ constexpr bool is_valid_x_coord(const XCoord& x_coord)\n    {\n        // Check each dimension against tensor bounds\n        bool valid = true;\n        static_for&lt;0, ndim_x, 1&gt;{}([&](auto dim) {\n            if constexpr (is_known_at_compile_time&lt;decltype(x_coord.at(dim))&gt;::value) {\n                // Compile-time bounds checking\n                static_assert(x_coord.at(dim) &lt; x_lengths.at(dim));\n            } else {\n                // Runtime bounds checking\n                valid = valid && (x_coord.at(dim) &lt; x_lengths.at(dim));\n                valid = valid && (x_coord.at(dim) &gt;= 0);\n            }\n        });\n        return valid;\n    }\n    \n    // Method 4: X-coordinate arithmetic\n    // Used for sliding window operations, padding, etc.\n    __device__ constexpr auto add_x_coordinates(const auto& x1, const auto& x2)\n    {\n        auto result = x1;\n        static_for&lt;0, ndim_x, 1&gt;{}([&](auto dim) {\n            result.at(dim) = x1.at(dim) + x2.at(dim);\n        });\n        return result;\n    }\n    \n    // Method 5: X-coordinate to tensor element access\n    template &lt;typename TensorView&gt;\n    __device__ auto access_tensor_element(TensorView& tensor, const auto& x_coord)\n    {\n        // Direct access using X-coordinates\n        return tensor(x_coord);\n        \n        // Alternative: Manual offset calculation\n        const auto offset = tensor.get_tensor_descriptor().calculate_offset(x_coord);\n        return tensor.get_buffer_view().template get&lt;1&gt;(0, offset, true);\n    }\n}\n\n// X-space descriptor types\ntemplate &lt;typename... XLengths&gt;\nstruct x_space_descriptor\n{\n    static constexpr auto x_lengths = make_tuple(XLengths{}...);\n    static constexpr index_t ndim_x = sizeof...(XLengths);\n    \n    // Row-major strides (C-style layout)\n    static constexpr auto x_strides = calculate_row_major_strides(x_lengths);\n    \n    // Total elements in X-space\n    static constexpr index_t total_elements = (XLengths::value * ...);\n    \n    // Convert X-coordinates to linear offset\n    template &lt;typename XCoord&gt;\n    __device__ static constexpr auto calculate_offset(const XCoord& x_coord)\n    {\n        index_t offset = 0;\n        static_for&lt;0, ndim_x, 1&gt;{}([&](auto dim) {\n            offset += x_coord.at(dim) * x_strides.at(dim);\n        });\n        return offset;\n    }\n};\n\n// Example: 2D matrix X-space\nusing matrix_x_space = x_space_descriptor&lt;number&lt;1024&gt;, number&lt;768&gt;&gt;;\nstatic_assert(matrix_x_space::ndim_x == 2);\nstatic_assert(matrix_x_space::total_elements == 1024 * 768);\n\n// Example: 4D tensor X-space (batch, channels, height, width)\nusing conv_x_space = x_space_descriptor&lt;number&lt;32&gt;, number&lt;256&gt;, number&lt;224&gt;, number&lt;224&gt;&gt;;\nstatic_assert(conv_x_space::ndim_x == 4);\n\n\nPython vs C++ X-Space Differences\n\n1. Coordinate Resolution\n\nC++: Compile-time and runtime coordinate handling\n// Compile-time coordinates (fastest)\nconstexpr auto x_coord = make_multi_index(number&lt;3&gt;{}, number&lt;4&gt;{});\nconstexpr auto offset = tensor_desc.calculate_offset(x_coord);\n// Entire calculation happens at compile time\n\n// Runtime coordinates (flexible)\nconst auto x_dynamic = make_multi_index(row, col);\nconst auto offset_dynamic = tensor_desc.calculate_offset(x_dynamic);\n\nCompile-time coordinates enable aggressive optimization\nRuntime coordinates provide flexibility for dynamic access\nHybrid approaches possible for partially-known coordinates\n\nPython: Always runtime\n# All coordinate calculations happen at runtime\nx_coord = [3, 4]\nlinear_idx = x_coord[0] * tensor_size[1] + x_coord[1]\n\nConsistent runtime model easier to understand\nGood for learning coordinate-to-memory mapping\nNo compile-time optimization benefits\n\n\n\n\n2. Memory Layout Control\n\nC++: Multiple layout options with compile-time specification\n// Row-major layout (C-style)\nauto row_major_desc = make_naive_tensor_descriptor_packed(\n    make_tuple(number&lt;M&gt;{}, number&lt;N&gt;{})\n);\n// Strides: [N, 1]\n\n// Column-major layout (Fortran-style)\nauto col_major_desc = make_tensor_descriptor(\n    make_tuple(number&lt;M&gt;{}, number&lt;N&gt;{}),\n    make_tuple(number&lt;1&gt;{}, number&lt;M&gt;{}),  // Custom strides\n    make_tuple(number&lt;0&gt;{}, number&lt;0&gt;{})\n);\n// Strides: [1, M]\n\n// Complex blocked layout\nauto blocked_desc = make_tensor_descriptor(\n    make_tuple(number&lt;M&gt;{}, number&lt;N&gt;{}),\n    make_tuple(number&lt;block_size * N&gt;{}, number&lt;1&gt;{}),  // Blocked strides\n    make_tuple(number&lt;0&gt;{}, number&lt;0&gt;{})\n);\nPython: Simplified row-major model\n# Only basic row-major layout\nlinear_idx = x_coord[0] * tensor_size[1] + x_coord[1]\n\n\n\n3. Bounds Checking and Safety\n\nC++: Configurable safety levels\n// Debug mode: Compile-time and runtime checks\n#ifdef DEBUG\n    static_assert(x_coord.at(number&lt;0&gt;{}) &lt; tensor_lengths.at(number&lt;0&gt;{}));\n    assert(x_coord.at(number&lt;1&gt;{}) &lt; tensor_lengths.at(number&lt;1&gt;{}));\n#endif\n\n// Release mode: Bounds checking disabled for performance\n// Undefined behavior for out-of-bounds access\n\nCompile-time bounds checking when coordinates are known\nRuntime bounds checking configurable\nZero overhead in optimized builds\n\nPython: Always safe\n# Bounds checking helps learning\nif x_coord[0] &gt;= tensor_size[0] or x_coord[1] &gt;= tensor_size[1]:\n    print(\"Out of bounds access!\")\n\nAlways safe for learning and experimentation\nClear error messages for invalid coordinates\nFocus on correctness over performance\n\n\nKey Insight: X-space represents the actual tensor coordinates that users think about. This is the “physical reality” of where data lives.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#the-core-transformation-p-y-x",
    "href": "concepts/04_coordinate_systems.html#the-core-transformation-p-y-x",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "The Core Transformation: P + Y → X",
    "text": "The Core Transformation: P + Y → X\nThis is the heart of tile distribution: combining thread identity (P) with logical work coordinates (Y) to get physical tensor coordinates (X).\n\nP+Y→X Transformation Visualization\n\ngraph LR\n    subgraph \"Input\"\n        P[\"P-coordinatesThread identityP=[1,0]\"]\n        Y[\"Y-coordinatesElement in tileY=[0,1]\"]\n    end\n    \n    subgraph \"Transformation\"\n        T[\"P + Y → XBase position + Offset\"]\n    end\n    \n    subgraph \"Output\"\n        X[\"X-coordinatesTensor positionX=[2,1]\"]\n    end\n    \n    subgraph \"Example\"\n        E[\"Thread P=[1,0] at base (2,0)Element Y=[0,1] adds offset (0,1)Result X=[2,1] in tensor\"]\n    end\n    \n    P --&gt; T\n    Y --&gt; T\n    T --&gt; X\n    X --&gt; E\n    \n    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:2px\n\n\n\n\n\n\n\nKey Insight: The P+Y→X transformation is what makes tile distribution work. It automatically maps logical thread work to physical tensor locations.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#r-space-replication-coordinates",
    "href": "concepts/04_coordinate_systems.html#r-space-replication-coordinates",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "R-space: Replication Coordinates",
    "text": "R-space: Replication Coordinates\nR-space handles data that needs to be shared across threads, useful for broadcast operations and reductions.\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/tile/tile_distribution.hpp\n#include &lt;ck_tile/core/tile/tile_distribution.hpp&gt;\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n#include &lt;ck_tile/core/algorithm/reduction.hpp&gt;\n\n// R-space coordinate handling for thread cooperation\ntemplate &lt;typename TileDistribution&gt;\n__device__ void example_r_space_operations()\n{\n    constexpr auto tile_distribution = TileDistribution{};\n    \n    // R-space dimensions for replication patterns\n    constexpr auto r_lengths = tile_distribution.get_r_lengths();\n    constexpr index_t ndim_r = r_lengths.size();\n    \n    // Method 1: Broadcasting with R-space\n    // One thread computes, others replicate the result\n    template &lt;typename DataType&gt;\n    __device__ auto broadcast_across_r_space(DataType value)\n    {\n        // Get this thread's R-coordinates\n        const auto r_coord = tile_distribution.calculate_r_coord();\n        \n        // Check if this is the \"source\" thread for broadcasting\n        constexpr auto r_origin = make_multi_index(number&lt;0&gt;{}, number&lt;0&gt;{});\n        const bool is_source = (r_coord == r_origin);\n        \n        // Use shared memory for broadcasting\n        __shared__ DataType shared_value;\n        \n        if (is_source) {\n            shared_value = value;  // Source thread stores the value\n        }\n        __syncthreads();\n        \n        // All threads in R-space read the replicated value\n        return shared_value;\n    }\n    \n    // Method 2: Reduction across R-space\n    // Multiple threads contribute, one thread collects result\n    template &lt;typename DataType&gt;\n    __device__ auto reduce_across_r_space(DataType local_value)\n    {\n        // Get R-space coordinates for this thread\n        const auto r_coord = tile_distribution.calculate_r_coord();\n        \n        // Use CK's built-in reduction primitives\n        __shared__ DataType reduction_buffer[32];  // Size based on R-space\n        \n        // Each R-coordinate contributes to reduction\n        const index_t r_linear = r_coord.at(number&lt;0&gt;{}) * r_lengths.at(number&lt;1&gt;{})\n                               + r_coord.at(number&lt;1&gt;{});\n        \n        reduction_buffer[r_linear] = local_value;\n        __syncthreads();\n        \n        // Reduction tree across R-space\n        block_reduce_sum(reduction_buffer, r_lengths.size());\n        \n        // First thread in R-space gets the result\n        constexpr auto r_origin = make_multi_index(number&lt;0&gt;{}, number&lt;0&gt;{});\n        if (r_coord == r_origin) {\n            return reduction_buffer[0];  // Reduced result\n        } else {\n            return DataType{0};  // Other threads get zero\n        }\n    }\n    \n    // Method 3: R-space data distribution patterns\n    // Different R-coordinates access different data portions\n    template &lt;typename TensorView&gt;\n    __device__ auto distribute_across_r_space(TensorView& tensor)\n    {\n        const auto r_coord = tile_distribution.calculate_r_coord();\n        \n        // Each R-coordinate processes different data slice\n        const index_t r0_offset = r_coord.at(number&lt;0&gt;{}) * chunk_size;\n        const index_t r1_offset = r_coord.at(number&lt;1&gt;{}) * chunk_size;\n        \n        // Access tensor with R-space offset\n        const auto base_x_coord = make_multi_index(r0_offset, r1_offset);\n        \n        return tensor(base_x_coord);\n    }\n    \n    // Method 4: Cross-warp cooperation using R-space\n    // R-space enables efficient warp-level primitives\n    template &lt;typename DataType&gt;\n    __device__ auto warp_cooperative_operation(DataType value)\n    {\n        const auto r_coord = tile_distribution.calculate_r_coord();\n        const index_t warp_id = r_coord.at(number&lt;0&gt;{});  // Which warp\n        const index_t lane_id = r_coord.at(number&lt;1&gt;{});  // Which lane in warp\n        \n        // Warp-level shuffle operations\n        DataType result = __shfl_xor_sync(0xFFFFFFFF, value, 1);  // XOR shuffle\n        \n        // Cross-warp communication via shared memory\n        __shared__ DataType warp_results[8];  // Up to 8 warps\n        \n        if (lane_id == 0) {\n            warp_results[warp_id] = result;  // Warp representative stores result\n        }\n        __syncthreads();\n        \n        // All threads can access cross-warp data\n        return warp_results[warp_id];\n    }\n}\n\n// R-space structure definition\ntemplate &lt;index_t... RSizes&gt;\nstruct r_space_definition\n{\n    static constexpr auto r_lengths = make_tuple(number&lt;RSizes&gt;{}...);\n    static constexpr index_t ndim_r = sizeof...(RSizes);\n    \n    // Total replication factor\n    static constexpr index_t replication_factor = (RSizes * ...);\n    \n    // R-space iteration\n    template &lt;typename Func&gt;\n    __device__ static constexpr void for_each_r(Func&& func)\n    {\n        // Iterate over all R-coordinates\n        unroll_r_space&lt;RSizes...&gt;(func);\n    }\n    \n    // Check if R-space is trivial (no replication)\n    static constexpr bool is_trivial = (replication_factor == 1);\n};\n\n// Example: No replication (most common)\nusing no_replication = r_space_definition&lt;1&gt;;\nstatic_assert(no_replication::replication_factor == 1);\nstatic_assert(no_replication::is_trivial == true);\n\n// Example: 2-way replication across warps\nusing warp_replication = r_space_definition&lt;2, 1&gt;;\nstatic_assert(warp_replication::replication_factor == 2);\nstatic_assert(warp_replication::is_trivial == false);\n\n// Example: Complex 3D replication pattern\nusing complex_replication = r_space_definition&lt;2, 2, 4&gt;;\nstatic_assert(complex_replication::replication_factor == 16);\n\n\nPython vs C++ R-Space Differences\n\n1. Thread Cooperation Mechanisms\n\nC++: Hardware-aware cooperation primitives\n// Warp-level operations (32 threads)\nDataType result = __shfl_down_sync(0xFFFFFFFF, value, 16);\n\n// Block-level shared memory\n__shared__ DataType shared_data[1024];\nshared_data[threadIdx.x] = local_value;\n__syncthreads();\n\n// Cross-block cooperation via global memory\n__device__ DataType* global_buffer;\natomicAdd(&global_buffer[blockIdx.x], contribution);\n\nDirect access to GPU hardware cooperation features\nZero-overhead synchronization primitives\nOptimal memory hierarchy utilization\n\nPython: Conceptual replication model\n# Simulates replication patterns for understanding\nfor r0 in range(replication_factor[0]):\n    for r1 in range(replication_factor[1]):\n        r_coord = [r0, r1]\n        # Shows logical replication structure\n\nHelps understand data sharing concepts\nShows replication coordinate organization\nGood for visualizing cooperation patterns\n\n\n\n\n2. Data Sharing Patterns\n\nC++: Efficient hardware-specific sharing\n// Shared memory (fast, block-local)\n__shared__ float lds_buffer[1024];\n\n// Warp shuffle (fastest, warp-local)\nfloat neighbor_value = __shfl_sync(0xFFFFFFFF, value, neighbor_lane);\n\n// Global memory (slow, device-wide)\n__device__ float* global_shared;\n\nMultiple memory hierarchy levels\nHardware-optimized data movement\nLatency and bandwidth characteristics known\n\nPython: Abstract sharing model\n# Focus on logical sharing patterns\nreplica_count = replication_factor[0] * replication_factor[1]\n# Shows how many threads share each piece of data\n\n\n\n3. Reduction Operations\n\nC++: Hardware-accelerated reductions\n// Warp-level reduction (single instruction)\nfloat sum = warp_reduce_sum(local_value);\n\n// Block-level reduction (shared memory tree)\nfloat total = block_reduce_sum(local_value);\n\n// Device-level reduction (cooperative groups)\nfloat global_sum = device_reduce_sum(local_value);\n\nHardware reduction instructions\nOptimized reduction trees\nMinimal synchronization overhead\n\nPython: Educational reduction concepts\n# Shows logical reduction structure\nprint(\"Broadcasting: Same value to multiple threads\")\nprint(\"Reductions: Collecting results from multiple threads\")\n\nExplains reduction operation concepts\nShows data flow patterns\nGood for understanding cooperation requirements\n\n\nKey Insight: R-space enables thread cooperation by managing data that needs to be shared or replicated across multiple threads.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#d-space-linearized-storage-2",
    "href": "concepts/04_coordinate_systems.html#d-space-linearized-storage-2",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "D-space: Linearized Storage",
    "text": "D-space: Linearized Storage\nD-space is the final step - converting 2D coordinates to linear memory addresses for efficient access.\n\n\n\n\n\n\n\nC++ Implementation Reference\nFile: include/ck_tile/core/tensor/tensor_descriptor.hpp\n#include &lt;ck_tile/core/tensor/tensor_descriptor.hpp&gt;\n#include &lt;ck_tile/core/container/multi_index.hpp&gt;\n#include &lt;ck_tile/core/numeric/tuple.hpp&gt;\n\n// D-space linearization in production kernels\ntemplate &lt;typename TensorDescriptor&gt;\n__device__ void example_d_space_linearization()\n{\n    constexpr auto tensor_desc = TensorDescriptor{};\n    \n    // Get tensor layout information\n    constexpr auto lengths = tensor_desc.get_lengths();  // Tensor dimensions\n    constexpr auto strides = tensor_desc.get_strides();  // Memory layout strides\n    constexpr index_t ndim = lengths.size();\n    \n    // Method 1: Standard linearization (X → D transformation)\n    template &lt;typename XCoord&gt;\n    __device__ constexpr auto calculate_linear_offset(const XCoord& x_coord)\n    {\n        index_t linear_offset = 0;\n        \n        // Dot product: offset = Σ(x_coord[i] * strides[i])\n        static_for&lt;0, ndim, 1&gt;{}([&](auto dim) {\n            linear_offset += x_coord.at(dim) * strides.at(dim);\n        });\n        \n        return linear_offset;\n    }\n    \n    // Method 2: Specialized layout patterns\n    \n    // Row-major (C-style) linearization\n    template &lt;typename XCoord&gt;\n    __device__ constexpr auto row_major_linearization(const XCoord& x_coord)\n    {\n        // For shape [M, N]: offset = x0 * N + x1\n        static_assert(ndim == 2);\n        return x_coord.at(number&lt;0&gt;{}) * lengths.at(number&lt;1&gt;{})\n             + x_coord.at(number&lt;1&gt;{});\n    }\n    \n    // Column-major (Fortran-style) linearization\n    template &lt;typename XCoord&gt;\n    __device__ constexpr auto column_major_linearization(const XCoord& x_coord)\n    {\n        // For shape [M, N]: offset = x1 * M + x0\n        static_assert(ndim == 2);\n        return x_coord.at(number&lt;1&gt;{}) * lengths.at(number&lt;0&gt;{})\n             + x_coord.at(number&lt;0&gt;{});\n    }\n    \n    // Method 3: Blocked/tiled linearization patterns\n    // Used for cache-efficient access patterns\n    template &lt;index_t BlockM, index_t BlockN&gt;\n    __device__ constexpr auto blocked_linearization(const auto& x_coord)\n    {\n        // Block-wise storage: arrange data in BlockM×BlockN tiles\n        const index_t block_row = x_coord.at(number&lt;0&gt;{}) / BlockM;\n        const index_t block_col = x_coord.at(number&lt;1&gt;{}) / BlockN;\n        const index_t in_block_row = x_coord.at(number&lt;0&gt;{}) % BlockM;\n        const index_t in_block_col = x_coord.at(number&lt;1&gt;{}) % BlockN;\n        \n        const index_t blocks_per_row = (lengths.at(number&lt;1&gt;{}) + BlockN - 1) / BlockN;\n        const index_t block_id = block_row * blocks_per_row + block_col;\n        const index_t in_block_offset = in_block_row * BlockN + in_block_col;\n        \n        return block_id * (BlockM * BlockN) + in_block_offset;\n    }\n    \n    // Method 4: Vectorized access patterns\n    // D-space coordinates for SIMD operations\n    template &lt;index_t VectorSize&gt;\n    __device__ constexpr auto vectorized_d_coordinates(const auto& base_x_coord)\n    {\n        // Generate D-coordinates for vector load/store\n        auto vector_d_coords = make_array&lt;index_t, VectorSize&gt;{};\n        \n        const auto base_d = calculate_linear_offset(base_x_coord);\n        \n        static_for&lt;0, VectorSize, 1&gt;{}([&](auto vec_idx) {\n            vector_d_coords[vec_idx] = base_d + vec_idx;\n        });\n        \n        return vector_d_coords;\n    }\n    \n    // Method 5: Memory coalescing analysis\n    // Check if D-coordinates enable coalesced access\n    template &lt;index_t NumThreads&gt;\n    __device__ bool check_coalescing_pattern()\n    {\n        // For optimal coalescing, consecutive threads should access\n        // consecutive memory addresses (consecutive D-coordinates)\n        \n        __shared__ index_t thread_d_coords[NumThreads];\n        \n        // Each thread calculates its D-coordinate\n        const index_t tid = threadIdx.x;\n        const auto my_x_coord = calculate_thread_x_coord(tid);\n        const auto my_d_coord = calculate_linear_offset(my_x_coord);\n        \n        thread_d_coords[tid] = my_d_coord;\n        __syncthreads();\n        \n        // Check if D-coordinates are consecutive\n        bool coalesced = true;\n        if (tid == 0) {\n            for (index_t i = 1; i &lt; NumThreads; ++i) {\n                if (thread_d_coords[i] != thread_d_coords[i-1] + 1) {\n                    coalesced = false;\n                    break;\n                }\n            }\n        }\n        \n        return coalesced;\n    }\n}\n\n// D-space descriptor with compile-time layout specification\ntemplate &lt;typename Lengths, typename Strides&gt;\nstruct d_space_descriptor\n{\n    static constexpr auto lengths = Lengths{};\n    static constexpr auto strides = Strides{};\n    static constexpr index_t ndim = lengths.size();\n    \n    // Total memory footprint\n    static constexpr index_t total_elements = calculate_total_elements(lengths);\n    \n    // Linearization function\n    template &lt;typename XCoord&gt;\n    __device__ static constexpr auto linearize(const XCoord& x_coord)\n    {\n        index_t offset = 0;\n        static_for&lt;0, ndim, 1&gt;{}([&](auto dim) {\n            offset += x_coord.at(dim) * strides.at(dim);\n        });\n        return offset;\n    }\n    \n    // Memory access pattern analysis\n    static constexpr bool is_row_major = check_row_major_strides(lengths, strides);\n    static constexpr bool is_column_major = check_column_major_strides(lengths, strides);\n    static constexpr bool is_contiguous = check_contiguous_layout(lengths, strides);\n};\n\n// Example: Row-major 2D matrix\nusing row_major_matrix = d_space_descriptor&lt;\n    tuple&lt;number&lt;1024&gt;, number&lt;768&gt;&gt;,  // Shape: 1024×768\n    tuple&lt;number&lt;768&gt;, number&lt;1&gt;&gt;      // Strides: [768, 1]\n&gt;;\nstatic_assert(row_major_matrix::is_row_major == true);\nstatic_assert(row_major_matrix::total_elements == 1024 * 768);\n\n// Example: Column-major 2D matrix\nusing col_major_matrix = d_space_descriptor&lt;\n    tuple&lt;number&lt;1024&gt;, number&lt;768&gt;&gt;,  // Shape: 1024×768\n    tuple&lt;number&lt;1&gt;, number&lt;1024&gt;&gt;     // Strides: [1, 1024]\n&gt;;\nstatic_assert(col_major_matrix::is_column_major == true);\n\n\nPython vs C++ D-Space Differences\n\n1. Linearization Performance\n\nC++: Compile-time offset calculation\n// Compile-time linearization (zero runtime cost)\nconstexpr auto x_coord = make_multi_index(number&lt;3&gt;{}, number&lt;4&gt;{});\nconstexpr auto offset = tensor_desc.calculate_offset(x_coord);\n// Entire calculation resolved at compile time\n\n// Runtime linearization (when needed)\nconst auto dynamic_offset = tensor_desc.calculate_offset(runtime_coord);\n// Optimized with unrolled loops, no function call overhead\n\nCompile-time calculation when coordinates are known\nOptimized runtime calculation for dynamic coordinates\nZero function call overhead\n\nPython: Always runtime calculation\n# Runtime linearization for all coordinates\nd_coord = x_coord[0] * tensor_shape[1] + x_coord[1]\n\nConsistent runtime model\nGood for understanding linearization concepts\nNo compile-time optimization\n\n\n\n\n2. Memory Layout Control\n\nC++: Multiple specialized layout types\n// Row-major descriptor\nauto row_major = make_naive_tensor_descriptor_packed(shape);\n\n// Column-major descriptor  \nauto col_major = make_tensor_descriptor(shape, col_major_strides, offsets);\n\n// Blocked/tiled descriptor\nauto blocked = make_tensor_descriptor(shape, blocked_strides, offsets);\n\n// Each type optimizes for different access patterns\n\nHardware-specific layout optimizations\nCache-friendly blocked patterns\nVectorization-friendly alignments\n\nPython: Single row-major model\n# Only row-major linearization shown\nd_coord = x_coord[0] * tensor_shape[1] + x_coord[1]\n\nSimplified for learning\nFocuses on core linearization concept\nEasy to understand and verify\n\n\n\n\n3. Memory Coalescing Awareness\n\nC++: Hardware coalescing optimization\n// Ensure consecutive threads access consecutive memory\nconst index_t tid = threadIdx.x;\nconst auto x_coord = make_multi_index(tid / 32, tid % 32);  // Coalesced pattern\nconst auto d_offset = tensor_desc.calculate_offset(x_coord);\n// d_offset values: 0, 1, 2, 3, ... (consecutive)\n\n// Vectorized loads/stores when possible\nauto vector_data = tensor_view.template get_vectorized&lt;4&gt;(d_offset);\n\nDirect control over memory access patterns\nHardware-aware access optimization\nVectorization opportunities identified\n\nPython: Abstract linearization model\n# Shows linearization without hardware considerations\nlinear_idx = x_coord[0] * tensor_shape[1] + x_coord[1]\n\nFocus on mathematical linearization\nNo hardware performance considerations\nGood for understanding coordinate-to-address mapping\n\n\nKey Insight: D-space handles the final step of converting logical coordinates to actual memory addresses that the hardware can access.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#complete-pipeline-py-x-d",
    "href": "concepts/04_coordinate_systems.html#complete-pipeline-py-x-d",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Complete Pipeline: P+Y → X → D",
    "text": "Complete Pipeline: P+Y → X → D\nLet’s trace a complete example from thread identification all the way to memory access.\n\nComplete Coordinate Pipeline\n\ngraph TB\n    subgraph \"Step 1: Thread Identification\"\n        TID[\"Thread ID = 5\"]\n        P[\"P-coordinatesP = [0, 5](warp 0, lane 5)\"]\n    end\n    \n    subgraph \"Step 2: Work Assignment\"\n        Y[\"Y-coordinatesY = [1, 0](element in tile)\"]\n    end\n    \n    subgraph \"Step 3: P+Y Transformation\"\n        TRANS[\"P + Y → XThread position + Element offset\"]\n        X[\"X-coordinatesX = [1, 5](tensor position)\"]\n    end\n    \n    subgraph \"Step 4: Linearization\"\n        LIN[\"X → DRow-major: D = x₀ × width + x₁\"]\n        D[\"D-coordinateD = 13(memory address)\"]\n    end\n    \n    subgraph \"Step 5: Memory Access\"\n        MEM[\"Hardware accessesmemory[13]\"]\n    end\n    \n    TID --&gt; P\n    P --&gt; TRANS\n    Y --&gt; TRANS\n    TRANS --&gt; X\n    X --&gt; LIN\n    LIN --&gt; D\n    D --&gt; MEM\n    \n    style P fill:#e3f2fd,stroke:#1976d2,stroke-width:3px\n    style Y fill:#fff3e0,stroke:#f57c00,stroke-width:3px\n    style X fill:#e8f5e9,stroke:#388e3c,stroke-width:3px\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px\n    style MEM fill:#ffebee,stroke:#c62828,stroke-width:3px",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#practical-example-matrix-multiplication",
    "href": "concepts/04_coordinate_systems.html#practical-example-matrix-multiplication",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Practical Example: Matrix Multiplication",
    "text": "Practical Example: Matrix Multiplication\nLet’s see how coordinate spaces work in a real matrix multiplication kernel.",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#real-tile-distribution-example-rmsnorm",
    "href": "concepts/04_coordinate_systems.html#real-tile-distribution-example-rmsnorm",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Real Tile Distribution Example: RMSNorm",
    "text": "Real Tile Distribution Example: RMSNorm\nLet’s see how these coordinate spaces work with a production CK tile distribution - RMSNorm:",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#testing-your-understanding",
    "href": "concepts/04_coordinate_systems.html#testing-your-understanding",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Testing Your Understanding",
    "text": "Testing Your Understanding\nLet’s verify your understanding of coordinate systems:",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/04_coordinate_systems.html#key-takeaways",
    "href": "concepts/04_coordinate_systems.html#key-takeaways",
    "title": "Coordinate Systems - The Mathematical Foundation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nUnderstanding coordinate systems is crucial for mastering tile distribution:\n🎯 The Five Coordinate Spaces:\n\nP-space (Partition): Thread identification\n\n✅ Each thread gets unique coordinates\n✅ Maps to hardware thread IDs\n✅ Foundation for work distribution\n\nY-space (Logical Tile): Per-thread work structure\n\n✅ Defines what each thread processes\n✅ Same structure for all threads\n✅ Logical organization of computation\n\nX-space (Physical Tensor): Actual data locations\n\n✅ Real tensor coordinates users understand\n✅ Where data actually lives\n✅ Target of P+Y transformation\n\nR-space (Replication): Data sharing\n\n✅ Enables thread cooperation\n✅ Handles broadcast and reduction\n✅ Manages shared data\n\nD-space (Linearized Storage): Memory addresses\n\n✅ Final hardware-level addresses\n✅ Enables efficient memory access\n✅ Hardware interface layer\n\n\n🔄 The Core Transformation: P + Y → X → D\n\n✅ Maps thread work to physical memory\n✅ Enables automatic memory coalescing\n✅ Provides predictable access patterns\n✅ Foundation for GPU performance\n\n💡 Why This Matters:\n\n✅ Automatic thread cooperation\n✅ Optimal memory access patterns\n✅ Hardware-agnostic programming\n✅ Predictable performance characteristics\n\nThese coordinate systems are the mathematical foundation that makes tile distribution both powerful and elegant. Master them, and you’ll understand how CK achieves its remarkable performance!",
    "crumbs": [
      "Coordinate Systems",
      "Coordinate Systems - The Mathematical Foundation"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#python-vs-c-key-differences",
    "href": "concepts/01_buffer_view.html#python-vs-c-key-differences",
    "title": "Buffer Views - Raw Memory Access",
    "section": "",
    "text": "C++: BufferView uses template metaprogramming with compile-time constants\n\nSize is encoded in the type: number&lt;8&gt;{}\nAddress space is a template parameter\nEnables aggressive compiler optimizations\n\nPython: Everything is runtime\n\nSize is a regular integer\nAddress space is an enum value\nDesigned for learning and experimentation\n\n\n\n\n\n\nC++: Direct pointer to GPU memory\n\nNo memory allocation - uses existing memory\nPointer arithmetic for addressing\nZero-copy access to device memory\n\nPython: NumPy array simulation\n\nUses host memory to simulate GPU memory\nPython manages memory lifetime\nEducational approximation of GPU behavior\n\n\n\n\n\n\nC++: Strong compile-time typing\nbuffer_view&lt;float*, address_space_enum::global&gt;  // Type encodes everything\nPython: Dynamic typing with runtime checks\nBufferView  # Generic class, properties stored as members",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#creation-modes",
    "href": "concepts/01_buffer_view.html#creation-modes",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Creation Modes",
    "text": "Creation Modes\n\nZero Value Mode\n// Basic buffer view creation with automatic zero for invalid elements\nvoid basic_creation_example() {\n    // Create data array\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    \n    // Create global memory buffer view\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size);\n}\n\n\nCustom Value Mode\nvoid custom_invalid_value_example() {\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    float custom_invalid = 13.0f;\n    \n    // Create buffer view with custom invalid value\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(\n        data, buffer_size, custom_invalid);\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#scalar-access",
    "href": "concepts/01_buffer_view.html#scalar-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Scalar Access",
    "text": "Scalar Access\nScalar Access Parameters:\n\ni (index): Base offset in terms of T elements\nlinear_offset: Additional offset to add to the base index\nis_valid_element: Boolean controlling whether the access is valid\n\nInvalid Value Modes:\nIs specified while making the buffer view as shown in the above example.\n\nZero mode (InvalidElementUseNumericalZeroValue = true): Returns zero for invalid accesses.\nCustom mode (InvalidElementUseNumericalZeroValue = false): Returns the specified invalid value\n\nOut-of-Bounds Handling: The buffer view automatically handles bounds checking when AMD buffer addressing is enabled.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#vector-access",
    "href": "concepts/01_buffer_view.html#vector-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Vector Access",
    "text": "Vector Access\nVector operations use template parameters to specify the vector type (e.g., ext_vector_t&lt;float, N&gt; for N elements). The same parameters apply as scalar access, but the operation reads/writes multiple contiguous elements.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#scalar-vs-vectorized-memory-access",
    "href": "concepts/01_buffer_view.html#scalar-vs-vectorized-memory-access",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Scalar vs Vectorized Memory Access",
    "text": "Scalar vs Vectorized Memory Access\n\ngraph LR\n    subgraph \"Scalar Access (4 instructions)\"\n        S1[\"Load float[0]\"] --&gt; R1[\"Register 1\"]\n        S2[\"Load float[1]\"] --&gt; R2[\"Register 2\"]\n        S3[\"Load float[2]\"] --&gt; R3[\"Register 3\"]\n        S4[\"Load float[3]\"] --&gt; R4[\"Register 4\"]\n    end\n    \n    subgraph \"Vectorized Access (1 instruction)\"\n        V1[\"Load float4[0]\"] --&gt; VR[\"Vector Register(4 floats)\"]\n    end\n    \n    subgraph \"Performance Impact\"\n        Perf[\"4x fewer instructionsBetter memory bandwidthReduced latency\"]\n    end\n    \n    R1 & R2 & R3 & R4 --&gt; Perf\n    VR --&gt; Perf\n    \n    style S1 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S2 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S3 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style S4 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style V1 fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Perf fill:#fef3c7,stroke:#f59e0b,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#understanding-bufferview-indexing",
    "href": "concepts/01_buffer_view.html#understanding-bufferview-indexing",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Understanding BufferView Indexing",
    "text": "Understanding BufferView Indexing\n\nflowchart LR\n    subgraph \"Input Parameters\"\n        Offset[\"Offset(e.g., 5)\"]\n        ValidFlag[\"Valid Flag(optional)\"]\n    end\n    \n    subgraph \"Processing\"\n        BoundsCheck{{\"Bounds Checkoffset &lt; buffer_size?\"}}\n        FlagCheck{{\"Flag Checkvalid_flag == True?\"}}\n        Access[\"Access Memorybuffer[offset]\"]\n    end\n    \n    subgraph \"Output\"\n        ValidResult[\"Valid ResultReturn value\"]\n        Invalid[\"Invalid ResultReturn 0 or default\"]\n    end\n    \n    Offset --&gt; BoundsCheck\n    ValidFlag --&gt; FlagCheck\n    \n    BoundsCheck --&gt;|Yes| FlagCheck\n    BoundsCheck --&gt;|No| Invalid\n    \n    FlagCheck --&gt;|Yes| Access\n    FlagCheck --&gt;|No| Invalid\n    \n    Access --&gt; ValidResult\n    \n    style Offset fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style ValidFlag fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style ValidResult fill:#d1fae5,stroke:#10b981,stroke-width:2px\n    style Invalid fill:#fee2e2,stroke:#ef4444,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-get-operations",
    "href": "concepts/01_buffer_view.html#c-get-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Get Operations",
    "text": "C++ Get Operations\n__device__ void example_get_operations()\n{\n    // Create buffer view\n    float data[8] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, 8);\n\n    // Simple get - compile-time bounds checking when possible\n    auto value_buf = buffer_view.template get&lt;float&gt;(0,1,true); //get the buffer from the buffer view\n    float value = value_buf.get(0); //get the value from the buffer\n\n    // Get with valid flag - branchless conditional access\n    bool valid_flag = false;\n    value_buf = buffer_view.template get&lt;float&gt;(0,1,valid_flag);\n    value = value_buf.get(0);\n    // Returns 0 valid_flag is false\n\n    // vectorized get\n    using float2 = ext_vector_t&lt;float, 2&gt;;\n    auto vector_buf = buffer_view.template get&lt;float2&gt;(0, 0, true);\n    // Loads 2 floats in a single instruction\n    float val1 = vector_buf.get(0);\n    float val2 = vector_buf.get(1);\n}\n\nCustom Value Return Mode for OOB & Invalid Access\nvoid scalar_get_operations_example() {\n\n   // Create data array\n   constexpr size_t buffer_size = 8;\n   float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n   float custom_invalid = 13.0f;\n   \n   // Create global memory buffer view with zero invalid value mode (default)\n   auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size, custom_invalid);\n   \n   // Invalid element access with is_valid_element=false\n   // Returns custom_invalid due to custom invalid value mode\n   auto invalid_value = buffer_view.template get&lt;float&gt;(0, 0, false);\n   printf(\"Invalid element: %.1f\\n\", invalid_value.get(0));\n   \n   // Out of bounds access - AMD buffer addressing handles bounds checking\n   // Will return custom_invalid when accessing beyond buffer_size\n   auto oob_value = buffer_view.template get&lt;float&gt;(0, 100, true);\n   printf(\"Out of bounds: %.1f\\n\", oob_value.get(0));\n}\nNOTE: Partial Out Of Bound (OOB) access during vector reads will return ‘junk’ values for the OOB access. Zero or custom invalid value is only returned for complete invalid/OOB access, i.e. when the first address of the vector is invalid.",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-update-operations",
    "href": "concepts/01_buffer_view.html#c-update-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Update Operations",
    "text": "C++ Update Operations\nvoid scalar_set_operations_example() {\n        \n    // Create data array\n    constexpr size_t buffer_size = 8;\n    float data[buffer_size] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};\n    \n    // Create global memory buffer view\n    auto buffer_view = make_buffer_view&lt;address_space_enum::global&gt;(data, buffer_size);\n    \n    // Basic set: set&lt;T&gt;(i, linear_offset, is_valid_element, value)\n    // Sets element at position i + linear_offset = 0 + 2 = 2\n    buffer_view.template set&lt;float&gt;(0, 2, true, 99.0f);\n    \n    // Invalid write with is_valid_element=false (ignored)\n    buffer_view.template set&lt;float&gt;(0, 3, false, 777.0f);\n    \n    // Out of bounds write - handled safely by AMD buffer addressing\n    buffer_view.template set&lt;float&gt;(0, 100, true, 555.0f);\n\n    // Vector set\n    using float2 = ext_vector_t&lt;float, 2&gt;;\n    float2 pair_values{100.0f, 200.0f};\n    buffer_view.template set&lt;float2&gt;(0, 5, true, pair_values);\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#atomic-vs-non-atomic-operations",
    "href": "concepts/01_buffer_view.html#atomic-vs-non-atomic-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "Atomic vs Non-Atomic Operations",
    "text": "Atomic vs Non-Atomic Operations\n\ngraph TB\n    subgraph \"Non-Atomic Operation (Race Condition)\"\n        NA1[\"Thread 1: Read value (10)\"] --&gt; NA2[\"Thread 1: Add 5 (15)\"]\n        NA3[\"Thread 2: Read value (10)\"] --&gt; NA4[\"Thread 2: Add 3 (13)\"]\n        NA2 --&gt; NA5[\"Thread 1: Write 15\"]\n        NA4 --&gt; NA6[\"Thread 2: Write 13\"]\n        NA5 & NA6 --&gt; NA7[\"Final value: 13 ❌(Lost update from Thread 1)\"]\n    end\n    \n    subgraph \"Atomic Operation (Thread-Safe)\"\n        A1[\"Thread 1: atomic_add(5)\"] --&gt; A2[\"Hardware ensuresserialization\"]\n        A3[\"Thread 2: atomic_add(3)\"] --&gt; A2\n        A2 --&gt; A4[\"Final value: 18 ✓(Both updates applied)\"]\n    end\n    \n    style NA7 fill:#fee2e2,stroke:#ef4444,stroke-width:2px\n    style A4 fill:#d1fae5,stroke:#10b981,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_buffer_view.html#c-atomic-operations",
    "href": "concepts/01_buffer_view.html#c-atomic-operations",
    "title": "Buffer Views - Raw Memory Access",
    "section": "C++ Atomic Operations",
    "text": "C++ Atomic Operations\n__device__ void example_atomic_operations()\n{\n    // Shared memory for workgroup-level reductions\n    __shared__ float shared_sum[256];\n    auto shared_buffer_view = make_buffer_view&lt;address_space_enum::lds&gt;(\n        shared_sum, 256\n    );\n\n    // Initialize shared memory\n    if (threadIdx.x &lt; 256) {\n        shared_buffer_view.template set&lt;float&gt;(threadIdx.x, 0.0f, true);\n    }\n    __syncthreads();\n\n    // Each thread atomically adds to shared memory\n    auto my_value = static_cast&lt;float&gt;(threadIdx.x);\n    shared_buffer_view.template update&lt;memory_operation_enum::atomic_add, float&gt;(0,0,true,my_value);\n    \n    // Atomic max for finding maximum value\n    shared_buffer_view.template update&lt;memory_operation_enum::atomic_max, float&gt;(0,1,true,my_value);\n    \n    __syncthreads();\n}",
    "crumbs": [
      "Foundation",
      "Buffer Views - Raw Memory Access"
    ]
  },
  {
    "objectID": "concepts/01_tensor_view.html#flow-layouts-from-coordinates-to-memory",
    "href": "concepts/01_tensor_view.html#flow-layouts-from-coordinates-to-memory",
    "title": "Tensor Views - Multi-Dimensional Structure",
    "section": "Flow & Layouts: From Coordinates to Memory",
    "text": "Flow & Layouts: From Coordinates to Memory\n\nflowchart LR\n    subgraph \"User Input\"\n        Coord[\"Coordinate(1, 2)\"]\n    end\n    \n    subgraph \"TensorView Processing\"\n        Shape[\"Shape Checkrow &lt; 3?col &lt; 4?\"]\n        Stride[\"Apply Stridesoffset = 1×4 + 2×1\"]\n        Buffer[\"BufferView Accessbuffer[6]\"]\n    end\n    \n    subgraph \"Result\"\n        Value[\"Value: 6\"]\n    end\n    \n    Coord --&gt; Shape\n    Shape --&gt;|Valid| Stride\n    Stride --&gt; Buffer\n    Buffer --&gt; Value\n    \n    style Coord fill:#e0e7ff,stroke:#4338ca,stroke-width:2px\n    style Shape fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style Stride fill:#dcfce7,stroke:#10b981,stroke-width:2px\n    style Buffer fill:#dbeafe,stroke:#3b82f6,stroke-width:2px\n    style Value fill:#d1fae5,stroke:#10b981,stroke-width:2px\n\n\ngraph TB\n    subgraph \"Row-Major Layout (C-style)\"\n        RM[\"Memory: [0,1,2,3,4,5,6,7,8,9,10,11]Shape: (3,4)Strides: (4,1)\"]\n        RMMatrix[\"[[0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11]]\"]\n        RM --&gt; RMMatrix\n    end\n    \n    subgraph \"Column-Major Layout (Fortran-style)\"\n        CM[\"Memory: [0,3,6,9,1,4,7,10,2,5,8,11]Shape: (3,4)Strides: (1,3)\"]\n        CMMatrix[\"[[0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11]]\"]\n        CM --&gt; CMMatrix\n    end\n    \n    subgraph \"Custom Stride (Transposed View)\"\n        TV[\"Memory: [0,1,2,3,4,5,6,7,8,9,10,11]Shape: (4,3)Strides: (1,4)\"]\n        TVMatrix[\"[[0, 4, 8] [1, 5, 9] [2, 6, 10] [3, 7, 11]]\"]\n        TV --&gt; TVMatrix\n    end\n    \n    style RM fill:#e0f2fe,stroke:#0284c7,stroke-width:2px\n    style CM fill:#fef3c7,stroke:#f59e0b,stroke-width:2px\n    style TV fill:#f3e8ff,stroke:#9333ea,stroke-width:2px",
    "crumbs": [
      "Foundation",
      "Tensor Views - Multi-Dimensional Structure"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#unmergetransform---multi-d-to-linear",
    "href": "concepts/02_transforms.html#unmergetransform---multi-d-to-linear",
    "title": "Individual Transforms",
    "section": "2. UnmergeTransform - Multi-D to Linear",
    "text": "2. UnmergeTransform - Multi-D to Linear\nConverts multi-dimensional coordinates to linear (1D) coordinates.\n\ngraph LR\n    subgraph \"Upper (3D)\"\n        U1[\"Shape: [3, 4, 2]Coord: (1, 3, 0)\"]\n    end\n    \n    subgraph \"Lower (1D)\"\n        L1[\"Linear index: 14\"]\n    end\n    \n    subgraph \"Calculation\"\n        C[\"14 = 1×(4×2) + 3×2 + 014 = 8 + 6 + 0\"]\n    end\n    \n    U1 --&gt; C\n    C --&gt; L1\n    \n    style U1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create unmerge transform for 3x4x2 tensor\nauto transform = make_unmerge_transform(\n    make_tuple(3, 4, 2)  // upper lengths\n);\n\n// The unmerge transform inherits from base_transform&lt;1, UpLengths::size()&gt;\n// meaning 1 lower dimension → multiple upper dimensions\n\n// Forward: Multi-D → Linear\nmulti_index&lt;3&gt; upper_coord{1, 3, 0};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 1*(4*2) + 3*2 + 0 = 14\n\n// Backward: Linear → Multi-D (unpacking)\nmulti_index&lt;1&gt; packed_idx{14};\nmulti_index&lt;3&gt; unpacked_coord;\n// This would compute: unpacked_coord = [1, 3, 0]\n\n// Use in tensor descriptor\nauto desc = make_naive_tensor_descriptor_packed(\n    make_tuple(number&lt;3&gt;{}, number&lt;4&gt;{}, number&lt;2&gt;{})\n);\n// UnmergeTransform is used internally for packed layouts",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#mergetransform---linear-to-multi-d",
    "href": "concepts/02_transforms.html#mergetransform---linear-to-multi-d",
    "title": "Individual Transforms",
    "section": "3. MergeTransform - Linear to Multi-D",
    "text": "3. MergeTransform - Linear to Multi-D\nInverse of UnmergeTransform - expands linear coordinates to multi-dimensional.\n\ngraph LR\n    subgraph \"Upper (1D)\"\n        U1[\"Linear index: 13\"]\n    end\n    \n    subgraph \"Lower (2D)\"\n        L1[\"Shape: [4, 5]Coord: (2, 3)\"]\n    end\n    \n    subgraph \"Calculation\"\n        C[\"13 = 2×5 + 3row=13÷5=2, col=13%5=3\"]\n    end\n    \n    U1 --&gt; C\n    C --&gt; L1\n    \n    style U1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style L1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n\n\n\n\n\n\n\n\nC++ Implementation\n// From composable_kernel/include/ck_tile/core/algorithm/coordinate_transform.hpp\n\n// Create merge transform for 4x5 tensor\nauto transform = make_merge_transform(\n    make_tuple(4, 5)  // lower lengths\n);\n\n// The merge transform inherits from base_transform&lt;LowLengths::size(), 1&gt;\n// meaning multiple lower dimensions → 1 upper dimension\n\n// Forward: Linear → Multi-D (splitting)\nmulti_index&lt;1&gt; upper_coord{13};\nmulti_index&lt;2&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 13 / 5 = 2 (row)\n//         lower_coord[1] = 13 % 5 = 3 (col)\n\n// CK provides two merge implementations:\n// 1. merge_v2_magic_division (default) - uses magic number division\n// 2. merge_v3_division_mod - for power-of-2 dimensions\n\n// Common usage: dimension reduction\nauto desc = transform_tensor_descriptor(\n    input_desc,\n    make_tuple(make_merge_transform(make_tuple(M, N))),\n    make_tuple(sequence&lt;0, 1&gt;{}),  // merge dims 0,1\n    make_tuple(sequence&lt;0&gt;{})      // to single dim 0\n);",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_transforms.html#additional-transforms-in-composable-kernel",
    "href": "concepts/02_transforms.html#additional-transforms-in-composable-kernel",
    "title": "Individual Transforms",
    "section": "Additional Transforms in Composable Kernel",
    "text": "Additional Transforms in Composable Kernel\n\n8. XorTransform - 2D XOR Mapping\n// From composable_kernel - special 2D coordinate transform\nauto transform = make_xor_transform(make_tuple(4, 8));  // 4x8 dimensions\n\n// The xor_t transform inherits from base_transform&lt;2, 2&gt;\n// Special mapping: lower[1] = upper[1] ^ (upper[0] % lengths[1])\n\nmulti_index&lt;2&gt; upper_coord{2, 5};\nmulti_index&lt;2&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower[0] = 2, lower[1] = 5 ^ (2 % 8) = 5 ^ 2 = 7\n\n// Used for specialized memory access patterns in some algorithms\n\n\n9. SliceTransform - Extract Sub-region\n// From composable_kernel - extract a slice from a dimension\nauto transform = make_slice_transform(\n    10,    // low_length (total dimension)\n    2,     // slice_begin\n    7      // slice_end\n);\n\n// Maps upper range [0, 5) to lower range [2, 7)\nmulti_index&lt;1&gt; upper_coord{3};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 3 + 2 = 5\n\n\n10. ModuloTransform - Cyclic Wrapping\n// From composable_kernel - modulo operation for cyclic access\nauto transform = make_modulo_transform(\n    4,     // modulus\n    16     // up_length\n);\n\n// Maps coordinates cyclically\nmulti_index&lt;1&gt; upper_coord{13};\nmulti_index&lt;1&gt; lower_coord;\ntransform.calculate_lower_index(lower_coord, upper_coord);\n// Result: lower_coord[0] = 13 % 4 = 1",
    "crumbs": [
      "Transformation Engine",
      "Individual Transforms"
    ]
  },
  {
    "objectID": "concepts/02_adaptors.html#advanced-c-patterns",
    "href": "concepts/02_adaptors.html#advanced-c-patterns",
    "title": "Tensor Adaptors - Chaining Transformations",
    "section": "Advanced C++ Patterns",
    "text": "Advanced C++ Patterns\n\nComplex Nested Transforms in C++\n// From composable_kernel - complex nested transform patterns\n\n// Example: 4D tensor with complex transformations\n// Shape: [A, B, C, D] with various transforms\n\n// 1. Create base descriptor\nauto base_desc = make_naive_tensor_descriptor_packed(\n    make_tuple(A, B, C, D)\n);\n\n// 2. Apply multiple transformations\n// First: merge first 3 dimensions\nauto step1_desc = transform_tensor_descriptor(\n    base_desc,\n    make_tuple(make_merge_transform(make_tuple(A, B, C)),\n               make_pass_through_transform(D)),\n    make_tuple(sequence&lt;0, 1, 2&gt;{}, sequence&lt;3&gt;{}),  // input mapping\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{})         // output: 2D\n);\n\n// 3. Then unmerge back but with different grouping\nauto step2_desc = transform_tensor_descriptor(\n    step1_desc,\n    make_tuple(make_unmerge_transform(make_tuple(A*B, C)),\n               make_pass_through_transform(D)),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),        // from 2D\n    make_tuple(sequence&lt;0, 1&gt;{}, sequence&lt;2&gt;{})      // to 3D\n);\n\n// The adaptor chain is embedded in the descriptors\n// CK optimizes these at compile time\n\n\nGPU Memory Layout Example\n// From composable_kernel - typical GPU block descriptor pattern\n\n// Create descriptor for thread block tile: 64x64\n// With 8x8 vector loads per thread\nconstexpr auto BlockM = 64;\nconstexpr auto BlockN = 64;\nconstexpr auto VectorM = 8;\nconstexpr auto VectorN = 8;\n\n// Thread arrangement: 8x8 threads\nconstexpr auto ThreadM = BlockM / VectorM;  // 8\nconstexpr auto ThreadN = BlockN / VectorN;  // 8\n\n// Create block descriptor with proper layout\nauto block_desc = transform_tensor_descriptor(\n    make_naive_tensor_descriptor_packed(\n        make_tuple(number&lt;BlockM&gt;{}, number&lt;BlockN&gt;{})\n    ),\n    make_tuple(\n        make_unmerge_transform(make_tuple(\n            number&lt;ThreadM&gt;{}, number&lt;VectorM&gt;{}\n        )),\n        make_unmerge_transform(make_tuple(\n            number&lt;ThreadN&gt;{}, number&lt;VectorN&gt;{}\n        ))\n    ),\n    make_tuple(sequence&lt;0&gt;{}, sequence&lt;1&gt;{}),           // from 2D\n    make_tuple(sequence&lt;0, 2&gt;{}, sequence&lt;1, 3&gt;{})     // to 4D: [TM,TN,VM,VN]\n);\n\n// This creates the layout:\n// - Dimension 0,1: Thread indices\n// - Dimension 2,3: Vector indices within thread\n// Enables coalesced memory access on GPU",
    "crumbs": [
      "Transformation Engine",
      "Tensor Adaptors - Chaining Transformations"
    ]
  }
]