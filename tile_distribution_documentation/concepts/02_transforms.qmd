---
title: "Individual Transforms"
format: 
  live-html:
    mermaid:
      theme: default
engine: jupyter
pyodide:
  packages:
    - micropip
---

The transformation engine is built from individual transform types that each handle specific coordinate conversions. Understanding these building blocks is essential for mastering the tile distribution system.

## ðŸŽ® **Interactive Exploration**

Explore transformation concepts interactively:

**[ðŸ”„ Tensor Transform Visualizer](https://ck.silobrain.com/tensor-transform/)** - Explore tensor descriptor transformations with visual graphs and mathematical formulas. See how data layouts change through various transformations.

## What Are Transforms?

Each transform type converts coordinates between different spaces:

- **Forward**: Upper coordinates â†’ Lower coordinates  
- **Backward**: Lower coordinates â†’ Upper coordinates

### Transform System Architecture

```{=html}
<div class="mermaid">
graph TB
    subgraph "Coordinate Spaces"
        US["Upper Space<br/>Logical Coordinates"]
        LS["Lower Space<br/>Physical Coordinates"]
    end
    
    subgraph "Transform Types"
        EMB["EmbedTransform<br/>Shape + Strides"]
        UNM["UnmergeTransform<br/>Linear â†’ Multi-D"]
        MRG["MergeTransform<br/>Multi-D â†’ Linear"]
        REP["ReplicateTransform<br/>Broadcast"]
        OFF["OffsetTransform<br/>Translation"]
        PAS["PassThroughTransform<br/>Identity"]
        PAD["PadTransform<br/>Boundaries"]
    end
    
    subgraph "Operations"
        FWD["Forward<br/>calculate_lower_index()"]
        BWD["Backward<br/>calculate_upper_index()"]
        UPD["Update<br/>update_lower_index()"]
    end
    
    US -->|Transform| LS
    LS -->|Inverse| US
    
    EMB --> FWD
    UNM --> FWD
    MRG --> FWD
    REP --> FWD
    OFF --> FWD
    PAS --> FWD
    PAD --> FWD
    
    style US fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style LS fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style FWD fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

```

## 1. EmbedTransform - View into Larger Space

The most fundamental transform - creates a view of shape into a larger buffer.

```{=html}
<div class="mermaid">
graph LR
    subgraph "Upper Space"
        U["Shape: [2, 3]<br/>Coords: (0,0) to (1,2)"]
    end
    
    subgraph "Lower Space"
        L["Buffer: 24 elements<br/>Strides: [12, 1]"]
    end
    
    subgraph "Mapping"
        M["(i,j) â†’ i*12 + j*1"]
    end
    
    U --> M
    M --> L
    
    style U fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style L fill:#fff3e0,stroke:#f57c00,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import EmbedTransform
from pytensor.tensor_coordinate import MultiIndex

# Create a 2x3 view with custom strides
transform = EmbedTransform(
    lengths=[2, 3],     # View shape
    strides=[12, 1]     # Row-major with padding
)

print(f"Lengths: {transform.lengths}")
print(f"Strides: {transform.strides}")
print(f"Dimensions: {transform.ndim}")

# Forward transform: logical â†’ physical
upper_coord = MultiIndex(size=2, initial_values=[1, 2])
lower_offset = transform.calculate_lower_index(upper_coord)
print(f"\nForward: {upper_coord.to_list()} â†’ offset {lower_offset}")
print(f"Calculation: 1Ã—12 + 2Ã—1 = {lower_offset}")

# This creates a 2x3 matrix view at position (0,0) in the buffer
# Useful for: sub-matrix views, sliding windows, padded layouts
```

### C++ Implementation

```cpp
// Create 3x4 view into larger buffer
auto transform = make_embed_transform(
    make_tuple(number<3>{}, number<4>{}),  // upper shape
    make_tuple(number<8>{}, number<8>{}),  // lower shape  
    make_tuple(number<8>{}, number<1>{})   // strides
);

// Use in tensor operations
auto upper_coord = make_tuple(number<1>{}, number<2>{});
auto lower_coord = transform.calculate_lower_index(upper_coord);
// Result: (1*8 + 2*1) = (10) in 1D
```

## 2. UnmergeTransform - Linear to Multi-D

Converts linear (1D) coordinates to multi-dimensional coordinates.

```{=html}
<div class="mermaid">
graph LR
    subgraph "Upper (1D)"
        U1["Linear index: 14"]
    end
    
    subgraph "Lower (3D)"
        L1["Shape: [3, 4, 2]<br/>Coord: (1, 3, 0)"]
    end
    
    subgraph "Calculation"
        C["14 = 1Ã—(4Ã—2) + 3Ã—2 + 0<br/>14 = 8 + 6 + 0"]
    end
    
    U1 --> C
    C --> L1
    
    style U1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style L1 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import UnmergeTransform

# Convert linear index to 3D coordinates
transform = UnmergeTransform(lengths=[3, 4, 2])

print(f"Lengths: {transform.lengths}")
print(f"Strides: {transform.strides}")

# Forward: 1D â†’ 3D
linear_idx = MultiIndex(size=1, initial_values=[14])
coord_3d = transform.calculate_lower_index(linear_idx)
print(f"\nLinear index {linear_idx.to_list()[0]} â†’ 3D coord {coord_3d.to_list()}")

# Verify the calculation
idx = coord_3d.to_list()[0] * (4*2) + coord_3d.to_list()[1] * 2 + coord_3d.to_list()[2]
print(f"Verification: {coord_3d.to_list()[0]}Ã—8 + {coord_3d.to_list()[1]}Ã—2 + {coord_3d.to_list()[2]} = {idx}")

# Backward: 3D â†’ 1D
coord_3d_input = MultiIndex(size=3, initial_values=[1, 3, 0])
linear_result = transform.calculate_upper_index(coord_3d_input)
print(f"\n3D coord {coord_3d_input.to_list()} â†’ Linear index {linear_result}")
```

## 3. MergeTransform - Multi-D to Linear

Inverse of UnmergeTransform - flattens multi-dimensional coordinates.

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import MergeTransform

# Flatten 2D coordinates to linear
transform = MergeTransform(lengths=[4, 5])

# Forward: 2D â†’ 1D
coord_2d = MultiIndex(size=2, initial_values=[2, 3])
linear = transform.calculate_lower_index(coord_2d)
print(f"2D coord {coord_2d.to_list()} â†’ Linear index {linear}")
print(f"Calculation: 2Ã—5 + 3 = {linear}")

# Backward: 1D â†’ 2D  
linear_input = MultiIndex(size=1, initial_values=[13])
coord_2d_result = transform.calculate_upper_index(linear_input)
print(f"\nLinear index {linear_input.to_list()[0]} â†’ 2D coord {coord_2d_result.to_list()}")
```

## 4. ReplicateTransform - Broadcasting

Broadcasts coordinates by ignoring certain dimensions.

```{=html}
<div class="mermaid">
graph TB
    subgraph "Broadcasting Example"
        U2D["Upper: [3, 4]<br/>(2D matrix)"]
        L1D["Lower: [4]<br/>(1D vector)"]
        B["Broadcast:<br/>All rows see same vector"]
    end
    
    U2D -->|"Ignore dim 0"| L1D
    
    style U2D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style L1D fill:#fff3e0,stroke:#f57c00,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import ReplicateTransform

# ReplicateTransform in Python has a simpler API
# It creates upper dimensions that all map to the same value
transform = ReplicateTransform(num_upper_dims=2)

# Show conceptual behavior
print("ReplicateTransform behavior:")
print("  Creates dimensions that all map to the same value")
print("  Used for broadcasting operations")
print("  Example: All threads in a dimension access the same data")

# Conceptual usage pattern
print("\nConceptual pattern:")
print("  Upper coords [0,2], [1,2], [2,2] all map to same lower element")
print("  This enables efficient broadcasting in GPU kernels")
```

## 5. OffsetTransform - Translation

Shifts coordinates by a fixed offset - useful for padding and boundaries.

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import OffsetTransform

# OffsetTransform adds a constant offset
element_space_size = 64  # Total buffer size
offset = 16             # Starting offset in buffer

transform = OffsetTransform(
    element_space_size=element_space_size,
    offset=offset
)

print(f"Element space size: {transform.element_space_size}")
print(f"Offset: {transform.offset}")

# Show offset behavior
print("\nOffset transform behavior:")
print(f"  Input index 0 â†’ Output index {offset}")
print(f"  Input index 5 â†’ Output index {offset + 5}")
print("  Used for accessing sub-regions of buffers")
```

## 6. PassThroughTransform - Identity

No-op transform that passes coordinates unchanged.

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import PassThroughTransform

# Identity transform - no change
element_space_size = 60  # 3*4*5 elements
transform = PassThroughTransform(element_space_size=element_space_size)

print(f"Element space size: {transform.element_space_size}")

# Show pass-through behavior
print("\nPassThroughTransform behavior:")
print("  Input index = Output index (identity)")
print("  Used when no transformation needed")
print("  Optimized away at compile time")
```

## 7. PadTransform - Smart Boundaries

Handles out-of-bounds access with padding values.

```{pyodide}
#| echo: true
#| output: true

from pytensor.tensor_descriptor import PadTransform

# PadTransform handles boundary conditions
lower_length = 3        # Original dimension length
left_pad = 1           # Padding on left
right_pad = 1          # Padding on right

transform = PadTransform(
    lower_length=lower_length,
    left_pad=left_pad,
    right_pad=right_pad
)

print(f"Lower length: {transform.lower_length}")
print(f"Left pad: {transform.left_pad}")
print(f"Right pad: {transform.right_pad}")
print(f"Upper length: {transform.get_upper_length()}")

# Show padding behavior
print("\nPadTransform behavior:")
print("  Maps padded coordinates to valid/invalid regions")
print("  Handles out-of-bounds access gracefully")
print("  Essential for convolution and stencil operations")
```

## Transform Composition

Transforms can be chained to create complex coordinate mappings:

```{=html}
<div class="mermaid">
graph LR
    subgraph "Transform Chain"
        I["Input<br/>(1, 2)"]
        T1["Offset<br/>+[10, 20]"]
        T2["Embed<br/>Stride [4, 1]"]
        O["Output<br/>(44, 22)"]
    end
    
    I --> T1
    T1 -->|"(11, 22)"| T2
    T2 --> O
    
    style I fill:#e0e7ff,stroke:#4338ca,stroke-width:2px
    style O fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

```{pyodide}
#| echo: true
#| output: true

# Example: Composing transforms
# In practice, transforms are composed at compile time in C++
# Here we show the conceptual pattern

# 1. Offset transform adds a base offset
offset_transform = OffsetTransform(
    element_space_size=100,
    offset=20  # Start at position 20
)

# 2. Embed transform for strided access
embed_transform = EmbedTransform(
    lengths=[5, 5],
    strides=[10, 1]  # Row-major with stride 10
)

print("Transform composition example:")
print(f"  Offset: {offset_transform.offset}")
print(f"  Embed strides: {embed_transform.strides}")

# Conceptual: coordinate [1,2] would map to:
# embed: 1*10 + 2*1 = 12
# offset: 12 + 20 = 32
print("\nExample mapping:")
print("  Coordinate [1,2] â†’ embed offset 12 â†’ final offset 32")
print("\nIn C++, these transforms compose at compile time for zero overhead!")
```

## Performance Considerations

```{=html}
<div class="mermaid">
graph TB
    subgraph "Transform Costs"
        Pass["PassThrough<br/>Zero cost"]
        Off["Offset<br/>Addition only"]
        Emb["Embed<br/>Multiply + Add"]
        Merge["Merge/Unmerge<br/>Multiple ops"]
        Pad["Pad<br/>Bounds check"]
    end
    
    Pass -->|Fastest| Performance
    Off --> Performance
    Emb --> Performance
    Merge --> Performance
    Pad -->|Slowest| Performance
    
    Performance["Performance Impact"]
    
    style Pass fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style Pad fill:#fee2e2,stroke:#ef4444,stroke-width:2px
</div>
```

### C++ Transform Chain Example

```cpp
// Complex transform composition in CK
// Transforms are composed through tensor descriptors

// Create base descriptor
auto base_desc = make_naive_tensor_descriptor(
    make_tuple(number<3>{}, number<3>{}),  // shape
    make_tuple(number<3>{}, number<1>{})   // strides
);

// Apply padding transform
auto padded_desc = transform_tensor_descriptor(
    base_desc,
    make_tuple(make_pad_transform(number<3>{}, number<1>{}, number<1>{}),  // dim 0
               make_pad_transform(number<3>{}, number<1>{}, number<1>{})), // dim 1
    make_tuple(sequence<0>{}, sequence<1>{}),      // lower dim mapping
    make_tuple(sequence<0>{}, sequence<1>{})       // upper dim mapping
);

// CK composes transforms at compile time for zero overhead!
// The resulting descriptor incorporates all transformations
```

## Summary

Individual transforms provide:
- **Modularity**: Each transform does one thing well
- **Composability**: Chain transforms for complex mappings
- **Efficiency**: Compile-time optimization in C++
- **Flexibility**: Handle any coordinate conversion need

Understanding these building blocks enables you to:
1. Create custom tensor views
2. Implement efficient data access patterns
3. Handle padding and boundaries correctly
4. Optimize memory layouts for GPU access