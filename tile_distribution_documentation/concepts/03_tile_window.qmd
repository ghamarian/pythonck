---
title: "Tile Window - Data Access Gateway"
format: live-html
---

## Overview

The TileWindow abstraction represents the culmination of the CK framework's approach to efficient tensor data access on GPUs. While TileDistribution determines the mapping between threads and tensor coordinates, TileWindow provides the actual mechanism for loading and storing data with optimal memory access patterns. This abstraction encapsulates the complexity of coalesced memory accesses, vectorization, and boundary handling into a clean interface that enables developers to focus on algorithmic logic rather than low-level memory management.

At its core, TileWindow implements a sophisticated windowing mechanism that views a subset of a larger tensor through the lens of a tile distribution. This windowing is not merely a simple sub-tensor extraction but a distribution-aware view that automatically generates the most efficient memory access patterns for the underlying hardware. The system achieves this by combining knowledge of the tensor's layout, the distribution pattern, and the GPU's memory subsystem characteristics to generate optimized load and store operations.

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import make_static_tile_distribution, make_tile_distribution_encoding
from pytensor.tile_window import make_tile_window, TileWindowWithStaticDistribution
from pytensor.tensor_view import make_tensor_view
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
from pytensor.static_distributed_tensor import make_static_distributed_tensor
```

## What is a TileWindow?

The fundamental challenge in GPU programming lies in the gap between logical tensor operations and the physical realities of memory access. While TileDistribution elegantly solves the problem of work assignment by mapping threads to tensor coordinates, it does not address how threads actually access the data at those coordinates. This is where TileWindow enters the picture, serving as the critical bridge between logical work assignment and physical memory operations.

TileWindow implements a distribution-aware windowing mechanism that transforms abstract coordinate mappings into concrete memory access patterns. The abstraction understands not just which data elements each thread needs, but also how to access them in a way that maximizes memory bandwidth utilization. This involves sophisticated techniques such as memory coalescing, where adjacent threads access adjacent memory locations, and vectorization, where multiple elements are loaded or stored in a single transaction.

The C++ implementation of TileWindow reveals its sophisticated architecture:

```cpp
// From ck_tile/core/tensor/tile_window.hpp
template <typename TensorView_, 
          typename WindowLengths_, 
          typename TileDistribution_>
struct tile_window_with_static_distribution
{
    using TensorView = remove_cvref_t<TensorView_>;
    using Distribution = remove_cvref_t<TileDistribution_>;
    
    // Core components
    TensorView tensor_view_;
    Distribution distribution_;
    
    // Window-specific information
    static constexpr auto window_lengths = WindowLengths{};
    
    // Load operation with automatic coalescing
    template <typename DistributedTensor>
    CK_TILE_DEVICE void load(DistributedTensor& dst) const
    {
        // Sophisticated loading logic that ensures:
        // 1. Coalesced memory access
        // 2. Vectorized operations where possible
        // 3. Proper handling of boundaries
        // 4. Optimal instruction scheduling
    }
};
```

The key insight is that TileWindow acts as an intelligent memory access coordinator. It receives high-level requests from the computational kernel ("load the data for this tile") and translates them into low-level memory operations that respect the GPU's hardware constraints. This translation is not trivial - it must account for warp-level memory coalescing rules, shared memory bank conflicts, and the limited number of memory transactions available per cycle.

The complete data flow through the TileWindow system follows a carefully orchestrated pattern. First, the TileDistribution determines which tensor coordinates each thread should process. Then, TileWindow translates these logical coordinates into physical memory addresses, taking into account the tensor's layout and stride patterns. Next, it generates optimized load instructions that maximize memory bandwidth utilization, often using vector loads to fetch multiple elements simultaneously. Finally, after computation, TileWindow provides similar optimizations for storing results back to global memory.

This abstraction is particularly powerful because it hides the complexity of GPU memory systems while still achieving near-optimal performance. Developers can write code that logically operates on tensor tiles without worrying about memory coalescing patterns, bank conflicts, or vectorization - TileWindow handles all these concerns automatically.

## Creating a TileWindow

The creation of a TileWindow involves establishing a sophisticated mapping between a tensor view and a tile distribution, with specific window dimensions and origin coordinates. This process encapsulates several layers of abstraction that work together to provide efficient data access patterns. Understanding the creation process is crucial for leveraging the full power of the CK framework's memory optimization capabilities.

The TileWindow construction requires four essential components: a tensor view that provides the underlying data access mechanism, window lengths that define the size of the tile in each dimension, an origin coordinate that specifies where the window starts within the larger tensor, and a tile distribution that determines how the window's data is distributed across threads. Each component plays a specific role in the overall memory access optimization strategy.

```{pyodide}
# First, create some sample data
data = np.arange(16, dtype=np.float32).reshape(4, 4)
print("Creating a TileWindow")
print("=" * 30)
print("Sample data - 4x4 matrix:")
print(data)
print()

# Create a tensor view for the data
tensor_desc = make_naive_tensor_descriptor_packed([4, 4])
tensor_view = make_tensor_view(data, tensor_desc)
print(f"Tensor view created: {tensor_view}")
print()

# Create a tile distribution
encoding = make_tile_distribution_encoding(
    rs_lengths=[],
    hs_lengthss=[[2], [2]],  # 2x2 tile
    ps_to_rhss_major=[[], []],
    ps_to_rhss_minor=[[], []],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)
distribution = make_static_tile_distribution(encoding)
print(f"Distribution created: {distribution}")
print()

# Create the tile window
window_lengths = [2, 2]  # 2x2 window
window_origin = [1, 1]   # Start at position [1,1]

tile_window = make_tile_window(
    tensor_view=tensor_view,
    window_lengths=window_lengths,
    origin=window_origin,
    tile_distribution=distribution
)

print(f"TileWindow created: {tile_window}")
print(f"Window size: {window_lengths}")
print(f"Window origin: {window_origin}")
print(f"Has distribution: {hasattr(tile_window, 'tile_distribution')}")
```

### C++ Implementation Reference

The TileWindow creation pattern shown above corresponds to the following C++ implementation:

**File**: `include/ck_tile/core/tensor/tile_window.hpp`

```cpp
// Creating a tile window in practice
template <typename BlockTileShape, typename DataType>
__device__ auto create_input_window(
    const DataType* __restrict__ p_global,
    index_t M, index_t N)
{
    // Create tensor view
    auto tensor_view = make_tensor_view<AddressSpaceEnum::Global>(
        p_global,
        make_naive_tensor_descriptor_packed(make_tuple(M, N)));
    
    // Define window parameters
    constexpr auto window_lengths = BlockTileShape{};
    const auto origin = make_tuple(blockIdx.x * window_lengths[I0],
                                   blockIdx.y * window_lengths[I1]);
    
    // Create window with distribution
    return make_tile_window(
        tensor_view,
        window_lengths,
        origin,
        make_static_tile_distribution(TileDistribution{}));
}
```

The window creation process demonstrates several important principles of the CK framework. First, the window dimensions (2√ó2 in this example) are chosen to match the tile distribution pattern, ensuring that each thread has work to do. Second, the origin coordinates allow the window to target specific regions of the tensor, enabling techniques like overlapped tiling for convolutions or sliding window operations. Third, the integration with tile distribution means that the window automatically knows how to map its data to the appropriate threads, eliminating the need for manual index calculations.

In the C++ implementation, the tile window creation involves template instantiation that captures all these parameters at compile time:

```cpp
// From ck_tile/core/tensor/tile_window.hpp
template <typename TensorView,
          typename WindowLengths,
          typename Origin,
          typename Distribution>
CK_TILE_HOST_DEVICE constexpr auto
make_tile_window(const TensorView& tensor_view,
                 const WindowLengths& window_lengths,
                 const Origin& origin,
                 const Distribution& distribution)
{
    // Create specialized window type based on distribution
    using WindowType = tile_window_with_static_distribution<
        TensorView, WindowLengths, Distribution>;
    
    // Construct window with all parameters
    return WindowType{tensor_view, window_lengths, origin, distribution};
}
```

## Loading Data with TileWindow

The load operation represents one of the most critical performance-sensitive operations in GPU kernel development. TileWindow's load functionality transforms a high-level request ("load this tile of data") into a series of optimized memory transactions that respect GPU hardware constraints while maximizing bandwidth utilization. This transformation involves several sophisticated techniques working in concert to achieve near-optimal performance.

The loading process begins with the tile window analyzing the distribution pattern to determine which data elements each thread needs to load. This analysis considers not just the logical mapping but also the physical memory layout, identifying opportunities for coalesced access where adjacent threads load from adjacent memory locations. The system then generates load instructions that take advantage of the GPU's memory subsystem capabilities, including vector loads that can fetch multiple elements in a single transaction.

```{pyodide}
print("Loading Data with TileWindow")
print("=" * 35)

# Show what we're loading
print("Source data window at [1,1] size [2,2]:")
print(data[1:3, 1:3])
print()

# Load data from the window - automatically creates distributed tensor!
try:
    distributed_tensor = tile_window.load()
    print("Load operation successful!")
    print(f"Distributed tensor created automatically: {type(distributed_tensor).__name__}")
    print()
    
    # Show what each thread sees
    print("What each thread loaded:")
    y_lengths = distribution.get_y_vector_lengths()
    
    for y0 in range(y_lengths[0]):
        for y1 in range(y_lengths[1]):
            y_indices = [y0, y1]
            try:
                value = distributed_tensor.get_element(y_indices)
                print(f"  Thread Y{y_indices}: loaded value {value}")
            except Exception as e:
                print(f"  Thread Y{y_indices}: error - {e}")
                
except Exception as e:
    print(f"Load operation failed: {e}")
```

The automatic creation of distributed tensors during the load operation represents a key design principle of the CK framework: eliminating boilerplate code while maintaining performance. When `load()` is called, the system automatically allocates thread-local storage in registers, determines the appropriate data type and size based on the distribution pattern, and generates optimized load instructions that minimize memory latency. This automation is achieved through template metaprogramming that resolves all decisions at compile time:

## Memory Access Pattern Visualization

The tile window's load operation generates optimal memory access patterns:

```{mermaid}
flowchart TD
    subgraph "Global Memory"
        GM["4x4 Tensor<br/>[0,1,2,3]<br/>[4,5,6,7]<br/>[8,9,10,11]<br/>[12,13,14,15]"]
    end
    
    subgraph "Tile Window View"
        TW["2x2 Window at [1,1]<br/>[5,6]<br/>[9,10]"]
    end
    
    subgraph "Thread Distribution"
        T0["Thread 0<br/>Y[0,0] = 5"]
        T1["Thread 1<br/>Y[0,1] = 6"]
        T2["Thread 2<br/>Y[1,0] = 9"]
        T3["Thread 3<br/>Y[1,1] = 10"]
    end
    
    subgraph "Distributed Tensor"
        DT["Thread-Local Storage<br/>Registers"]
    end
    
    GM --> TW
    TW --> T0
    TW --> T1
    TW --> T2
    TW --> T3
    T0 --> DT
    T1 --> DT
    T2 --> DT
    T3 --> DT
    
    style GM fill:#e3f2fd
    style TW fill:#fff3e0
    style DT fill:#c8e6c9
```

### C++ Load Implementation

The load operation is implemented with sophisticated optimization techniques:

**File**: `include/ck_tile/core/tensor/tile_window.hpp`

```cpp
// Automatic distributed tensor creation during load
template <typename Distribution>
CK_TILE_DEVICE auto load() const
{
    // Automatically deduce distributed tensor type
    using DstType = static_distributed_tensor<
        typename TensorView::DataType,
        typename Distribution::Ys2DDescriptor>;
    
    // Create distributed tensor with proper size
    DstType dst;
    
    // Generate optimized load operations
    constexpr auto spans = Distribution::get_distributed_spans();
    sweep_tile_spans(spans, [&](auto indices) {
        auto y_indices = Distribution::get_y_indices_from_distributed_indices(indices);
        auto x_coords = calculate_global_coordinates(y_indices);
        
        // Vectorized load when possible
        if constexpr (is_vectorizable(x_coords)) {
            dst.template get_vectorized_elements<VectorSize>(y_indices) = 
                tensor_view_.template get_vectorized_elements<VectorSize>(x_coords);
        } else {
            dst(y_indices) = tensor_view_(x_coords);
        }
    });
    
    return dst;
}
```

This approach ensures that every load operation is as efficient as possible while maintaining a clean, high-level interface that shields developers from low-level complexity.

## Computing on Distributed Data

The computational phase in the tile window paradigm represents the bridge between memory operations and algorithmic logic. Once data resides in thread-local registers through the distributed tensor abstraction, computations can proceed with maximum efficiency, leveraging the full computational throughput of the GPU's arithmetic units. The key to achieving this efficiency lies in maintaining data in registers throughout the computational phase, avoiding unnecessary round trips to slower memory hierarchies.

The distributed tensor provides a natural abstraction for element-wise operations, where each thread operates on its assigned data elements independently. This programming model aligns perfectly with the GPU's SIMT (Single Instruction, Multiple Thread) execution model, where threads in a warp execute the same instructions on different data. The tile window system ensures that these operations proceed without memory stalls or synchronization overhead, as each thread owns its data exclusively.

```{pyodide}
print("Computing on Distributed Data")
print("=" * 35)
print("Performing computation (multiply by 2) with direct store:")
print()

# Create output data and window
output_data = data.copy()
output_tensor_view = make_tensor_view(output_data, tensor_desc)
output_window = make_tile_window(
    tensor_view=output_tensor_view,
    window_lengths=window_lengths,
    origin=window_origin,
    tile_distribution=distribution
)

# Load current output values and compute directly
result_tensor = output_window.load()

# Do computation on each element
y_lengths = distribution.get_y_vector_lengths()
for y0 in range(y_lengths[0]):
    for y1 in range(y_lengths[1]):
        y_indices = [y0, y1]
        try:
            # Load input value
            input_value = distributed_tensor.get_element(y_indices)
            
            # Compute result (simple example: multiply by 2)
            output_value = input_value * 2
            
            # Store result directly in output tensor
            result_tensor.set_element(y_indices, output_value)
            
            print(f"  Y{y_indices}: {input_value} ‚Üí {output_value}")
            
        except Exception as e:
            print(f"  Y{y_indices}: error - {e}")

# Store results back to memory
try:
    output_window.store(result_tensor)
    print("Store operation successful - computed and stored in one step!")
except Exception as e:
    print(f"Store operation failed: {e}")
```

The optimization of avoiding intermediate tensors represents a crucial performance technique in GPU programming. Traditional approaches often create temporary storage for intermediate results, leading to increased register pressure and potential spills to slower memory. The tile window pattern enables in-place computation where results are computed and stored directly in the output distributed tensor, minimizing register usage and maximizing throughput. This pattern is particularly effective for element-wise operations and reductions where the computation can be fused with the memory operations:

```cpp
// Optimized compute pattern with fused operations
template <typename InputWindow, typename OutputWindow, typename ComputeFunc>
CK_TILE_DEVICE void fused_tile_operation(
    const InputWindow& input_window,
    OutputWindow& output_window,
    ComputeFunc compute_func)
{
    // Load and compute are fused - no intermediate storage
    auto input_tile = input_window.load();
    
    // Direct computation on distributed tensor
    auto output_tile = make_static_distributed_tensor<...>();
    
    // Apply computation with automatic vectorization
    constexpr auto spans = get_distributed_spans();
    sweep_tile_spans(spans, [&](auto indices) {
        auto y_idx = get_y_indices_from_distributed_indices(indices);
        output_tile(y_idx) = compute_func(input_tile(y_idx));
    });
    
    // Store immediately - data never leaves registers
    output_window.store(output_tile);
}
```

This fusion of load, compute, and store operations represents one of the most powerful optimization techniques in GPU kernel development, enabling memory-bound operations to achieve near-theoretical bandwidth utilization.

```{pyodide}
print("Verification:")
print("  Window region [1:3, 1:3] should be doubled:")
print(f"  Original: {data[1:3, 1:3].flatten()}")
print(f"  Result:   {output_data[1:3, 1:3].flatten()}")
print(f"  Expected: {(data[1:3, 1:3] * 2).flatten()}")
```

### C++ Implementation Reference

The computation pattern shown above is commonly used in CK kernels:

**File**: `include/ck_tile/ops/elementwise/unary_elementwise.hpp`

```cpp
// Element-wise operation pattern
template <typename TileWindow, typename Op>
__device__ void tile_elementwise_inplace(TileWindow& window, Op op)
{
    // Load tile data
    auto tile = window.load();
    
    // Apply operation to each element
    sweep_tile(tile, [&](auto& element) {
        element = op(element);
    });
    
    // Store back
    window.store(tile);
}
```

## The Complete Load-Compute-Store Pattern

The load-compute-store pattern represents the fundamental building block of efficient GPU kernels in the CK framework. This pattern encapsulates the entire lifecycle of tensor data as it flows through the GPU's memory hierarchy, from global memory through registers and back to global memory. Understanding and optimizing this pattern is crucial for achieving peak performance in real-world applications.

The pattern's efficiency stems from its careful orchestration of memory operations and computation to hide latency and maximize throughput. While a thread waits for its load operations to complete, the GPU's warp scheduler can execute instructions from other warps, effectively hiding memory latency. Once data arrives in registers, computation proceeds at full speed without memory bottlenecks. Finally, store operations are scheduled to overlap with subsequent loads, creating a pipeline that keeps all GPU resources fully utilized.

The C++ implementation of this pattern reveals sophisticated optimization techniques:

```cpp
// Complete load-compute-store pattern with overlapping
template <typename TileShape, typename ComputeFunc>
__global__ void optimized_tile_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    ComputeFunc compute_func)
{
    // Create tile windows for input and output
    constexpr auto distribution = make_static_tile_distribution<...>();
    
    auto input_window = make_tile_window(
        make_tensor_view(input, input_desc),
        TileShape{}, origin, distribution);
        
    auto output_window = make_tile_window(
        make_tensor_view(output, output_desc),
        TileShape{}, origin, distribution);
    
    // Pipelined execution
    auto tile = input_window.load();        // Initiate load
    __syncthreads();                        // Ensure load completes
    
    // Computation overlaps with memory operations
    sweep_tile(tile, compute_func);         // Process tile
    
    output_window.store(tile);              // Initiate store
    // Next iteration's load can overlap with this store
}
```

This pattern achieves optimal performance by ensuring that memory operations and computation are perfectly balanced, with neither becoming a bottleneck for the other.

```{pyodide}
print("Complete Load-Compute-Store Pattern")
print("=" * 45)

def tile_operation(input_data, window_origin, window_size, operation_name, compute_func):
    """Complete tile operation pattern."""
    print(f"Performing {operation_name}")
    print(f"   Input size: {input_data.shape}")
    print(f"   Window: {window_size} at {window_origin}")
    print()
    
    # Setup
    tensor_desc = make_naive_tensor_descriptor_packed(list(input_data.shape))
    input_view = make_tensor_view(input_data, tensor_desc)
    
    # Create distribution
    encoding = make_tile_distribution_encoding(
        rs_lengths=[],
        hs_lengthss=[[window_size[0]], [window_size[1]]],
        ps_to_rhss_major=[[], []],
        ps_to_rhss_minor=[[], []],
        ys_to_rhs_major=[1, 2],
        ys_to_rhs_minor=[0, 0]
    )
    distribution = make_static_tile_distribution(encoding)
    
    # Create input window
    input_window = make_tile_window(input_view, window_size, window_origin, distribution)
    
    # Load data - automatically creates distributed tensor!
    distributed_input = input_window.load()
    
    # Optimized pattern: Compute and store directly - no intermediate tensor needed!
    output_data = input_data.copy()
    output_view = make_tensor_view(output_data, tensor_desc)
    output_window = make_tile_window(output_view, window_size, window_origin, distribution)
    
    # Create output distributed tensor and compute+store in one loop
    distributed_output = output_window.load()  # Load current values
    y_lengths = distribution.get_y_vector_lengths()
    
    for y0 in range(y_lengths[0]):
        for y1 in range(y_lengths[1]):
            y_indices = [y0, y1]
            # Load, compute, and store in one step - more efficient!
            input_val = distributed_input.get_element(y_indices)
            output_val = compute_func(input_val)
            distributed_output.set_element(y_indices, output_val)
    
    # Store results back to memory
    output_window.store(distributed_output)
    
    return output_data

# Test with different operations
test_data = np.arange(9, dtype=np.float32).reshape(3, 3)
print("Test data:")
print(test_data)
print()

# Square operation
result1 = tile_operation(test_data, [0, 0], [2, 2], "Square", lambda x: x ** 2)
print("After square operation:")
print(result1)
print()

# Add 10 operation
result2 = tile_operation(test_data, [1, 1], [2, 2], "Add 10", lambda x: x + 10)
print("After add 10 operation:")
print(result2)
```

## Window Properties and Flexibility

TileWindow provides flexible windowing capabilities:

```{pyodide}
print("ü™ü Window Properties and Flexibility")
print("=" * 40)

# Create different window configurations
test_matrix = np.arange(25, dtype=np.float32).reshape(5, 5)
print("Test matrix (5x5):")
print(test_matrix)
print()

window_configs = [
    ([2, 2], [0, 0], "Top-left 2x2"),
    ([3, 3], [1, 1], "Center 3x3"),
    ([2, 3], [2, 1], "Bottom 2x3"),
    ([1, 5], [2, 0], "Row slice"),
    ([5, 1], [0, 2], "Column slice")
]

for window_size, origin, description in window_configs:
    print(f"üìç {description} - size {window_size} at {origin}:")
    
    # Extract the window data manually for verification
    end_row = origin[0] + window_size[0]
    end_col = origin[1] + window_size[1]
    
    if end_row <= test_matrix.shape[0] and end_col <= test_matrix.shape[1]:
        window_data = test_matrix[origin[0]:end_row, origin[1]:end_col]
        print(f"   Window data: {window_data.flatten()}")
    else:
        print(f"   Window extends beyond matrix bounds!")
    print()
```

## Testing Your Understanding

Let's verify that TileWindow operations work correctly:

```{pyodide}
print("üß™ Testing TileWindow Operations")
print("=" * 35)

def test_window_creation():
    """Test creating tile windows."""
    try:
        data = np.ones((4, 4), dtype=np.float32)
        desc = make_naive_tensor_descriptor_packed([4, 4])
        view = make_tensor_view(data, desc)
        
        encoding = make_tile_distribution_encoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
        dist = make_static_tile_distribution(encoding)
        
        window = make_tile_window(view, [2, 2], [1, 1], dist)
        return window is not None
    except Exception:
        return False

def test_load_store_roundtrip():
    """Test complete load-store cycle."""
    try:
        # Create test data
        data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)
        desc = make_naive_tensor_descriptor_packed([3, 3])
        view = make_tensor_view(data, desc)
        
        # Create distribution
        encoding = make_tile_distribution_encoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
        dist = make_static_tile_distribution(encoding)
        
        # Create window
        window = make_tile_window(view, [2, 2], [0, 0], dist)
        
        # Load data - automatically creates distributed tensor!
        loaded = window.load()
        
        # Verify we can access the data
        y_lengths = dist.get_y_vector_lengths()
        for y0 in range(y_lengths[0]):
            for y1 in range(y_lengths[1]):
                val = loaded.get_element([y0, y1])
                if val <= 0:  # Should have positive values
                    return False
        
        return True
    except Exception:
        return False

def test_window_flexibility():
    """Test different window configurations."""
    try:
        data = np.ones((5, 5), dtype=np.float32)
        desc = make_naive_tensor_descriptor_packed([5, 5])
        view = make_tensor_view(data, desc)
        
        # Test different window sizes
        window_configs = [
            ([2, 2], [0, 0]),
            ([3, 3], [1, 1]),
            ([1, 5], [2, 0])
        ]
        
        for window_size, origin in window_configs:
            encoding = make_tile_distribution_encoding(
                rs_lengths=[],
                hs_lengthss=[[window_size[0]], [window_size[1]]],
                ps_to_rhss_major=[[], []],
                ps_to_rhss_minor=[[], []],
                ys_to_rhs_major=[1, 2],
                ys_to_rhs_minor=[0, 0]
            )
            dist = make_static_tile_distribution(encoding)
            
            window = make_tile_window(view, window_size, origin, dist)
            if window is None:
                return False
                
        return True
    except Exception:
        return False

# Run tests
tests = [
    ("Window creation", test_window_creation),
    ("Load-store roundtrip", test_load_store_roundtrip),
    ("Window flexibility", test_window_flexibility)
]

print("Running TileWindow tests:")
for test_name, test_func in tests:
    result = test_func()
    status = "‚úÖ PASS" if result else "‚ùå FAIL"
    print(f"  {status}: {test_name}")
```

## Key Takeaways

TileWindow is the essential bridge between distribution and data access:

**1. Smart Data Access**
   - ‚úÖ Provides windowed views into large tensors
   - ‚úÖ Handles distribution-aware memory access
   - ‚úÖ Optimizes load/store operations automatically

**2. Seamless Integration**
   - ‚úÖ Works perfectly with TileDistribution
   - ‚úÖ Bridges tensor views and distributed computation
   - ‚úÖ Enables efficient thread cooperation

**3. Flexible Windowing**
   - ‚úÖ Supports arbitrary window sizes and positions
   - ‚úÖ Handles different tensor layouts
   - ‚úÖ Adapts to various access patterns

**4. Optimized Workflow**
   - ‚úÖ `load()` automatically creates distributed tensors
   - ‚úÖ Compute and store directly - no intermediate tensors needed
   - ‚úÖ More efficient than traditional load‚Üícompute‚Üístore pattern

**5. Two Loading Methods**
   - ‚úÖ `load()` for convenience (automatically creates tensor)
   - ‚úÖ `load_into(tensor)` for control (when you need specific tensor setup)

TileWindow completes the data access story. Next, we'll learn about sweep operations - the elegant way to iterate over all distributed data elements! 