---
title: "Tile Distribution - The Core API"
format: live-html
---

## Overview

TileDistribution is the heart of Composable Kernels' efficient GPU computation. It automatically maps logical coordinates to physical threads and memory locations, eliminating the need for manual thread management. This is the high-level API that GPU programmers actually use.

## ðŸŽ® **Interactive Exploration**

Explore tile distribution concepts interactively:

**[ðŸ“Š Tile Distribution Visualizer](https://ck.silobrain.com/tile-distribution)** - Interactive visualization of tile distribution structures and GPU memory layouts. Perfect for understanding how data is distributed across parallel processing elements.

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import (
    TileDistribution, make_static_tile_distribution, make_tile_distribution_encoding
)
from pytensor.tensor_coordinate import make_tensor_adaptor_coordinate, MultiIndex
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
```

## What is Tile Distribution?

Before diving into code, let's understand the fundamental problem TileDistribution solves:

**The Challenge**: You have a 256Ã—256 matrix multiplication and 64 GPU threads. How do you divide the work efficiently?

**The Solution**: TileDistribution automatically maps logical coordinates (like matrix position [i,j]) to physical threads and memory locations.

**ðŸŽ¯ Without TileDistribution:**

- Manual global memory address calculations
- Complex index arithmetic 
- Error-prone memory access patterns
- Different code for different matrix sizes

**ðŸŽ¯ With TileDistribution:**

- Automatic work assignment and memory access patterns
- Same code works for any size
- Hardware-aware thread cooperation by copying data to the correct thread registers
- No manual global memory address calculations

**Key Insight**: TileDistribution provides two essential functions: given the `warp id` and `thread id`, it returns the `global memory address` and the `access pattern` for the thread. 

## Creating Real-World Distributions

Let's create a real tile distribution that is used for many operations including RMSNorm, a common GPU operation. For the sake of naming, let's call it RMSNorm Distribution.

```{pyodide}
print("ðŸ“Š Creating a common distribution")
print("=" * 40)

# RMSNorm distribution parameters
# Repeat_M=4, WarpPerBlock_M=2, ThreadPerWarp_M=8, Vector_M=4
# Repeat_N=4, WarpPerBlock_N=2, ThreadPerWarp_N=8, Vector_N=4
encoding = make_tile_distribution_encoding(
    rs_lengths=[],  # Empty R sequence
    hs_lengthss=[
        [4, 2, 8, 4],  # H for X0: Repeat_M, WarpPerBlock_M, ThreadPerWarp_M, Vector_M
        [4, 2, 8, 4]   # H for X1: Repeat_N, WarpPerBlock_N, ThreadPerWarp_N, Vector_N
    ],
    ps_to_rhss_major=[[1, 2], [1, 2]],  # P maps to H dimensions
    ps_to_rhss_minor=[[1, 1], [2, 2]],  # P minor mappings
    ys_to_rhs_major=[1, 1, 2, 2],       # Y maps to H dimensions
    ys_to_rhs_minor=[0, 3, 0, 3]        # Y minor mappings
)

distribution = make_static_tile_distribution(encoding)

print(f"Distribution created: {type(distribution).__name__}")
print(f"X dimensions: {distribution.ndim_x} (logical data dimensions)")
print(f"Y dimensions: {distribution.ndim_y} (access pattern dimensions)")
print(f"P dimensions: {distribution.ndim_p} (thread partition dimensions)")
```

## Understanding Distribution Structure

Let's analyze what this distribution actually represents:

**ðŸ“ˆ RMSNorm Distribution Structure:**

The hierarchical structure breaks down as:

- **X0 (M dimension)**: 4Ã—2Ã—8Ã—4 = 256 elements
- **X1 (N dimension)**: 4Ã—2Ã—8Ã—4 = 256 elements
- **Total tile size**: 256Ã—256
- **P dimensions**: 2 (warp + thread within warp)
- **Y dimensions**: 4 (access pattern)
- **R dimensions**: 0 (no replication)

A tile distribution encoding specifies three different concepts. The first is an `adapter` that maps `P + Y` to `X`. The second is a `descriptor` that explains how `Y` dimensions are organized in registers. The third is called `spans`, which is basically a list of `Y` dimensions that are organized in registers and is used to loop over the `Y` dimensions. Let's see all of them in action. We start with the adapter, i.e., the `ps_ys_to_xs_adapter` that maps `P + Y` to `X`.

## Thread Work Assignments

Now let's see how different threads get their work assignments. The first and most important application of the adapter is to find the beginning of the `X` dimension that this thread is responsible for. There is a convenient method called `calculate_index` in `tile_distribution.hpp` that accepts the partition_index, i.e., `[Warp_id, Thread_id]` as a parameter. It then creates a vector consisting of the partition_index appended with as many zeros as we have Y dimensions. For the distribution in this particular example, it will be `[Warp_id, Thread_id, 0, 0, 0, 0]`.

```{pyodide}
print("ðŸ§µ Thread Work Assignments (2D Thread Grid):")
print("-" * 45)

# Use specific (M, N) tuples to ensure both dimensions change
thread_positions = [
    (0, 0), (0, 1), (1, 0), (1, 1),  # Basic 2x2 grid
    (2, 0), (0, 2), (2, 2), (3, 3)   # Extended positions
]

for i, (thread_m, thread_n) in enumerate(thread_positions):
    partition_index = [thread_m, thread_n]  # M, N thread coordinates
    
    try:
        # Calculate what X coordinates this thread handles
        x_index = distribution.calculate_index(partition_index)
        
        print(f"Thread[{thread_m},{thread_n}] (position {i+1}):")
        print(f"  â†’ Partition: {partition_index}")
        print(f"  â†’ X coordinates: {x_index.to_list()}")
        
        # Show Y access pattern for this thread
        y_lengths = distribution.get_y_vector_lengths()
        print(f"  â†’ Y pattern: {y_lengths}")
        print()
        
    except Exception as e:
        print(f"Thread[{thread_m},{thread_n}]: Error - {e}")
        print()
```

Another way to understand the `ps_ys_to_xs_adapter` is to use the `make_tensor_adaptor_coordinate` function directly. This function takes a `TensorAdaptor` and a `MultiIndex` and returns a `TensorAdaptorCoordinate`. We have seen this function in detail in previous sections. Let's try to see how the mapping works in our example distribution.

```{pyodide}
print("ðŸ§µ Thread Work Assignments (2D Thread Grid):")
print("-" * 45)

# Get the PS_YS to XS adapter
ps_ys_to_xs_adaptor = distribution.ps_ys_to_xs_adaptor

# Test coordinate transformations with various examples
test_cases = [
    # (ps_coords, ys_coords, description)
    ([0, 0], [0, 0, 0, 0], "Origin coordinates"),
    ([1, 0], [0, 0, 0, 0], "P0=1, P1=0, Ys at origin"),
    ([0, 1], [0, 0, 0, 0], "P0=0, P1=1, Ys at origin"),
    ([1, 1], [0, 0, 0, 0], "Both P coordinates at 1"),
    ([0, 0], [1, 0, 0, 0], "Y0=1, others at 0"),
    ([0, 0], [0, 1, 0, 0], "Y1=1, others at 0"),
    ([0, 0], [0, 0, 1, 0], "Y2=1, others at 0"),
    ([0, 0], [0, 0, 0, 1], "Y3=1, others at 0"),
    ([1, 1], [1, 1, 1, 1], "All coordinates at 1"),
    ([2, 2], [2, 2, 2, 2], "All coordinates at 2"),
]

for i, (ps_coords, ys_coords, description) in enumerate(test_cases):
    print(f"\n--- Test Case {i+1}: {description} ---")
    print(f"Input: ps_coords={ps_coords}, ys_coords={ys_coords}")
    
    # Concatenate P and Y coordinates into a single MultiIndex
    ps_ys_coords = ps_coords + ys_coords
    ps_ys_multi_idx = MultiIndex(len(ps_ys_coords), ps_ys_coords)
    
    # Transform coordinates
    coord = make_tensor_adaptor_coordinate(ps_ys_to_xs_adaptor, ps_ys_multi_idx)
    xs_coords = coord.get_bottom_index().to_list()
    print(f"Output: xs_coords={xs_coords}")
```



**2D Thread Cooperation**: Notice how threads in different positions (M,N) get different X coordinates. Thread[0,0] handles different data than Thread[1,0] or Thread[0,1]. This creates a 2D grid of work distribution where:

- **M dimension changes**: Thread[0,0] vs Thread[1,0] - different M positions
- **N dimension changes**: Thread[0,0] vs Thread[0,1] - different N positions

Each thread gets a unique partition index and automatically calculates which data elements to process. This calculation usually works through utility functions in `tile_window` and `sweep_tile`.

**The Magic**: Notice what you DON'T see: manual thread ID arithmetic, complex index calculations, or memory offset computations. TileDistribution handles all of that automatically!

## ys_to_d_descriptor

use `test_tile_window_ys_to_d.py` to see how the `ys_to_d_descriptor` works.


## Key Takeaways

TileDistribution is the foundation of efficient GPU computation in Composable Kernels:


Master TileDistribution, and you're ready to understand how it connects to actual data access through TileWindow in the next section! 