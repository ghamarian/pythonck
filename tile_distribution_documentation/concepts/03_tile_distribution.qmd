---
title: "Tile Distribution - The Core API"
format: live-html
---

## Overview

TileDistribution is the heart of Composable Kernels' efficient GPU computation. It automatically maps logical coordinates to physical threads and memory locations, eliminating the need for manual thread management. This is the high-level API that GPU programmers actually use.

## ðŸŽ® **Interactive Exploration**

Explore tile distribution concepts interactively:

**[ðŸ“Š Tile Distribution Visualizer](https://ck.silobrain.com/tile-distribution)** - Interactive visualization of tile distribution structures and GPU memory layouts. Perfect for understanding how data is distributed across parallel processing elements.

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import (
    TileDistribution, make_static_tile_distribution, make_tile_distribution_encoding
)
from pytensor.tensor_coordinate import make_tensor_adaptor_coordinate, MultiIndex
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
```

## What is Tile Distribution?

Before diving into code, let's understand the fundamental problem TileDistribution solves:

**The Challenge**: You have a 256Ã—256 matrix multiplication and 64 GPU threads. How do you divide the work efficiently?

**The Solution**: TileDistribution automatically maps logical coordinates (like matrix position [i,j]) to physical threads and memory locations.

**ðŸŽ¯ Without TileDistribution:**

- Manual global memory address calculations
- Complex index arithmetic 
- Error-prone memory access patterns
- Different code for different matrix sizes

**ðŸŽ¯ With TileDistribution:**

- Automatic work assignment and memory access patterns
- Same code works for any size
- Hardware-aware thread cooperation by copying data to the correct thread registers
- No manual global memory address calculations

**Key Insight**: TileDistribution provides two essential functions: given the `warp id` and `thread id`, it returns the `global memory address` and the `access pattern` for the thread. 

## Creating Real-World Distributions

Let's create a real tile distribution that is used for many operations including RMSNorm, a common GPU operation. For the sake of naming, let's call it RMSNorm Distribution.

```{pyodide}
print("ðŸ“Š Creating a common distribution")
print("=" * 40)

# RMSNorm distribution parameters
# Repeat_M=4, WarpPerBlock_M=2, ThreadPerWarp_M=8, Vector_M=4
# Repeat_N=4, WarpPerBlock_N=2, ThreadPerWarp_N=8, Vector_N=4
encoding = make_tile_distribution_encoding(
    rs_lengths=[],  # Empty R sequence
    hs_lengthss=[
        [4, 2, 8, 4],  # H for X0: Repeat_M, WarpPerBlock_M, ThreadPerWarp_M, Vector_M
        [4, 2, 8, 4]   # H for X1: Repeat_N, WarpPerBlock_N, ThreadPerWarp_N, Vector_N
    ],
    ps_to_rhss_major=[[1, 2], [1, 2]],  # P maps to H dimensions
    ps_to_rhss_minor=[[1, 1], [2, 2]],  # P minor mappings
    ys_to_rhs_major=[1, 1, 2, 2],       # Y maps to H dimensions
    ys_to_rhs_minor=[0, 3, 0, 3]        # Y minor mappings
)

distribution = make_static_tile_distribution(encoding)

print(f"Distribution created: {type(distribution).__name__}")
print(f"X dimensions: {distribution.ndim_x} (logical data dimensions)")
print(f"Y dimensions: {distribution.ndim_y} (access pattern dimensions)")
print(f"P dimensions: {distribution.ndim_p} (thread partition dimensions)")
```

## Understanding Distribution Structure

Let's analyze what this distribution actually represents:

**ðŸ“ˆ RMSNorm Distribution Structure:**

The hierarchical structure breaks down as:

- **X0 (M dimension)**: 4Ã—2Ã—8Ã—4 = 256 elements
- **X1 (N dimension)**: 4Ã—2Ã—8Ã—4 = 256 elements
- **Total tile size**: 256Ã—256
- **P dimensions**: 2 (warp + thread within warp)
- **Y dimensions**: 4 (access pattern)
- **R dimensions**: 0 (no replication)

A tile distribution encoding specifies three different concepts. The first is an `adapter` that maps `P + Y` to `X`. The second is a `descriptor` that explains how `Y` dimensions are organized in registers. The third is called `spans`, which is basically a list of `Y` dimensions that are organized in registers and is used to loop over the `Y` dimensions. Let's see all of them in action. We start with the adapter, i.e., the `ps_ys_to_xs_adapter` that maps `P + Y` to `X`.

## Thread Work Assignments

Now let's see how different threads get their work assignments. The first and most important application of the adapter is to find the beginning of the `X` dimension that this thread is responsible for. There is a convenient method called `calculate_index` in `tile_distribution.hpp` that accepts the partition_index, i.e., `[Warp_id, Thread_id]` as a parameter. It then creates a vector consisting of the partition_index appended with as many zeros as we have Y dimensions. For the distribution in this particular example, it will be `[Warp_id, Thread_id, 0, 0, 0, 0]`.

```{pyodide}
print("ðŸ§µ Thread Work Assignments (2D Thread Grid):")
print("-" * 45)

# Use specific (M, N) tuples to ensure both dimensions change
thread_positions = [
    (0, 0), (0, 1), (1, 0), (1, 1),  # Basic 2x2 grid
    (2, 0), (0, 2), (2, 2), (3, 3)   # Extended positions
]

for i, (thread_m, thread_n) in enumerate(thread_positions):
    partition_index = [thread_m, thread_n]  # M, N thread coordinates
    
    try:
        # Calculate what X coordinates this thread handles
        x_index = distribution.calculate_index(partition_index)
        
        print(f"Thread[{thread_m},{thread_n}] (position {i+1}):")
        print(f"  â†’ Partition: {partition_index}")
        print(f"  â†’ X coordinates: {x_index.to_list()}")
        
        # Show Y access pattern for this thread
        y_lengths = distribution.get_y_vector_lengths()
        print(f"  â†’ Y pattern: {y_lengths}")
        print()
        
    except Exception as e:
        print(f"Thread[{thread_m},{thread_n}]: Error - {e}")
        print()
```

Another way to understand the `ps_ys_to_xs_adapter` is to use the `make_tensor_adaptor_coordinate` function directly. This function takes a `TensorAdaptor` and a `MultiIndex` and returns a `TensorAdaptorCoordinate`. We have seen this function in detail in previous sections. Let's try to see how the mapping works in our example distribution.

```{pyodide}
print("ðŸ§µ Thread Work Assignments (2D Thread Grid):")
print("-" * 45)

# Get the PS_YS to XS adapter
ps_ys_to_xs_adaptor = distribution.ps_ys_to_xs_adaptor

# Test coordinate transformations with various examples
test_cases = [
    # (ps_coords, ys_coords, description)
    ([0, 0], [0, 0, 0, 0], "Origin coordinates"),
    ([1, 0], [0, 0, 0, 0], "P0=1, P1=0, Ys at origin"),
    ([0, 1], [0, 0, 0, 0], "P0=0, P1=1, Ys at origin"),
    ([1, 1], [0, 0, 0, 0], "Both P coordinates at 1"),
    ([0, 0], [1, 0, 0, 0], "Y0=1, others at 0"),
    ([0, 0], [0, 1, 0, 0], "Y1=1, others at 0"),
    ([0, 0], [0, 0, 1, 0], "Y2=1, others at 0"),
    ([0, 0], [0, 0, 0, 1], "Y3=1, others at 0"),
    ([1, 1], [1, 1, 1, 1], "All coordinates at 1"),
    ([2, 2], [2, 2, 2, 2], "All coordinates at 2"),
]

for i, (ps_coords, ys_coords, description) in enumerate(test_cases):
    print(f"\n--- Test Case {i+1}: {description} ---")
    print(f"Input: ps_coords={ps_coords}, ys_coords={ys_coords}")
    
    # Concatenate P and Y coordinates into a single MultiIndex
    ps_ys_coords = ps_coords + ys_coords
    ps_ys_multi_idx = MultiIndex(len(ps_ys_coords), ps_ys_coords)
    
    # Transform coordinates
    coord = make_tensor_adaptor_coordinate(ps_ys_to_xs_adaptor, ps_ys_multi_idx)
    xs_coords = coord.get_bottom_index().to_list()
    print(f"Output: xs_coords={xs_coords}")
```

**2D Thread Cooperation**: Notice how threads in different positions (M,N) get different X coordinates. Thread[0,0] handles different data than Thread[1,0] or Thread[0,1]. This creates a 2D grid of work distribution where:

- **M dimension changes**: Thread[0,0] vs Thread[1,0] - different M positions
- **N dimension changes**: Thread[0,0] vs Thread[0,1] - different N positions

Each thread gets a unique partition index and automatically calculates which data elements to process. This calculation usually works through utility functions in `tile_window` and `sweep_tile`.

**The Magic**: Notice what you DON'T see: manual thread ID arithmetic, complex index calculations, or memory offset computations. TileDistribution handles all of that automatically!

## The `ys_to_d_descriptor`: A Thread's Local Memory Map

After the `ps_ys_to_xs_adapter` assigns a large tile of work to a thread, the thread needs an efficient way to manage its local piece of that data. This is where the `ys_to_d_descriptor` comes in.

- **What it is:** It's the bridge between the logical, multi-dimensional `Y` coordinate space (how a programmer thinks about the thread's data) and the physical, 1D `D` offset (how that data is actually laid out in a flat register file or thread-local memory).
- **The Analogy:** It's a local filing system for each thread. A thread asks for data using a logical coordinate like `(y0=1, y1=0)`, and the descriptor instantly returns the shelf number (`D` offset) where that data is stored.

### From a Large Window to a Small Buffer

A key concept is that a large `TileWindow` (e.g., a 64x64 tile with 4096 elements) is loaded into a much smaller, thread-local buffer. The `Y` dimensions represent the small, repeating access pattern a single thread performs, and the buffer (the `D` space) only needs to be large enough to hold data for one unit of this pattern.

The `TileWindow.load()` operation intelligently fetches data from the large global tensor and places it into the correct slot in this small, local buffer. This powerful data distribution is at the heart of the system's efficiency.

### The Y-to-D Transformation in Action

Let's see this in action. We'll create a distribution, load data from a global tensor into a thread's local buffer (`StaticDistributedTensor`), and then access that data using logical `Y` coordinates.

```{pyodide}
#| file: validation_scripts/validate_ys_to_d_descriptor.py
#| echo: true
#| autorun: true


import sys
import numpy as np
import os

from pytensor.tile_distribution import make_tile_distribution_encoding, make_static_tile_distribution
from pytensor.static_distributed_tensor import StaticDistributedTensor
from pytensor.tensor_view import make_naive_tensor_view_packed
from pytensor.tile_window import TileWindowWithStaticDistribution

def demonstrate_ys_to_d_descriptor():
    """
    A minimal demonstration of creating a distribution and accessing thread-local
    data via Y coordinates, proving the function of ys_to_d_descriptor.
    """
    # 1. Create a realistic tile distribution
    encoding = make_tile_distribution_encoding(
        rs_lengths=[],
        hs_lengthss=[
            [2, 4, 4, 2],  # X0: 64 elements
            [2, 4, 4, 2]   # X1: 64 elements
        ],
        ps_to_rhss_major=[[1, 2], [1, 2]],
        ps_to_rhss_minor=[[1, 1], [2, 2]],
        ys_to_rhs_major=[1, 1, 2, 2],
        ys_to_rhs_minor=[0, 3, 0, 3]
    )
    distribution = make_static_tile_distribution(encoding)
    ys_to_d_desc = distribution.ys_to_d_descriptor
    y_lengths = ys_to_d_desc.get_lengths()

    # 2. Create a global tensor and a TileWindow
    x0_size = np.prod(encoding.hs_lengthss[0])  # 64
    x1_size = np.prod(encoding.hs_lengthss[1])  # 64
    global_shape = [x0_size, x1_size]
    global_data = np.arange(np.prod(global_shape), dtype=np.float32).reshape(global_shape)
    
    global_view = make_naive_tensor_view_packed(
        data=global_data.flatten(),
        lengths=global_shape
    )
    
    tile_window = TileWindowWithStaticDistribution(
        bottom_tensor_view=global_view,
        window_lengths=[x0_size, x1_size],
        window_origin=[0, 0],
        tile_distribution=distribution
    )

    # 3. Load data into the thread-local buffer (StaticDistributedTensor)
    distributed_tensor = tile_window.load()

    print("--- Conceptual Overview ---")
    print(f"A large {x0_size}x{x1_size} ({np.prod(global_shape)} elements) TileWindow was loaded.")
    print(f"The data for this thread is stored in a small {len(distributed_tensor.thread_buffer)}-element buffer (the 'D' space).")
    print(f"The ys_to_d_descriptor maps a logical Y coordinate to an offset in this buffer.")
    
    print("\n--- Y-to-D Transformation ---")
    print(f"Logical Y dimensions: {y_lengths}")
    strides = [np.prod(y_lengths[i+1:]) if i+1 < len(y_lengths) else 1 for i in range(len(y_lengths))]
    print(f"Calculated Strides: {strides}")
    print(f"Formula: D = y0*{strides[0]} + y1*{strides[1]} + y2*{strides[2]} + y3*{strides[3]}")
    
    print("\n--- Access Examples ---")
    
    example_coords = [
        ([0, 0, 0, 0], "Origin"),
        ([1, 0, 0, 0], "Step in Y0"),
        ([0, 1, 0, 0], "Step in Y1"),
        ([1, 1, 1, 1], "Max coordinate")
    ]

    for y_coords, desc in example_coords:
        # High-level access (what the programmer uses)
        value = distributed_tensor.get_element(y_coords)
        
        # Low-level mapping (what happens internally)
        d_offset = ys_to_d_desc.calculate_offset(y_coords)
        
        print(f"Logical Y coord: {str(y_coords):<15} | {desc:<18} | -> Maps to D offset: {d_offset:<2} | Fetches Value: {value}")

demonstrate_ys_to_d_descriptor()
```

### Performance Insights: Why This Design is Fast

The `ys_to_d_descriptor` is not just an organizational tool; it's a critical performance feature.

- **Cache Efficiency:** The linear `D` memory layout is extremely cache-friendly, ensuring that once data is loaded, subsequent accesses are fast.
- **No Synchronization:** Because each thread operates on its own local buffer, there is no need for slow synchronization primitives like locks or atomics between threads.
- **Enables Vectorization:** A flat, contiguous memory layout is the ideal scenario for modern SIMD hardware, allowing the processor to operate on multiple data points in a single instruction.
- **Predictable Access:** The static, well-defined layout allows the compiler to heavily optimize the memory access patterns into the most efficient machine code possible.

## Key Takeaways

TileDistribution is the foundation of efficient GPU computation in Composable Kernels:

- It separates the **logical problem** (`X` dimensions) from the **thread partitioning** (`P` dimensions) and the **local access pattern** (`Y` dimensions).
- The `ps_ys_to_xs_adapter` assigns work to threads.
- The `ys_to_d_descriptor` provides a high-performance map for threads to access their local data.

Master TileDistribution, and you're ready to understand how it connects to actual data access through TileWindow in the next section!
