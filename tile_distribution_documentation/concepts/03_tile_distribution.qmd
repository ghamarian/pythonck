---
title: "Tile Distribution - The Core API"
format: 
  live-html:
    mermaid:
      theme: default
---

## Overview

TileDistribution is the heart of Composable Kernels' efficient GPU computation. It automatically maps logical coordinates to physical threads and memory locations, eliminating the need for manual thread management. This is the high-level API that GPU programmers actually use.

The fundamental architecture of tile distribution in CK revolves around a sophisticated coordinate transformation system that maps between multiple coordinate spaces. At its core, the system manages four primary coordinate dimensions: X (the physical tensor dimensions), Y (the tile access pattern dimensions), P (the processing element dimensions representing thread hierarchy), and optionally R (replication dimensions for redundant computation). This multi-dimensional mapping enables the framework to express complex data access patterns in a mathematically rigorous way while maintaining high performance on modern GPU architectures.

The C++ implementation encapsulates this complexity within the `tile_distribution` template class, which combines three essential components: a `PsYs2XsAdaptor` that performs the coordinate transformation from processing and pattern dimensions to physical tensor coordinates, a `Ys2DDescriptor` that linearizes the Y dimensions for efficient register allocation, and a `StaticTileDistributionEncoding` that captures the hierarchical decomposition of work across the GPU's compute resources. This design allows the same high-level code to work efficiently across different tensor sizes and GPU configurations without manual tuning.

## Complete Tile Distribution System Overview

```{=html}
<div class="mermaid">
graph TB
    subgraph "Logical View"
        T["Tensor<br/>Multi-dimensional data"]
        TD["TileDistribution<br/>Work assignment"]
        TW["TileWindow<br/>Data view"]
    end
    
    subgraph "Coordinate Spaces"
        X["X: Physical tensor coords"]
        Y["Y: Tile pattern coords"]
        P["P: Processing element coords"]
        R["R: Replication coords (optional)"]
    end
    
    subgraph "GPU Execution"
        W["Warps<br/>32 threads each"]
        L["Lanes<br/>Thread within warp"]
        REG["Registers<br/>Thread-local storage"]
    end
    
    T --> TD
    TD --> TW
    
    TD --> X
    TD --> Y
    TD --> P
    TD --> R
    
    P --> W
    P --> L
    TW --> REG
    
    style TD fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style P fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style REG fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Interactive Exploration

Explore tile distribution concepts interactively:

**[Tile Distribution Visualizer](https://ck.silobrain.com/tile-distribution)** - Interactive visualization of tile distribution structures and GPU memory layouts. Perfect for understanding how data is distributed across parallel processing elements.

## Coordinate System Architecture

```{=html}
<div class="mermaid">
flowchart LR
    subgraph "Input"
        TC["Thread Coordinates<br/>(warpId, laneId)"]
    end
    
    subgraph "Transformation Pipeline"
        P2Y["P → Y<br/>Thread to pattern"]
        Y2X["Y → X<br/>Pattern to physical"]
        Y2D["Y → D<br/>Pattern to register"]
    end
    
    subgraph "Output"
        MC["Memory Coordinates<br/>Global addresses"]
        RI["Register Indices<br/>Local storage"]
    end
    
    TC --> P2Y
    P2Y --> Y2X
    P2Y --> Y2D
    Y2X --> MC
    Y2D --> RI
    
    style TC fill:#e0e7ff,stroke:#4338ca,stroke-width:2px
    style MC fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style RI fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
</div>
```

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution import (
    TileDistribution, make_static_tile_distribution, make_tile_distribution_encoding
)
from pytensor.tensor_coordinate import make_tensor_adaptor_coordinate, MultiIndex
from pytensor.tensor_descriptor import make_naive_tensor_descriptor_packed
```

## What is Tile Distribution?

Before diving into code, let's understand the fundamental problem TileDistribution solves. In GPU programming, the challenge of efficiently distributing work across thousands of parallel threads is paramount. Consider a concrete scenario: you have a 256×256 matrix multiplication operation and 64 GPU threads organized in warps. The question becomes how to divide this computational work in a way that maximizes memory bandwidth utilization, minimizes bank conflicts, and ensures coalesced memory accesses.

The traditional approach without a tile distribution framework requires programmers to manually calculate global memory addresses for each thread, implement complex index arithmetic that accounts for thread hierarchy (threads within warps, warps within blocks), handle edge cases for non-divisible matrix dimensions, and create different implementations for various matrix sizes. This manual approach is not only error-prone but also fails to adapt to different GPU architectures and their specific memory access patterns.

TileDistribution elegantly solves these challenges through a systematic approach to work distribution. It automatically assigns work to threads based on a hierarchical decomposition of the problem space, generates memory access patterns that respect GPU hardware constraints, provides a uniform interface that works across different tensor sizes and shapes, and ensures optimal thread cooperation by automatically managing data movement to thread-local registers.

The key insight that makes TileDistribution powerful is its ability to abstract the mapping between logical problem coordinates and physical execution resources. Given a thread's position in the GPU's execution hierarchy (specified by warp ID and lane ID within the warp), TileDistribution computes two critical pieces of information: the global memory addresses that this thread should access, and the specific access pattern that ensures efficient memory transactions. This abstraction is implemented in C++ through the following core structure:

```cpp
template <typename PsYs2XsAdaptor_,
          typename Ys2DDescriptor_,
          typename StaticTileDistributionEncoding_,
          typename TileDistributionDetail_>
struct tile_distribution
{
    // Core functionality: map thread coordinates to data
    CK_TILE_HOST_DEVICE static auto _get_partition_index()
    {
        if constexpr(NDimP == 1)
            return array<index_t, 1>{get_lane_id()};
        else if constexpr(NDimP == 2)
            return array<index_t, 2>{get_warp_id(), get_lane_id()};
    }
    
    // Calculate which tensor elements this thread accesses
    template <typename PartitionIndex>
    CK_TILE_HOST_DEVICE static auto calculate_tile_Ys_index(const PartitionIndex& ps_idx)
    {
        return detail::calculate_tile_Ys_index(
            StaticTileDistributionEncoding{}, ps_idx);
    }
};
```

## Problem Space Mapping

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 60%;">

graph TB
    subgraph "Problem Space (256×256 Matrix)"
        M["Full Matrix<br/>65,536 elements"]
        T1["Tile 1<br/>32×32"]
        T2["Tile 2<br/>32×32"]
        TN["Tile N<br/>32×32"]
    end
    
    subgraph "Thread Assignment"
        W0["Warp 0<br/>32 threads"]
        W1["Warp 1<br/>32 threads"]
        L0["Lane 0-31<br/>Individual threads"]
    end
    
    subgraph "Memory Pattern"
        MP["Coalesced Access<br/>Sequential addresses<br/>No bank conflicts"]
    end
    
    M --> T1
    M --> T2
    M --> TN
    
    T1 --> W0
    T1 --> W1
    W0 --> L0
    L0 --> MP
    
    style M fill:#fee2e2,stroke:#ef4444,stroke-width:2px
    style MP fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

## Creating a TileDistribution

Let's see how to create and use a TileDistribution in practice:

```{pyodide}
#| echo: true
#| output: true

from pytensor.tile_distribution import make_static_tile_distribution, make_tile_distribution_encoding
import numpy as np

# Create a tile distribution encoding
# This defines how a tensor is distributed across threads
encoding = make_tile_distribution_encoding(
    rs_lengths=[],  # No replication dimensions
    hs_lengthss=[[2, 2], [2, 2]],  # Hierarchical lengths for each X dimension
    ps_to_rhss_major=[[1], [2]],   # P to RH major mappings
    ps_to_rhss_minor=[[0], [0]],   # P to RH minor mappings  
    ys_to_rhs_major=[1, 2],         # Y to RH major mappings
    ys_to_rhs_minor=[1, 1]          # Y to RH minor mappings
)

# Create the tile distribution from the encoding
tile_dist = make_static_tile_distribution(encoding)

print(f"\nTile distribution created:")
print(f"- X dimensions: {tile_dist.ndim_x}")
print(f"- Y dimensions: {tile_dist.ndim_y}")
print(f"- P dimensions: {tile_dist.ndim_p}")
print(f"- X lengths: {tile_dist.get_lengths()}")
```

## Understanding Thread-to-Data Mapping

The core functionality of TileDistribution is mapping thread IDs to tensor coordinates:

```{pyodide}
#| echo: true  
#| output: true

# Simulate thread access patterns
def simulate_thread_access(tile_dist, thread_id):
    """Show which elements a thread accesses"""
    # In GPU: thread_id would come from get_lane_id()
    
    # For a 1D partition, use thread_id directly
    # For 2D partition, convert to (warp, lane)
    if tile_dist.ndim_p == 1:
        partition_index = [thread_id]
    else:
        warp_id = thread_id // 32
        lane_id = thread_id % 32
        partition_index = [warp_id, lane_id]
    
    print(f"\nThread {thread_id}:")
    print(f"  Partition index: {partition_index}")
    
    # Calculate the X coordinates this thread accesses
    x_coords = tile_dist.calculate_index(partition_index)
    print(f"  Accesses X coordinates: {x_coords.to_list()}")

# Show access pattern for first 4 threads
for tid in range(4):
    simulate_thread_access(tile_dist, tid)
```

## Hierarchical Decomposition

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 45%;">
graph TB
    subgraph "Level 1: Block Distribution"
        B["Thread Block<br/>256 threads"]
        BT1["Block Tile 1<br/>64×64"]
        BT2["Block Tile 2<br/>64×64"]
    end
    
    subgraph "Level 2: Warp Distribution"
        W["Warp<br/>32 threads"]
        WT1["Warp Tile 1<br/>16×16"]
        WT2["Warp Tile 2<br/>16×16"]
    end
    
    subgraph "Level 3: Thread Distribution"
        T["Thread"]
        TT["Thread Tile<br/>2×2"]
    end
    
    B --> BT1
    BT1 --> W
    W --> WT1
    WT1 --> T
    T --> TT
    
    style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style W fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style T fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
</div>
```

## Advanced Example: Matrix Multiplication Distribution

```{pyodide}
#| echo: true
#| output: true

# Create a more complex distribution for GEMM
# Using real-world RMSNorm pattern as example

# RMSNorm-style distribution for matrix operations
# This matches actual CK implementation patterns
gemm_encoding = make_tile_distribution_encoding(
    rs_lengths=[],  # No replication
    hs_lengthss=[
        [4, 2, 8, 4],  # M dimension: Repeat, WarpPerBlock, ThreadPerWarp, Vector
        [4, 2, 8, 4]   # N dimension: Repeat, WarpPerBlock, ThreadPerWarp, Vector
    ],
    ps_to_rhss_major=[[1, 2], [1, 2]],  # 2D thread grid mapping
    ps_to_rhss_minor=[[1, 1], [2, 2]],
    ys_to_rhs_major=[1, 1, 2, 2],       # Y dimension mapping
    ys_to_rhs_minor=[0, 3, 0, 3]
)

gemm_dist = make_static_tile_distribution(gemm_encoding)

print("GEMM Tile Distribution (RMSNorm pattern):")
print(f"X dimensions: {gemm_dist.ndim_x} (M and N)")
print(f"Y dimensions: {gemm_dist.ndim_y} (access pattern)")
print(f"P dimensions: {gemm_dist.ndim_p} (thread hierarchy)")
print(f"\nX lengths: {gemm_dist.get_lengths()}")
print(f"Each dimension: 4×2×8×4 = {4*2*8*4} elements")
print(f"Total tile size: {4*2*8*4}×{4*2*8*4} = {(4*2*8*4)**2} elements")
```

## Work Distribution Pattern

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 30%;">
flowchart TB
    subgraph "Matrix C (128×128)"
        C["16,384 elements"]
    end
    
    subgraph "Thread Grid (32×32)"
        TG["1,024 threads"]
    end
    
    subgraph "Per Thread"
        PT["4×4 tile<br/>16 elements"]
    end
    
    subgraph "Memory Access"
        MA["Coalesced reads<br/>Efficient writes<br/>No conflicts"]
    end
    
    C --> TG
    TG --> PT
    PT --> MA
    
    style C fill:#fee2e2,stroke:#ef4444,stroke-width:2px
    style TG fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style PT fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style MA fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

## Memory Access Patterns

One of the key benefits of TileDistribution is generating optimal memory access patterns:

```{pyodide}
#| echo: true
#| output: true

def analyze_access_pattern(tile_dist, num_threads=8):
    """Analyze memory access pattern for coalescing"""
    print(f"Analyzing access pattern for {num_threads} consecutive threads:")
    print("Thread ID | X coordinates accessed | Y pattern")
    print("-" * 50)
    
    for tid in range(num_threads):
        # Calculate partition index based on thread ID
        if tile_dist.ndim_p == 1:
            partition_index = [tid]
        else:
            # For 2D: assume threads are linearized within warps
            partition_index = [tid // 32, tid % 32]
        
        try:
            # Get X coordinates this thread accesses
            x_coords = tile_dist.calculate_index(partition_index)
            y_pattern = tile_dist.get_y_vector_lengths()
            
            print(f"    {tid:2d}    | {x_coords.to_list():20s} | {y_pattern}")
        except:
            print(f"    {tid:2d}    | Out of bounds        |")
    
    print("\n💡 Memory Access Pattern:")
    print("• Each thread accesses a unique set of X coordinates")
    print("• Y dimensions define the access pattern within each thread's tile")
    print("• Hardware automatically optimizes for coalescing")

# Create a simple distribution for analysis
analysis_encoding = make_tile_distribution_encoding(
    rs_lengths=[],
    hs_lengthss=[[2, 4], [2, 4]],  # 2x4 tiles per dimension
    ps_to_rhss_major=[[1], [2]],
    ps_to_rhss_minor=[[1], [1]],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)

analysis_dist = make_static_tile_distribution(analysis_encoding)
analyze_access_pattern(analysis_dist)
```

## C++ Integration Example

The real power of TileDistribution comes from its C++ implementation:

```cpp
// Real GEMM kernel pattern using TileDistribution
template<typename AType, typename BType, typename CType>
__global__ void gemm_kernel(
    const AType* __restrict__ a_ptr,
    const BType* __restrict__ b_ptr,
    CType* __restrict__ c_ptr,
    index_t M, index_t N, index_t K)
{
    // Define the tile distribution encoding at compile time
    using Encoding = tile_distribution_encoding<
        sequence<>,                                    // R: no replication
        tuple<sequence<4, 2, 8, 4>,                   // H for M dimension
              sequence<4, 2, 8, 4>>,                  // H for N dimension
        tuple<sequence<1, 2>, sequence<1, 2>>,        // P to RH major
        tuple<sequence<1, 1>, sequence<2, 2>>,        // P to RH minor
        sequence<1, 1, 2, 2>,                         // Y to RH major
        sequence<0, 3, 0, 3>                          // Y to RH minor
    >;
    
    // Create the distribution
    constexpr auto distribution = make_static_tile_distribution(Encoding{});
    
    // Create tensor views
    auto a_view = make_tensor_view<const AType>(
        a_ptr, 
        make_naive_tensor_descriptor_packed(make_tuple(M, K)));
    
    // Create tile window for this thread block
    auto a_window = make_tile_window(
        a_view,
        make_tuple(number<256>{}, number<64>{}),  // window size
        {blockIdx.x * 256, 0},                    // origin
        distribution);
    
    // Load data to distributed tensor (registers)
    auto a_reg = make_static_distributed_tensor<AType>(distribution);
    
    a_window.load(a_reg);
    
    // Computation happens in registers
    // Results written back through another window
}
```

## Transformation Pipeline

```{=html}
<div class="mermaid" style="margin: 0 auto; display: block; width: 100%;">
graph LR
    subgraph "Input"
        TID["Thread ID<br/>(0-1023)"]
    end
    
    subgraph "Stage 1"
        P["P-coordinates<br/>(warp, lane)"]
    end
    
    subgraph "Stage 2"
        Y["Y-coordinates<br/>(tile position)"]
    end
    
    subgraph "Stage 3"
        X["X-coordinates<br/>(tensor indices)"]
    end
    
    subgraph "Output"
        ADDR["Memory addresses<br/>Register indices"]
    end
    
    TID --> P
    P --> Y
    Y --> X
    X --> ADDR
    
    style TID fill:#e0e7ff,stroke:#4338ca,stroke-width:2px
    style ADDR fill:#d1fae5,stroke:#10b981,stroke-width:2px
</div>
```

## Performance Comparison

```{=html}
<div class="mermaid">
graph TB
    subgraph "Manual Implementation"
        M1["Calculate indices manually"]
        M2["Handle boundary conditions"]
        M3["Ensure coalescing"]
        M4["Manage bank conflicts"]
        M5["~200 lines of code"]
    end
    
    subgraph "With TileDistribution"
        T1["make_tile_distribution()"]
        T2["Automatic optimization"]
        T3["~10 lines of code"]
    end
    
    subgraph "Performance"
        P1["Same performance"]
        P2["Fewer bugs"]
        P3["Portable across GPUs"]
    end
    
    M1 --> M5
    T1 --> T3
    
    M5 --> P1
    T3 --> P1
    P1 --> P2
    P2 --> P3
    
    style M5 fill:#fee2e2,stroke:#ef4444,stroke-width:2px
    style T3 fill:#d1fae5,stroke:#10b981,stroke-width:2px
    style P3 fill:#fef3c7,stroke:#f59e0b,stroke-width:2px
</div>
```

## Summary

TileDistribution provides:

- **Automatic work distribution**: Maps threads to data efficiently
- **Optimal memory patterns**: Ensures coalesced access and minimal conflicts
- **Hierarchical decomposition**: Handles complex tiling strategies
- **Zero overhead**: Compile-time optimization in C++
- **Portability**: Same code works across different GPU architectures

Key benefits:

1. **Correctness**: Eliminates manual index calculation errors
2. **Performance**: Achieves hand-tuned performance automatically
3. **Productivity**: Reduces code from hundreds of lines to just a few
4. **Maintainability**: Clear separation of algorithm from distribution

The TileDistribution API is the foundation that enables Composable Kernels to achieve both high performance and high productivity in GPU programming.