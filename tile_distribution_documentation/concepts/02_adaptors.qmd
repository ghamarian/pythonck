---
title: "Tensor Adaptors - Chaining Transformations"
format: live-html
---

## Overview

While individual transforms are powerful, TensorAdaptors let us chain multiple transforms together to create complex coordinate transformations. Think of adaptors as transformation pipelines that can reshape, reorder, and restructure tensors in sophisticated ways.

TensorAdaptors are the bridge between individual transforms and the high-level tensor operations you'll use in real applications.

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

## TensorAdaptor Basics

Let's start by understanding what a TensorAdaptor is and how it works:

```{pyodide}
#| echo: true
#| output: true

# Import required modules
from pytensor.tensor_adaptor import (
    make_single_stage_tensor_adaptor,
    transform_tensor_adaptor,
    chain_tensor_adaptors,
    chain_tensor_adaptors_multi,
    make_identity_adaptor,
    make_transpose_adaptor
)
from pytensor.tensor_descriptor import (
    TensorAdaptor,
    PassThroughTransform,
    PadTransform,
    MergeTransform,
    ReplicateTransform,
    EmbedTransform,
    UnmergeTransform,
    transform_tensor_descriptor,
    make_naive_tensor_descriptor_packed,
    make_merge_transform,
    make_pass_through_transform,
    make_unmerge_transform,
    make_tuple,
    number,
    sequence
)
from pytensor.tensor_coordinate import MultiIndex
import numpy as np

print("üîó TensorAdaptor Overview")
print("--" * 40)
print("  TensorAdaptor chains multiple transforms together")
print("  Each adaptor has:")
print("    ‚Ä¢ transforms: List of individual transforms")
print("    ‚Ä¢ lower_dimension_hidden_idss: How transforms connect")
print("    ‚Ä¢ upper_dimension_hidden_idss: Hidden dimension mappings")
print("    ‚Ä¢ bottom_dimension_hidden_ids: Input dimensions")
print("    ‚Ä¢ top_dimension_hidden_ids: Output dimensions")
```

The most important method of a TensorAdaptor is `calculate_bottom_index`, which calculates the lower index from the upper index. It achives this by applying the transforms in reverse order and calling `calculate_lower_index` on each transform. 

Let's go over some of the utility functions for creating tensor adaptors and see how they work in real life. We start with one of the simplest one.

## Transpose Adaptor: Dimension Reordering

The transpose adaptor reorders tensor dimensions according to a permutation pattern.

```{pyodide}
#| echo: true
#| output: true

print("2Ô∏è‚É£ Transpose Adaptor")
print("--" * 40)

# Create transpose adaptor: [0, 1, 2] ‚Üí [2, 0, 1]
transpose_adaptor = make_transpose_adaptor(3, [2, 0, 1])

print(f"  Permutation: [2, 0, 1] (dimension 0‚Üí2, 1‚Üí0, 2‚Üí1)")
print(f"  Number of transforms: {transpose_adaptor.get_num_of_transform()}")
print(f"  Bottom dimensions: {transpose_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {transpose_adaptor.get_num_of_top_dimension()}")

# Test coordinate transformation
test_coords = [[0, 1, 2], [1, 0, 2], [2, 1, 0]]
print("\n  Transpose transformation test:")
for coord_list in test_coords:
    top_coord = MultiIndex(3, coord_list)
    bottom_coord = transpose_adaptor.calculate_bottom_index(top_coord)
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()}")
```

## Single-Stage Adaptors: Custom Transform Chains

You can create custom adaptors by specifying exactly which transforms to use and how they connect, the API for that is called `make_single_stage_tensor_adaptor`:

```{pyodide}
#| echo: true
#| output: true

print("3Ô∏è‚É£ Single-Stage Custom Adaptor")
print("--" * 40)

# Create adaptor that splits 1D coordinates to 2D
# Note: MergeTransform has 1D upper (top) and 2D lower (bottom)
merge_adaptor = make_single_stage_tensor_adaptor(
    transforms=[MergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0, 1]],  # Bottom: 2D dimensions 0 and 1
    upper_dimension_new_top_idss=[[0]]       # Top: 1D dimension 0 (merged)
)

print(f"  Transform: MergeTransform([2, 3]) - splits 1D to 2D")
print(f"  Bottom dimensions: {merge_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {merge_adaptor.get_num_of_top_dimension()}")

# Test merge transformation: 1D top ‚Üí 2D bottom
test_indices = [0, 2, 3, 5]
print("\n  Merge transformation test (1D ‚Üí 2D):")
for idx in test_indices:
    top_coord = MultiIndex(1, [idx])  # 1D top coordinate
    bottom_coord = merge_adaptor.calculate_bottom_index(top_coord)
    expected_row = idx // 3
    expected_col = idx % 3
    print(f"    [{idx}] ‚Üí {bottom_coord.to_list()} (expected: [{expected_row}, {expected_col}])")
```

Now that we saw how we can create an adaptor, let's see how we can combine a few of them together.

## Chaining Adaptors: Building Complex Transformations

The real power comes from chaining multiple adaptors together to create sophisticated transformations. Below we try some trivial example of combining merge and unmerge just to show how these transformations combine.

```{pyodide}
#| echo: true
#| output: true

print("4Ô∏è‚É£ Chaining Adaptors")
print("--" * 40)

# Create first adaptor: 1D ‚Üí 2D (merge splits)
adaptor_merge = make_single_stage_tensor_adaptor(
    transforms=[MergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0, 1]],
    upper_dimension_new_top_idss=[[0]]
)

# Create second adaptor: 2D ‚Üí 1D (unmerge combines)
adaptor_unmerge = make_single_stage_tensor_adaptor(
    transforms=[UnmergeTransform([2, 3])],
    lower_dimension_old_top_idss=[[0]],
    upper_dimension_new_top_idss=[[0, 1]]
)

# Chain them together (should be identity overall)
chained_adaptor = chain_tensor_adaptors(adaptor_merge, adaptor_unmerge)

print(f"  Chain: 1D ‚Üí 2D ‚Üí 1D (but actually results in 2D ‚Üí 2D)")
print(f"  Number of transforms: {chained_adaptor.get_num_of_transform()}")
print(f"  Bottom dimensions: {chained_adaptor.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {chained_adaptor.get_num_of_top_dimension()}")

# Test the chained transformation (should be identity)
test_coords = [[0, 0], [0, 2], [1, 0], [1, 2]]
print("\n  Chained transformation test (should be identity):")
for coord_list in test_coords:
    top_coord = MultiIndex(2, coord_list)  # 2D input
    bottom_coord = chained_adaptor.calculate_bottom_index(top_coord)
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()}")
```

## Transform Addition: Extending Existing Adaptors

You can add new transforms to existing adaptors using `transform_tensor_adaptor`. **Important**: The `new_upper_dimension_new_top_idss` parameter controls the **final output dimensions** of the adaptor.

```{pyodide}
#| echo: true
#| output: true

print("7Ô∏è‚É£ Practical Example: Matrix Transpose + Padding")
print("--" * 40)

# Create a matrix transpose adaptor
matrix_transpose = make_transpose_adaptor(2, [1, 0])

padded_transpose_2d = transform_tensor_adaptor(
    old_adaptor=matrix_transpose,
    new_transforms=[
        PadTransform(lower_length=4, left_pad=1, right_pad=1),  # Pad first dimension
        PassThroughTransform(3)                                 # Keep second dimension
    ],
    new_lower_dimension_old_top_idss=[[0], [1]],  # Apply to both dimensions
    new_upper_dimension_new_top_idss=[[0], [1]]   # Keep both in final output
)

print(f"  Operation: 2D transpose ‚Üí 2D (pad first dim, pass second)")
print(f"  Total transforms: {padded_transpose_2d.get_num_of_transform()}")
print(f"  Bottom dimensions: {padded_transpose_2d.get_num_of_bottom_dimension()}")
print(f"  Top dimensions: {padded_transpose_2d.get_num_of_top_dimension()}")

# Test with a 3x4 matrix conceptually
test_coords = [[0, 0], [0, 2], [3, 0], [3, 2]]
print("\n  Matrix transpose with padding test:")
for coord_list in test_coords:
    top_coord = MultiIndex(2, coord_list)
    bottom_coord = padded_transpose_2d.calculate_bottom_index(top_coord)
    # Manual verification: [i,j] ‚Üí transpose ‚Üí [j,i] ‚Üí pad first ‚Üí [j-1,i]
    expected = [coord_list[1] - 1, coord_list[0]]
    print(f"    {coord_list} ‚Üí {bottom_coord.to_list()} (expected: {expected})")

print("\n  ‚ö†Ô∏è  Common mistake: Using [[0]] for new_upper_dimension_new_top_idss")
print("     This creates 1D output, losing the second dimension!")
```

## Summary

TensorAdaptors are the coordination layer that makes complex tensor operations possible:

- **Identity Adaptor**: Starting point for building transformations
- **Transpose Adaptor**: Dimension reordering with permutation patterns
- **Single-Stage Adaptors**: Custom transform chains with precise control
- **Chained Adaptors**: Complex multi-stage transformation pipelines
- **Transform Addition**: Extending existing adaptors with new transforms
- **Advanced Examples**: Complex nested transforms with flattening behavior
- **GPU Block Descriptors**: Real-world GPU memory layout patterns
- **C++ Equivalents**: **True working equivalent** of complex nested C++ transforms

Key concepts:
- **Bottom/Top Dimensions**: Input and output coordinate spaces
- **Hidden Dimensions**: Internal coordinate mappings between transforms
- **Transform Chains**: Sequential application of multiple transforms
- **Coordinate Transformation**: Bidirectional mapping between coordinate spaces
- **Nested Transforms**: Complex multi-level transformation hierarchies

### Breakthrough Discovery

We successfully created the **true C++ equivalent** of complex nested transforms:

```python
# C++ nested transform equivalent
cpp_equivalent = make_single_stage_tensor_adaptor(
    transforms=[
        UnmergeTransform([A, B, C]),  # Merges A,B,C dimensions  
        PassThroughTransform(D)       # Passes through D dimension
    ],
    lower_dimension_old_top_idss=[[0], [1]],          # Transform inputs
    upper_dimension_new_top_idss=[[0, 1, 2], [3]]     # Transform outputs
)
```

**Key insights:**
- **Transform direction**: Names refer to lower‚Üíhigher, but `calculate_lower_index()` goes higher‚Üílower
- **UnmergeTransform**: Performs merging when used with `calculate_lower_index()`
- **Parameter mapping**: Controls the coordinate flow between dimensions
- **Mathematical equivalence**: Exact same results as C++ nested structure

TensorAdaptors bridge the gap between low-level transforms and high-level tensor operations, providing the flexibility to create sophisticated data layouts and access patterns that are essential for efficient GPU computing.

Next, we'll see how TensorAdaptors are combined with element space information to create complete **TensorDescriptors**. 