---
title: "Encoding Internals - The Internal Machinery"
format: live-html
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")

# Setup pytensor path for pyodide environment
import sys
import os
import numpy as np

# Add the project root to path so we can import pytensor
sys.path.insert(0, '/home/aghamari/github/composable_kernel/visualisation')

# Import the actual CK modules
from pytensor.tile_distribution_encoding import TileDistributionEncoding
from pytensor.tile_distribution import make_static_tile_distribution
```

## Overview

The tile distribution encoding system represents the core mathematical framework that transforms high-level tensor distribution specifications into concrete, optimized GPU kernel implementations. This sophisticated compile-time machinery bridges the gap between abstract mathematical descriptions and executable coordinate transformations, enabling the Composable Kernel framework to generate highly efficient code for complex tensor operations.

At its heart, the encoding system defines how multi-dimensional tensor data is distributed across GPU processing elements through a hierarchical decomposition scheme. By specifying relationships between different coordinate spaces - replication (R), hierarchical (H), partition (P), and yield (Y) dimensions - the encoding provides a complete blueprint for data layout and access patterns that can be resolved entirely at compile time.

## What is Tile Distribution Encoding?

The `tile_distribution_encoding` struct serves as a compile-time blueprint for defining complex tensor data layouts in GPU programming. It addresses a fundamental challenge in high-performance computing: how to map abstract tensor operations to the hierarchical memory and execution architecture of modern GPUs while maintaining both performance and programmability.

The encoding system operates through a multi-stage transformation pipeline that converts high-level specifications into concrete coordinate mappings. This pipeline consists of three primary components:

The **ps_ys_to_xs_adaptor** performs the crucial transformation from processing element coordinates (P) and logical access pattern coordinates (Y) to physical tensor coordinates (X). This adaptor encodes the complete mapping logic through a chain of coordinate transformations including replicate, unmerge, and merge operations.

The **ys_to_d_descriptor** manages the linearization of the multi-dimensional Y coordinate space into a one-dimensional data space suitable for register allocation. This component ensures efficient register utilization by mapping logical access patterns to physical storage locations.

The **transformation chains** are automatically constructed from the encoding parameters through template metaprogramming, ensuring zero runtime overhead. Each transformation in the chain corresponds to a specific coordinate space manipulation, collectively implementing the complete distribution strategy.

The C++ implementation leverages advanced template metaprogramming techniques:

```cpp
// From ck_tile/core/tensor/tile_distribution_encoding.hpp
template <typename RsLengths_,    // Replication dimension lengths
          typename HsLengthss_,   // Hierarchical dimension lengths
          typename Ps2RHssMajor_, // P to RH mapping (major)
          typename Ps2RHssMinor_, // P to RH mapping (minor)
          typename Ys2RHsMajor_,  // Y to RH mapping (major)
          typename Ys2RHsMinor_>  // Y to RH mapping (minor)
struct tile_distribution_encoding
{
    // All computations resolved at compile time
    static constexpr index_t NDimX = HsLengthss::size();
    static constexpr index_t NDimP = Ps2RHssMajor::size();
    static constexpr index_t NDimY = Ys2RHsMajor::size();
    static constexpr index_t NDimR = RsLengths::size();
    
    // Nested detail struct performs complex compile-time calculations
    struct detail
    {
        // Precomputed mappings and transformations
        static constexpr auto get_h_dim_lengths_prefix_sum();
        static constexpr auto get_uniformed_idx_y_to_h();
        // ... extensive compile-time computation ...
    };
};
```

## Encoding Structure Deep Dive

The tile distribution encoding employs a sophisticated type system that captures the complete specification of tensor distribution patterns at compile time. Understanding this structure is essential for leveraging the full power of the CK framework's optimization capabilities.

The encoding revolves around several interconnected dimension types that collectively define the distribution strategy:

```{pyodide}
print("Tile Distribution Encoding Structure")
print("=" * 50)

# Create a simple encoding to examine
encoding = TileDistributionEncoding(
    rs_lengths=[],                    # No replication
    hs_lengthss=[[2, 2], [2, 2]],   # 2x2 hierarchical tiles
    ps_to_rhss_major=[[1], [2]],     # P0 to H1, P1 to H2  
    ps_to_rhss_minor=[[0], [0]],     # Use first component
    ys_to_rhs_major=[1, 1, 2, 2],    # Y mapping to H
    ys_to_rhs_minor=[0, 1, 0, 1]     # Y component selection
)

print("Encoding Parameters:")
print(f"  rs_lengths: {encoding.rs_lengths}")
print(f"  hs_lengthss: {encoding.hs_lengthss}")
print(f"  ps_to_rhss_major: {encoding.ps_to_rhss_major}")
print(f"  ps_to_rhss_minor: {encoding.ps_to_rhss_minor}")
print(f"  ys_to_rhs_major: {encoding.ys_to_rhs_major}")
print(f"  ys_to_rhs_minor: {encoding.ys_to_rhs_minor}")

print("\nEncoding created successfully!")
```

## Parameter Breakdown

Each template parameter in the encoding system serves a specific purpose in defining the overall distribution strategy. These parameters work together to create a complete specification that can be transformed into efficient GPU code.

### R-Dimensions: Replication Specification

The `RsLengths` parameter defines dimensions that are replicated across processing units, enabling data sharing patterns essential for many tensor operations. In the transformation pipeline, these dimensions generate `coord_transform_enum::replicate` operations that broadcast data across multiple processing elements.

Replication serves several critical purposes in GPU kernel optimization. It enables efficient data reuse in operations like matrix multiplication where the same input data is needed by multiple output computations. It also facilitates reduction operations where multiple threads collaborate to compute a single result. The replication pattern directly impacts memory access efficiency and register utilization.

For example, in a GEMM operation:
- Matrix A might have `RsLengths = sequence<NWarp>`, indicating replication across warps computing different N-dimension tiles
- Matrix B might have `RsLengths = sequence<MWarp>`, indicating replication across warps computing different M-dimension tiles
- The output matrix C typically has no replication at the outer level, as each element is uniquely owned

### H-Dimensions: Hierarchical Decomposition

The `HsLengthss` parameter represents the hierarchical decomposition of tensor dimensions, encoded as a tuple of sequences. Each sequence defines how a logical tensor dimension (X-dimension) is broken down into finer-grained components that map to the GPU's execution hierarchy.

This hierarchical decomposition is fundamental to achieving high performance on GPUs. It enables:
- **Memory coalescing**: By aligning the decomposition with warp and thread organization
- **Register blocking**: By defining tile sizes that fit in the register file
- **Shared memory utilization**: By creating tiles that exploit data reuse

The decomposition typically follows a pattern like `sequence<RepeatCount, WarpCount, ThreadCount, VectorSize>`, where:
- `RepeatCount`: Number of iterations each thread performs
- `WarpCount`: Number of warps assigned to this dimension
- `ThreadCount`: Number of threads per warp assigned to this dimension  
- `VectorSize`: Number of elements accessed in a single vector operation

In the transformation pipeline, each H-dimension group generates `coord_transform_enum::unmerge` operations that break down abstract dimensions into their hierarchical components:

```cpp
// Example from block GEMM implementation
constexpr auto hs_lengthss = tuple<
    sequence<MIterPerWarp, MWarp>,  // M-dimension decomposition
    sequence<KIterPerWarp>          // K-dimension decomposition
>;
```

### P-Dimensions: Partition Mapping

The `Ps2RHssMajor` and `Ps2RHssMinor` parameters define how partition dimensions map to the underlying RH-dimensions. Partition dimensions represent the fundamental unit of work assignment in the GPU's execution model, typically corresponding to hardware constructs like warps and threads.

The mapping mechanism uses a major/minor indexing scheme where:
- Major index identifies which RH-dimension group (0 for R-dimensions, 1 to NDimX for H-dimensions)
- Minor index identifies the specific component within that group

This mapping enables flexible work distribution strategies. For instance, in a typical block-level distribution:
- `NDimP = 1` might map to `{MWarp, NWarp}` for distributing work across warp groups
- `NDimP = 2` at the combined level typically maps to `{warp_id, lane_id}` for thread-level distribution

The P-dimension mapping directly influences memory access patterns and computational efficiency:

```cpp
// P-dimension retrieval in tile_distribution
CK_TILE_HOST_DEVICE static auto _get_partition_index()
{
    if constexpr(NDimP == 1)
        return array<index_t, 1>{get_lane_id()};
    else if constexpr(NDimP == 2)
        return array<index_t, 2>{get_warp_id(), get_lane_id()};
}
```

### Y-Dimensions: Logical View Mapping

The `Ys2RHsMajor` and `Ys2RHsMinor` parameters define the logical view of the tile data - how users perceive and access the distributed tensor. Y-dimensions represent the iteration space for accessing tile elements, abstracting away the complexity of the underlying distribution.

The Y-to-RH mapping serves multiple purposes:
- **Interface definition**: Provides a clean API for accessing distributed data
- **Access pattern encoding**: Defines the order in which elements are processed
- **Vectorization support**: Enables efficient vector operations by grouping contiguous elements

In practice, Y-dimensions often correspond to the logical dimensions of the operation being performed. For a matrix multiplication tile:
- Y0 might represent the M-dimension iteration space
- Y1 might represent the K-dimension iteration space

The mapping ensures that logical access patterns translate to efficient physical memory operations.

- `[[4, 4], [4, 4]]` → 4x4 tiles (16 elements per thread)
- `[[2, 4], [4, 2]]` → 2x4 and 4x2 tiles (8 elements per thread)

### P→RH Mappings

**🔹 ps_to_rhss_major/minor (P→RH Mappings)**

Maps partition coordinates to RH space:
- Controls which H dimensions each P dimension affects
- major/minor specify different levels of the mapping

**📝 Conceptual Example:**
`ps_to_rhss_major=[[1], [2]]` means:
- P dimension 0 maps to H dimension 1
- P dimension 1 maps to H dimension 2

This determines how thread coordinates affect tile placement.

### Y→RH Mappings

**🔹 ys_to_rhs_major/minor (Y→RH Mappings)**

Maps Y coordinates to RH space:
- Determines how logical Y coordinates map to hierarchical structure
- Controls the internal organization of each thread's tile

**📝 Example Mapping:**
`ys_to_rhs_major=[1, 1, 2, 2]` means:
- Y[0] maps to H1
- Y[1] maps to H1  
- Y[2] maps to H2
- Y[3] maps to H2

This creates the internal structure of thread tiles.

## From Encoding to Tile Distribution

The magic happens when `make_static_tile_distribution()` transforms the mathematical encoding into runtime components:

```{pyodide}
print("🔧 From Encoding to Tile Distribution")
print("=" * 50)

print("The transformation process:")
print("1. Analyze encoding parameters")
print("2. Build transformation chains")
print("3. Create runtime components")
print("4. Optimize for performance")

# Create tile distribution from encoding
try:
    tile_distribution = make_static_tile_distribution(encoding)
    
    print("\n✅ Tile distribution created successfully!")
    print("\n📦 Internal Components Created:")
    print("  • ps_ys_to_xs_adaptor: Maps (P,Y) coordinates to X coordinates")
    print("  • ys_to_d_descriptor: Maps Y coordinates to linearized storage")
    print("  • encoding: Original mathematical specification")
    
    # Show the components exist
    print(f"\n🔍 Component Verification:")
    components = [
        ('ps_ys_to_xs_adaptor', 'P+Y → X transformation'),
        ('ys_to_d_descriptor', 'Y → D linearization'),
        ('encoding', 'Original specification')
    ]
    
    for component, description in components:
        has_component = hasattr(tile_distribution, component)
        status = "✅" if has_component else "❌"
        print(f"  {status} {component}: {description}")
        
except Exception as e:
    print(f"⚠️ Creation failed: {e}")
    print("Note: This demonstrates the concept even if creation fails")
```

## The P+Y → X Transformation Chain

The heart of the system is the adaptor that implements the P+Y → X transformation.

**🔗 The P+Y → X Transformation Chain**

The `ps_ys_to_xs_adaptor` implements a chain of transformations:
1. Start with P coordinates (which thread)
2. Add Y coordinates (which element in thread's tile)
3. Apply replication transforms (R-space)
4. Apply hierarchical transforms (H-space)
5. Merge into final X coordinates

**💡 Why This Chain Works:**
- Each transform handles one aspect of the mapping
- Transforms are composable and efficient
- The chain is built automatically from encoding
- Same pattern works for any distribution strategy

**📝 Conceptual Example:**
- Input: P=[1,0] + Y=[0,1] → Combined=[1,0,0,1]
- Transform 1: Handle replication (none in this case)
- Transform 2: Handle hierarchical structure
- Transform 3: Merge to final coordinates
- Output: X=[0,3] (final tensor position)

## The Y to D Linearization

The descriptor handles the linearization of Y coordinates to memory addresses.

### Y to D Linearization Details

The `ys_to_d_descriptor` handles memory layout within each thread:
1. Start with Y coordinates [y0, y1, y2, y3]
2. Apply thread's local layout (usually row-major)
3. Compute linear offset within thread's buffer
4. Result: D coordinate (memory address)

**📝 Example with [2, 2] tile:**
- Y=[0,0] → D=0
- Y=[0,1] → D=1
- Y=[1,0] → D=2
- Y=[1,1] → D=3

**💡 Why Separate from Adaptor:**
- Adaptor handles inter-thread coordination (P+Y → X)
- Descriptor handles intra-thread layout (Y → D)
- This separation enables different memory layouts
- Each thread can have its own descriptor

## Practical Examples

Different encodings create different behaviors:

**🎯 Example 1: Simple 2x2 Distribution**

```python
simple_encoding = TileDistributionEncoding(
    rs_lengths=[],
    hs_lengthss=[[2], [2]],
    ps_to_rhss_major=[[], []],
    ps_to_rhss_minor=[[], []],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)
```

- No replication
- Simple hierarchical structure
- Direct P→H mapping
- Good for basic matrix operations

**🎯 Example 2: With Replication**

```python
replicated_encoding = TileDistributionEncoding(
    rs_lengths=[2],  # 2-way replication
    hs_lengthss=[[2], [2]],
    ps_to_rhss_major=[[], []],
    ps_to_rhss_minor=[[], []],
    ys_to_rhs_major=[1, 2],
    ys_to_rhs_minor=[0, 0]
)
```

- 2-way replication for data sharing
- Same hierarchical structure
- Good for broadcast operations
- Enables thread cooperation

## Testing Your Understanding

Let's verify your understanding of encoding internals:

```{pyodide}
print("🧪 Testing Encoding Internals Understanding")
print("=" * 50)

def test_encoding_creation():
    """Test that we can create valid encodings."""
    try:
        test_encoding = TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False

def test_encoding_has_required_fields():
    """Test that encoding has all required fields."""
    encoding = TileDistributionEncoding(
        rs_lengths=[],
        hs_lengthss=[[2], [2]],
        ps_to_rhss_major=[[], []],
        ps_to_rhss_minor=[[], []],
        ys_to_rhs_major=[1, 2],
        ys_to_rhs_minor=[0, 0]
    )
    
    required_fields = [
        'rs_lengths', 'hs_lengthss', 'ps_to_rhss_major', 
        'ps_to_rhss_minor', 'ys_to_rhs_major', 'ys_to_rhs_minor'
    ]
    
    for field in required_fields:
        if not hasattr(encoding, field):
            return False
    return True

def test_different_encodings():
    """Test creating different types of encodings."""
    encodings = [
        # Simple encoding
        TileDistributionEncoding(
            rs_lengths=[],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        ),
        # With replication
        TileDistributionEncoding(
            rs_lengths=[2],
            hs_lengthss=[[2], [2]],
            ps_to_rhss_major=[[], []],
            ps_to_rhss_minor=[[], []],
            ys_to_rhs_major=[1, 2],
            ys_to_rhs_minor=[0, 0]
        )
    ]
    
    return len(encodings) == 2

# Run tests
tests = [
    ("Encoding creation", test_encoding_creation),
    ("Required fields", test_encoding_has_required_fields),
    ("Different encodings", test_different_encodings)
]

print("Running encoding internals tests:")
for test_name, test_func in tests:
    try:
        result = test_func()
        status = "PASS" if result else "FAIL"
        print(f"  [{status}] {test_name}")
    except Exception as e:
        print(f"  [ERROR] {test_name} - {str(e)}")
```

## Key Takeaways

The tile distribution encoding system represents a sophisticated solution to the challenge of mapping high-level tensor operations to GPU hardware. Understanding its internals reveals several key architectural principles:

### Mathematical Foundation

The encoding structure provides a complete mathematical specification of thread organization and data distribution:

1. **Dimensional Hierarchy**: The system defines four types of dimensions (R, H, P, Y) that collectively describe the complete distribution strategy
2. **Compile-Time Resolution**: All relationships and transformations are resolved at compile time, ensuring zero runtime overhead
3. **Composable Transformations**: Individual coordinate transformations (replicate, unmerge, merge) compose to create complex mappings
4. **Hardware Alignment**: The hierarchical decomposition naturally aligns with GPU hardware organization

### Automatic Code Generation

The transformation from mathematical specification to executable code happens through a sophisticated pipeline:

1. **Encoding Analysis**: The `make_adaptor_encoding_for_tile_distribution` function analyzes the encoding parameters
2. **Transform Chain Construction**: Individual transformations are instantiated and connected based on the specification
3. **Optimization Application**: Compile-time optimizations eliminate overhead and generate efficient code
4. **Component Integration**: The generated adaptors and descriptors integrate seamlessly with the broader CK framework

### Component Architecture

The clean separation of concerns enables both flexibility and performance:

- The **ps_ys_to_xs_adaptor** handles the complex mapping from logical to physical coordinates
- The **ys_to_d_descriptor** manages efficient linearization for register allocation
- Each component has a well-defined interface enabling independent optimization
- Components compose naturally to create complete distribution systems

### Performance Implications

Every aspect of the encoding system is designed with GPU performance in mind:

- **Memory Coalescing**: Hierarchical decomposition ensures adjacent threads access adjacent memory
- **Register Efficiency**: Y-to-D linearization optimizes register allocation and minimizes spills
- **Bank Conflict Avoidance**: Careful dimension ordering prevents shared memory bank conflicts
- **Vectorization Support**: The encoding naturally supports vector operations for maximum throughput

### Practical Applications

The encoding system's flexibility enables efficient implementation of diverse operations:

- **Matrix Multiplication**: Hierarchical tiling with appropriate replication patterns
- **Convolution**: Spatial tiling with proper boundary handling
- **Reduction Operations**: Collaborative patterns through R-dimension specification
- **Complex Fusion**: Multiple operations can share distribution patterns for efficiency

The encoding internals demonstrate how the Composable Kernel framework achieves both mathematical elegance and practical performance. By leveraging compile-time computation and sophisticated type system design, the same mathematical framework that provides clarity and composability also generates code that rivals hand-optimized implementations. This synergy between abstraction and performance represents the core strength of the CK approach to GPU kernel development. 