---
title: "Tensor Descriptors - Complete Tensor Specifications"
format: live-html
---

```{pyodide}
#| echo: false
#| output: false
#| autorun: true

# Auto-install pythonck package
import micropip
await micropip.install("https://raw.githubusercontent.com/ghamarian/pythonck/master/documentation/pythonck-0.1.0-py3-none-any.whl")
```

## Overview

A `TensorDescriptor` is the complete blueprint for a tensor. It combines a shape, stride information, and a series of transformations into a single object that defines exactly how a tensor's data is laid out in memory. This guide will walk you through creating tensors, from basic layouts to complex, transformed views.

```{pyodide}
#| echo: true
#| output: true
# Import all required modules for the page
from pytensor.tensor_descriptor import (
    make_naive_tensor_descriptor,
    make_naive_tensor_descriptor_packed,
    make_naive_tensor_descriptor_aligned,
    transform_tensor_descriptor,
    PassThroughTransform,
    UnmergeTransform,
    MergeTransform, make_merge_transform
)
```

## Creating Basic Tensor Layouts

You can create descriptors for several common memory layouts.

### 1. Custom Strides

The most fundamental way to define a tensor is with custom strides. This gives you full control over how many elements to "jump" in memory to move to the next item along each dimension. This is useful for creating padded layouts.

```{pyodide}
#| echo: true
#| output: true
# Create a 3x4 tensor, but make each row take up 8 elements in memory (4 for data, 4 for padding)
lengths = [3, 4]
strides = [8, 1]  # To move to the next row (dim 0), jump 8 elements. To move to the next column (dim 1), jump 1.

descriptor = make_naive_tensor_descriptor(lengths, strides)

print(f"Tensor Shape: {descriptor.get_lengths()}")
print(f"Custom Strides: {strides}")
# The total memory needed is 3 rows * 8 elements/row = 24
print(f"Element Space Size: {descriptor.get_element_space_size()}")

# Calculate offset of the element at [row=1, col=2]
offset = descriptor.calculate_offset([1, 2])
print(f"Offset of [1, 2]: {offset} (Expected: 1*8 + 2*1 = 10)")
```

### 2. Packed (Row-Major) Layout

For most cases, a tightly packed, row-major layout is sufficient. The strides are calculated automatically, leaving no unused space between elements.

```{pyodide}
#| echo: true
#| output: true
lengths = [3, 4]
descriptor_packed = make_naive_tensor_descriptor_packed(lengths)

print(f"Tensor Shape: {descriptor_packed.get_lengths()}")
# Total memory is 3 * 4 = 12
print(f"Element Space Size: {descriptor_packed.get_element_space_size()}")

# Calculate offset of the element at [row=1, col=2]
offset = descriptor_packed.calculate_offset([1, 2])
print(f"Offset of [1, 2]: {offset} (Expected: 1*4 + 2*1 = 6)")
```

### 3. Aligned Layout

For GPU performance, memory layouts often need to be aligned. This function creates a row-major layout but ensures that each row's starting address is a multiple of a given alignment value, adding padding if necessary.

```{pyodide}
#| echo: true
#| output: true
lengths = [4, 5]
align = 8 # Align each row to an 8-byte boundary
descriptor_aligned = make_naive_tensor_descriptor_aligned(lengths, align)

print(f"Tensor Shape: {descriptor_aligned.get_lengths()}")
print(f"Alignment: {align}")

# Without alignment, size is 4*5=20. With alignment, the stride of the first dim (rows) is padded to be a multiple of 8.
# The stride for dim 1 (columns) is 1. The stride for dim 0 (rows) becomes 8 (the smallest multiple of 8 >= 5).
# Total size = 4 rows * 8 elements/row = 32.
print(f"Element Space Size: {descriptor_aligned.get_element_space_size()} (Expected: 32)")
```

## The Pipeline Concept

It's useful to think of every `TensorDescriptor` as a **transformation pipeline**. The functions above (`make_naive...`) create the *first stage* of this pipeline: they define the initial transformation that takes a simple, one-dimensional block of memory and presents it as a logical, multi-dimensional tensor view.

The power of the library comes from adding more stages to this pipeline to create increasingly complex layouts.

### The Initial Pipeline Stage: A Closer Look

Let's inspect the pipeline of a simple packed descriptor to see this first stage in action.

```{pyodide}
#| echo: true
#| output: true
print("Inspecting the initial pipeline of a [3, 4] tensor")
print("--" * 50)

# Create a simple packed tensor
simple_desc = make_naive_tensor_descriptor_packed([3, 4])

print(f"Descriptor Shape: {simple_desc.get_lengths()}")
print(f"Total Transforms in Pipeline: {simple_desc.get_num_of_transform()}")

print("\n--- Initial Transform Internals ---")
# This shows the inputs to the transform pipeline. For a naive descriptor, it's the raw buffer (ID 0).
print(f"Lower IDs (Inputs): {simple_desc.get_lower_dimension_hidden_idss()}")

# This shows the outputs of the transform pipeline, which become the logical dimensions.
print(f"Upper IDs (Outputs): {simple_desc.get_upper_dimension_hidden_idss()}")
```

As the output shows, creating a simple `[3, 4]` tensor sets up a pipeline with a single transform.

*   **`Lower IDs (Inputs): [[0]]`**: This means the transform takes one input: the raw, one-dimensional memory buffer, which is always at hidden dimension ID `0`.
*   **`Upper IDs (Outputs): [[1, 2]]`**: This means the transform produces two outputs, which are assigned to hidden dimension IDs `1` and `2`. These become the logical dimensions `0` and `1` that you interact with when you access the tensor.

Understanding this initial stage is key to seeing how `transform_tensor_descriptor` later adds *new* stages that take these output dimensions (`1` and `2`) as their inputs.

## Advanced Layouts: A Step-by-Step Transformation

The `transform_tensor_descriptor` function adds new stages to an existing descriptor's pipeline. Let's walk through this with a detailed example, mirroring the process of a debug script.

### Goal: Transform a `[2, 6]` Tensor into a `[2, 2, 3]` View

We will reinterpret a 2D tensor with shape `[2, 6]` as a 3D tensor with shape `[2, 2, 3]`, without changing the underlying 12-element memory buffer.

#### Step 1: Define and Analyze the Base Descriptor

First, we create the `[2, 6]` base descriptor. As we established, this creates an initial pipeline stage.

```{pyodide}
#| echo: true
#| output: true
print("--- Step 1: Analyze Base Descriptor ---")
base_descriptor = make_naive_tensor_descriptor_packed([2, 6])
print(f"Base Descriptor Shape: {base_descriptor.get_lengths()}")
print("Internal Analysis:")
print(f"  - This descriptor has {base_descriptor.get_num_of_transform()} transform.")
print(f"  - It takes the raw buffer (hidden ID 0) as input: Lower IDs {base_descriptor.get_lower_dimension_hidden_idss()}")
print(f"  - It produces two outputs (hidden IDs 1 and 2): Upper IDs {base_descriptor.get_upper_dimension_hidden_idss()}")
print("  - These outputs become logical dimensions 0 and 1.")
```

#### Step 2: Define the New Transformation Stage

To get from `[2, 6]` to `[2, 2, 3]`, we must add a new transform for each of the base descriptor's logical dimensions.

*   **For logical dimension 0 (length 2):** We want to preserve it, so we'll use a `PassThroughTransform`.
*   **For logical dimension 1 (length 6):** We want to split it, so we'll use an `UnmergeTransform([2, 3])`.

We wire this new stage into the pipeline using the `lower` and `upper` ID parameters, which operate on the **logical dimensions** of their respective descriptors (input and output).

#### Step 3: Apply Transformation and Analyze the Result

Now we apply the transform and inspect the final, complete pipeline.

```{pyodide}
#| echo: true
#| output: true
print("\n--- Step 3: Apply Transformation & Analyze ---")
# Define the two new transforms to add to the pipeline
transforms = [
    PassThroughTransform(2),     # For logical dim 0
    UnmergeTransform([2, 3])     # For logical dim 1
]

# Define the pipeline wiring using logical dimension indices
# Input side:
lower_dimension_hidden_idss = [[0], [1]] # Pipe logical dim 0 to transform 0, logical dim 1 to transform 1
# Output side:
upper_dimension_hidden_idss = [[0], [1, 2]] # Transform 0's output is final logical dim 0. Transform 1's outputs are final logical dims 1 and 2.

# Create the new descriptor by adding the transforms
transformed_descriptor = transform_tensor_descriptor(
    input_descriptor=base_descriptor,
    transforms=transforms,
    lower_dimension_hidden_idss=lower_dimension_hidden_idss,
    upper_dimension_hidden_idss=upper_dimension_hidden_idss
)

print(f"Final Descriptor Shape: {transformed_descriptor.get_lengths()}")
print(f"Total Transforms in Final Pipeline: {transformed_descriptor.get_num_of_transform()}")

print("\n--- Final Pipeline Internals ---")
print(f"Lower Hidden IDs (Inputs to each transform):")
print(f"  {transformed_descriptor.get_lower_dimension_hidden_idss()}")
print(f"Upper Hidden IDs (Outputs from each transform):")
print(f"  {transformed_descriptor.get_upper_dimension_hidden_idss()}")
print(f"Top-Level Hidden IDs (Backing the final logical dims):")
print(f"  {transformed_descriptor.get_top_dimension_hidden_idss()}")
```

### Analysis of the Final Pipeline

The final debug output shows the full story of our three-stage pipeline:

*   **Transform `[0]` (The Base `UnmergeTransform`)**:
    *   `Lower hidden IDs: [[0]]`: Takes the raw memory buffer as input.
    *   `Upper hidden IDs: [[1, 2]]`: Produces the two original logical dimensions, backed by hidden IDs `1` and `2`.

*   **Transform `[1]` (Our New `PassThroughTransform`)**:
    *   `Lower hidden IDs: [[1]]`: Correctly takes hidden ID `1` as its input. This is because we specified input **logical dimension `0`**, which was backed by hidden ID `1`.
    *   `Upper hidden IDs: [[3]]`: Produces a new output, hidden ID `3`.

*   **Transform `[2]` (Our New `UnmergeTransform`)**:
    *   `Lower hidden IDs: [[2]]`: Correctly takes hidden ID `2` as its input, as it was wired from **logical dimension `1`**.
    *   `Upper hidden IDs: [[4, 5]]`: Splits its input into two new outputs, hidden IDs `4` and `5`.

*   **Final Result**: The `Top dimension hidden IDs` of the final descriptor are `[3, 4, 5]`. These are the outputs of our new transforms, and they now back the final logical dimensions `0, 1, 2` of the `[2, 2, 3]` tensor.

## Real-World GPU Example: 5D to 3D Block Transformation

These concepts are critical in GPU programming. This example transforms a 5D tensor representing a GPU thread block's workload into a simpler 3D view using `MergeTransform`.

<!-- Image: 5D to 3D transformation diagram -->

The logic is the same: we start with a 5D descriptor and apply new transforms (`PassThrough` and `Merge`) to its logical dimensions to produce a new 3D descriptor.

```{pyodide}
#| echo: true
#| output: true
print("🔗 5D to 3D LDS Block Descriptor Example")
print("=" * 50)

# 1. Define Parameters (typical for a GPU block)
print("Input Parameters:")
Block_M = 256
NumWarps = 8
WarpSize = 64
KVector = 4
wavesPerK = 2
wavesPerM = NumWarps // wavesPerK
NumIssues = Block_M // wavesPerM
print(f"  NumIssues={NumIssues}, wavesPerM={wavesPerM}, wavesPerK={wavesPerK}, WarpSize={WarpSize}, KVector={KVector}")

# 2. Create the base 5D descriptor
base_lengths = [NumIssues, wavesPerM, wavesPerK, WarpSize, KVector]
base_descriptor = make_naive_tensor_descriptor_packed(base_lengths)
print(f"\nInput 5D descriptor: {base_descriptor.get_lengths()}")
print("  - This is the complex layout we want to simplify.")

# 3. Create transforms to simplify the layout
print("\nTransforms to be applied:")
print("  - PassThroughTransform to preserve the 'NumIssues' dimension.")
print("  - MergeTransform to combine 'wavesPerM' and 'wavesPerK'.")
print("  - MergeTransform to combine 'WarpSize' and 'KVector'.")
transforms = [
    PassThroughTransform(NumIssues),
    make_merge_transform([wavesPerM, wavesPerK]),
    make_merge_transform([WarpSize, KVector])
]

# 4. Define dimension mappings
lower_dimension_hidden_idss = [[0], [1, 2], [3, 4]]
upper_dimension_hidden_idss = [[0], [1], [2]]

# 5. Apply the transformation
transformed_descriptor = transform_tensor_descriptor(
    input_descriptor=base_descriptor,
    transforms=transforms,
    lower_dimension_hidden_idss=lower_dimension_hidden_idss,
    upper_dimension_hidden_idss=upper_dimension_hidden_idss
)

print(f"\nTransformed 3D descriptor: {transformed_descriptor.get_lengths()}")

# 6. Verify the result
expected_lengths = [
    NumIssues,
    wavesPerM * wavesPerK,
    WarpSize * KVector
]
print(f"Expected 3D descriptor:    {expected_lengths}")
print(f"Verification: {'Success' if transformed_descriptor.get_lengths() == expected_lengths else 'Failure'}")
```

## Summary

-   **`TensorDescriptor` is a Pipeline**: It describes transformations from a 1D buffer to a logical tensor view.
-   **`make_naive...` Creates the First Stage**: It sets up the initial transform from a buffer to a simple packed layout.
-   **`transform_tensor_descriptor` Adds New Stages**: It allows you to build complex views by adding transforms to the pipeline.
-   **Handle All Input Dimensions**: When transforming, you must provide a new transform for each logical dimension of the input descriptor to avoid losing data. 